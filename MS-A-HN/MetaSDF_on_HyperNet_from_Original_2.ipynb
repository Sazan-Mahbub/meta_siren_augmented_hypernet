{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTwikOyWZobA"
   },
   "source": [
    "# MetaSDF & Meta-SIREN\n",
    "\n",
    "This is a colab to explore MetaSDF, and its applications to rapidly fit neural implicit representations.\n",
    "\n",
    "Make sure to switch the runtime type to \"GPU\" under \"Runtime --> Change Runtime Type\"!\n",
    "\n",
    "We will show you how to run two experiments using gradient-based meta-learning: \n",
    "* [Fitting an image in 3 gradient descent steps with SIREN](#section_1)\n",
    "* [Fitting 2D Signed Distance Functions of MNIST digits](#section_2)\n",
    "\n",
    "Let's go! \n",
    "\n",
    "First, the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ49alfvZobQ",
    "outputId": "627391a5-abd0-4101-8a73-66302c056cbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sazan\\AppData\\Local\\Temp/ipykernel_23212/3975817298.py:12: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import OrderedDict, Mapping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scipy.ndimage\n",
    "from torch import nn \n",
    "from collections import OrderedDict, Mapping \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "from torch.nn.init import _calculate_correct_fan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrsG6X9cDsZe",
    "outputId": "a6e98051-c4ed-4948-8475-5c9775f19c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15]) torch.Size([2, 3]) \n",
      ">>\n",
      " tensor([[ 1,  2,  3,  1,  2,  3,  1,  2,  3,  1,  2,  3,  1,  2,  3],\n",
      "        [10, 20, 30, 10, 20, 30, 10, 20, 30, 10, 20, 30, 10, 20, 30]]) \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3]],\n",
      "\n",
      "        [[10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30]]]) tensor([[ 1,  2,  3],\n",
      "        [10, 20, 30]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [10,20,30]]) \n",
    "print(x.repeat(1, 5).shape, x.shape,'\\n>>\\n', x.repeat(1, 5), '\\n', x.repeat(1, 5).view([-1, 5, 3]), x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2D7783s2tg2"
   },
   "source": [
    "For meta-learning, we're using the excellent \"Torchmeta\" library. We have to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eX_d5hsYZtSq",
    "outputId": "4122e3c1-31d0-4e35-e770-fd46c4a5c8e4"
   },
   "outputs": [],
   "source": [
    "# !pip install torchmeta\n",
    "from torchmeta.modules import (MetaModule, MetaSequential, MetaLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je35HXUPZobV"
   },
   "source": [
    "We're now ready to implement a few neural network layers: Fully connected networks, and SIREN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DlEcUhGiZobX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BatchLinear(nn.Linear, MetaModule):\n",
    "    '''A linear meta-layer that can deal with batched weight matrices and biases, as for instance output by a\n",
    "    hypernetwork.'''\n",
    "    __doc__ = nn.Linear.__doc__\n",
    "\n",
    "    def forward(self, input, params=None):\n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "\n",
    "        bias = params.get('bias', None)\n",
    "        weight = params['weight']\n",
    "\n",
    "        output = input.matmul(weight.permute(*[i for i in range(len(weight.shape)-2)], -1, -2))\n",
    "        output += bias.unsqueeze(-2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MetaFC(MetaModule):\n",
    "    '''A fully connected neural network that allows swapping out the weights, either via a hypernetwork\n",
    "    or via MAML.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 num_hidden_layers, hidden_features,\n",
    "                 outermost_linear=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(MetaSequential(\n",
    "            BatchLinear(in_features, hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ))\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, hidden_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        if outermost_linear:\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, out_features),\n",
    "            ))\n",
    "        else:\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        self.net = MetaSequential(*self.net)\n",
    "        self.net.apply(init_weights_normal)\n",
    "\n",
    "    def forward(self, coords, params=None, **kwargs):\n",
    "        '''Simple forward pass without computation of spatial gradients.'''\n",
    "        output = self.net(coords, params=self.get_subdict(params, 'net'))\n",
    "        return output\n",
    "\n",
    "\n",
    "class SineLayer(MetaModule):\n",
    "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
    "\n",
    "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
    "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
    "    # hyperparameter.\n",
    "\n",
    "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
    "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = float(omega_0)\n",
    "\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.linear = BatchLinear(in_features, out_features, bias=bias)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features,\n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
    "                                            np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "\n",
    "    def forward(self, input, params=None):\n",
    "        intermed = self.linear(input, params=self.get_subdict(params, 'linear'))\n",
    "        return torch.sin(self.omega_0 * intermed)\n",
    "\n",
    "\n",
    "class Siren(MetaModule):\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False,\n",
    "                 first_omega_0=30, hidden_omega_0=30., special_first=True):\n",
    "        super().__init__()\n",
    "        self.hidden_omega_0 = hidden_omega_0\n",
    "\n",
    "        layer = SineLayer\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(layer(in_features, hidden_features,\n",
    "                              is_first=special_first, omega_0=first_omega_0))\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(layer(hidden_features, hidden_features,\n",
    "                                  is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = BatchLinear(hidden_features, out_features)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / 30.,\n",
    "                                             np.sqrt(6 / hidden_features) / 30.)\n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(layer(hidden_features, out_features, is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        self.net = nn.ModuleList(self.net)\n",
    "\n",
    "    def forward(self, coords, params=None):\n",
    "        x = coords\n",
    "\n",
    "        for i, layer in enumerate(self.net):\n",
    "            x = layer(x, params=self.get_subdict(params, f'net.{i}'))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def init_weights_normal(m):\n",
    "    if type(m) == BatchLinear or nn.Linear:\n",
    "        if hasattr(m, 'weight'):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if hasattr(m, 'bias'):\n",
    "            m.bias.data.fill_(0.)\n",
    "            \n",
    "            \n",
    "def get_mgrid(sidelen):\n",
    "    # Generate 2D pixel coordinates from an image of sidelen x sidelen\n",
    "    pixel_coords = np.stack(np.mgrid[:sidelen,:sidelen], axis=-1)[None,...].astype(np.float32)\n",
    "    pixel_coords /= sidelen    \n",
    "    pixel_coords -= 0.5\n",
    "    pixel_coords = torch.Tensor(pixel_coords).view(-1, 2)\n",
    "    return pixel_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRN7_GF4jwyT"
   },
   "source": [
    "Sazan: Now let's implement our Cross-Attention Hypernetwork. It will take the image as input and generate some matrices with the same dimension as the weights of the SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i0ai6Ry7jvmE"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from modules_custom import Conv2dResBlock\n",
    "\n",
    "class CrossAttentionHyperNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=5//2) # padding=kernel_size//2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=5//2) # padding=kernel_size//2\n",
    "        # self.conv3 = nn.Conv2d(32, 64, 5, padding=1)\n",
    "        # self.conv4_dim_reduction = nn.Conv2d(16+32+64, 64, 1, padding=0)\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 3, 1, 1),\n",
    "#             nn.ReLU(),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             nn.Conv2d(256, 256, 1, 1, 0)\n",
    "#         )\n",
    "\n",
    "        self.relu_2 = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "\n",
    "        if False:\n",
    "            self.fc0 = nn.Linear(64, 2)\n",
    "            self.fc1 = nn.Linear(64, 64)\n",
    "            self.fc2 = nn.Linear(64, 64)\n",
    "            self.fc3 = nn.Linear(64, 64)\n",
    "            self.fc4_1 = nn.Linear(64, 3)\n",
    "            self.fc4_2 = nn.Linear(3, 3)\n",
    "            self.fc4_bias = nn.Linear(64, 3)\n",
    "\n",
    "        if False:\n",
    "            self.weighted_mean = torch.nn.Conv1d(in_channels=64, out_channels=5, kernel_size=1)\n",
    "\n",
    "            self.bias0_fc = nn.Linear(64+64, 64)\n",
    "            self.bias1_fc = nn.Linear(64+64, 64)\n",
    "            self.bias2_fc = nn.Linear(64+64, 64)\n",
    "            self.bias3_fc = nn.Linear(64+64, 64)\n",
    "            self.bias4_fc = nn.Linear(3+64, 3)\n",
    "            \n",
    "            self.attn_bias0_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias1_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias2_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias3_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias4_fc = nn.Linear(3, 1)\n",
    "\n",
    "\n",
    "        self.wt_cross_attn0 = nn.MultiheadAttention(embed_dim=2, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.wt_cross_attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.wt_cross_attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.wt_cross_attn3 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.wt_cross_attn4 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        \n",
    "        self.bias_cross_attn0 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.bias_cross_attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.bias_cross_attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.bias_cross_attn3 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        self.bias_cross_attn4 = nn.MultiheadAttention(embed_dim=3, num_heads=1, dropout=0.1, bias=True, batch_first=True)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    net.0.linear.weight :\t torch.Size([64, 2]) \t torch.Size([16, 64, 2])\n",
    "    net.0.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.1.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.1.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.2.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.2.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.3.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.3.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.4.weight :\t torch.Size([3, 64]) \t torch.Size([16, 3, 64])\n",
    "    net.4.bias :\t torch.Size([3]) \t torch.Size([16, 3])\n",
    "    '''\n",
    "    \n",
    "    def forward_conv(self, x):\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        b = x.shape[0]\n",
    "        # print('1>', x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x))) # bx3x32x32 -> bx32x16x16\n",
    "        # print('2>', x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x))).permute(0, 2, 3, 1).contiguous() # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        # print('3>', x.shape)\n",
    "        # x3 = self.pool(F.relu(self.conv3(x2)))\n",
    "        # x = torch.cat([x1, x2], -1)\n",
    "        # x = F.relu(self.conv4_dim_reduction(x))\n",
    "        x = x.view([b, 64, 64]) # bx8x8x64 -> bx64x64 ## channel last\n",
    "        # print('4>', x.shape)\n",
    "        \n",
    "        x0 = F.relu(self.fc0(x)) # bx64x64 -> bx64x2\n",
    "        x1 = F.relu(self.fc1(x)) # bx64x64 -> bx64x64\n",
    "        x2 = F.relu(self.fc2(x)) # bx64x64 -> bx64x64\n",
    "        x3 = F.relu(self.fc3(x)) # bx64x64 -> bx64x64\n",
    "        x4 = F.relu(self.fc4_1(x.permute(0,2,1))) # bx64x64 -> bx64x64 -> bx64x3\n",
    "        x4 = F.relu(self.fc4_2(x4)).permute(0,2,1).contiguous() # bx64x3 -> bx64x3 -> bx3x64\n",
    "\n",
    "        # x_biases = F.relu(self.weighted_mean(x)) # bx64x64 -> bx5x64\n",
    "        # x0_bias = F.relu(self.bias0_fc(x_biases[:, 0, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x1_bias = F.relu(self.bias1_fc(x_biases[:, 1, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x2_bias = F.relu(self.bias2_fc(x_biases[:, 2, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x3_bias = F.relu(self.bias3_fc(x_biases[:, 3, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x4_bias = F.relu(self.bias4_fc(x_biases[:, 4, :])) # bx5x64 ->  bx64 -> bx3\n",
    "\n",
    "        return x, x0, x1, x2, x3, x4#, x0_bias, x1_bias, x2_bias, x3_bias, x4_bias\n",
    "    \n",
    "    def bias_attention(self, x, meta_param_bias, bias_fc, attn_bias_fc):\n",
    "        b, c = meta_param_bias.shape\n",
    "        meta_param_bias = meta_param_bias.view(b, 1, c)\n",
    "        param_bias = torch.cat([meta_param_bias.repeat(1,64,1), x], -1) # [bx1xc, bx64x64] -> [bx64xc, bx64x64] -> bx64x(c+64)\n",
    "        param_bias = F.relu(bias_fc(param_bias)) # bx64x(c+64) -> bx64xc\n",
    "        attention_scores = F.relu(attn_bias_fc(param_bias)).permute(0,2,1) # bx64xc -> bx64x1 -> bx1x64\n",
    "        attention_scores = F.softmax(attention_scores, -1) # bx1x64 -> bx1x64\n",
    "        param_bias = torch.bmm(attention_scores, param_bias).view(b, c) # [bx1x64, bx64xc] -> bx1xc -> bxc\n",
    "        return param_bias\n",
    "\n",
    "    def compute_biases(self, x, meta_params):\n",
    "        x0_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.0.linear.bias'], bias_fc=self.bias0_fc, attn_bias_fc=self.attn_bias0_fc)\n",
    "        x1_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.1.linear.bias'], bias_fc=self.bias1_fc, attn_bias_fc=self.attn_bias1_fc)\n",
    "        x2_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.2.linear.bias'], bias_fc=self.bias2_fc, attn_bias_fc=self.attn_bias2_fc)\n",
    "        x3_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.3.linear.bias'], bias_fc=self.bias3_fc, attn_bias_fc=self.attn_bias3_fc)\n",
    "        x4_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.4.bias'], bias_fc=self.bias4_fc, attn_bias_fc=self.attn_bias4_fc)\n",
    "        return x0_bias, x1_bias, x2_bias, x3_bias, x4_bias \n",
    "\n",
    "    def compute_loss(self, specialized_param, gt_specialized_param):\n",
    "        loss = 0.\n",
    "        for key in specialized_param:\n",
    "            loss += F.mse_loss(input=specialized_param[key], target=gt_specialized_param[key])\n",
    "        loss /= 10 # not sure if it will help\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, meta_params):\n",
    "        x, x0, x1, x2, x3, x4 = self.forward_conv(x) \n",
    "        # x0_bias, x1_bias, x2_bias, x3_bias, x4_bias = self.compute_biases(x, meta_params)\n",
    "\n",
    "        # x = self.forward_conv(x) # bx32x32x3 -> bx64x64\n",
    "        b, l, c = x.shape\n",
    "        # query -> from meta model ==> meta_params\n",
    "        # key and value -> from this model ==> x\n",
    "        specialized_param = OrderedDict()\n",
    "\n",
    "        # print('1>', meta_params['net.4.bias'].shape, x.shape)\n",
    "        # x_in = self.fc0(x)\n",
    "        specialized_param['net.0.linear.weight']  = self.wt_cross_attn0(query=meta_params['net.0.linear.weight'], key=x0, value=x0)[0]\n",
    "        specialized_param['net.1.linear.weight']  = self.wt_cross_attn1(query=meta_params['net.1.linear.weight'], key=x1, value=x1)[0]\n",
    "        specialized_param['net.2.linear.weight']  = self.wt_cross_attn2(query=meta_params['net.2.linear.weight'], key=x2, value=x2)[0]\n",
    "        specialized_param['net.3.linear.weight']  = self.wt_cross_attn3(query=meta_params['net.3.linear.weight'], key=x3, value=x3)[0]\n",
    "        specialized_param['net.4.weight']  = self.wt_cross_attn4(query=meta_params['net.4.weight'], key=x4, value=x4)[0]\n",
    "        x_out = F.relu(self.fc4_bias(x))\n",
    "        specialized_param['net.0.linear.bias']  = self.bias_cross_attn0(query=meta_params['net.0.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.1.linear.bias']  = self.bias_cross_attn1(query=meta_params['net.1.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.2.linear.bias']  = self.bias_cross_attn2(query=meta_params['net.2.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.3.linear.bias']  = self.bias_cross_attn3(query=meta_params['net.3.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.4.bias']  = self.bias_cross_attn4(query=meta_params['net.4.bias'].view(b, 1, -1), key=x_out, value=x_out)[0].view(b, -1)\n",
    "\n",
    "#         loss = self.compute_loss(specialized_param, gt_specialized_param)\n",
    "\n",
    "#         return loss, specialized_param\n",
    "        return specialized_param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhPvd2TUkZC8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cPO4PWDZobZ"
   },
   "source": [
    "Now, we implement MAML. The important parts of the code are commented, so it's easy to understand how each part works! Start by looking at the \"forward\" function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OL4dgzRiZoba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(prediction, gt):\n",
    "    return ((prediction - gt)**2).mean()\n",
    "\n",
    "\n",
    "class MAML(nn.Module):\n",
    "    def __init__(self, num_meta_steps, hypo_module, crossAttHypNet, loss, init_lr,\n",
    "                 lr_type='static', first_order=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hypo_module = hypo_module # The module who's weights we want to meta-learn.\n",
    "        self.crossAttHypNet = crossAttHypNet\n",
    "        self.first_order = first_order\n",
    "        self.loss = loss\n",
    "        self.lr_type = lr_type\n",
    "        self.log = []\n",
    "\n",
    "        self.register_buffer('num_meta_steps', torch.Tensor([num_meta_steps]).int())\n",
    "\n",
    "        if self.lr_type == 'static': \n",
    "            self.register_buffer('lr', torch.Tensor([init_lr]))\n",
    "        elif self.lr_type == 'global':\n",
    "            self.lr = nn.Parameter(torch.Tensor([init_lr]))\n",
    "        elif self.lr_type == 'per_step':\n",
    "            self.lr = nn.ParameterList([nn.Parameter(torch.Tensor([init_lr]))\n",
    "                                        for _ in range(num_meta_steps)])\n",
    "        elif self.lr_type == 'per_parameter': # As proposed in \"Meta-SGD\".\n",
    "            self.lr = nn.ParameterList([])\n",
    "            hypo_parameters = hypo_module.parameters()\n",
    "            for param in hypo_parameters:\n",
    "                self.lr.append(nn.Parameter(torch.ones(param.size()) * init_lr))\n",
    "        elif self.lr_type == 'per_parameter_per_step':\n",
    "            self.lr = nn.ModuleList([])\n",
    "            for name, param in hypo_module.meta_named_parameters():\n",
    "                self.lr.append(nn.ParameterList([nn.Parameter(torch.ones(param.size()) * init_lr)\n",
    "                                                 for _ in range(num_meta_steps)]))\n",
    "\n",
    "        param_count = 0\n",
    "        for param in self.parameters():\n",
    "            param_count += np.prod(param.shape)\n",
    "\n",
    "        print(param_count)\n",
    "\n",
    "    def _update_step(self, loss, param_dict, step):\n",
    "        grads = torch.autograd.grad(loss, param_dict.values(),\n",
    "                                    create_graph=False if self.first_order else True)\n",
    "        params = OrderedDict()\n",
    "        for i, ((name, param), grad) in enumerate(zip(param_dict.items(), grads)):\n",
    "            if self.lr_type in ['static', 'global']:\n",
    "                lr = self.lr\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_step']:\n",
    "                lr = self.lr[step]\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_parameter']:\n",
    "                lr = self.lr[i]\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_parameter_per_step']:\n",
    "                lr = self.lr[i][step]\n",
    "                params[name] = param - lr * grad\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return params, grads\n",
    "\n",
    "    def forward_with_params(self, query_x, fast_params, **kwargs):\n",
    "        output = self.hypo_module(query_x, params=fast_params)\n",
    "        return output\n",
    "\n",
    "    def generate_params(self, context_dict):\n",
    "        \"\"\"Specializes the model\"\"\"\n",
    "        x = context_dict.get('x').cuda()\n",
    "        y = context_dict.get('y').cuda()\n",
    "\n",
    "        meta_batch_size = x.shape[0]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            # First, replicate the initialization for each batch item.\n",
    "            # This is the learned initialization, i.e., in the outer loop,\n",
    "            # the gradients are backpropagated all the way into the \n",
    "            # \"meta_named_parameters\" of the hypo_module.\n",
    "            fast_params = OrderedDict()\n",
    "            meta_params = OrderedDict()\n",
    "            for name, param in self.hypo_module.meta_named_parameters():\n",
    "                fast_params[name] = param[None, ...].repeat((meta_batch_size,) + (1,) * len(param.shape))\n",
    "                meta_params[name] = param[None, ...].repeat((meta_batch_size,) + (1,) * len(param.shape))\n",
    "\n",
    "            prev_loss = 1e6\n",
    "            intermed_predictions = []\n",
    "            for j in range(self.num_meta_steps):\n",
    "                # Using the current set of parameters, perform a forward pass with the context inputs.\n",
    "                predictions = self.hypo_module(x, params=fast_params)\n",
    "\n",
    "                # Compute the loss on the context labels.\n",
    "                loss = self.loss(predictions, y)\n",
    "                intermed_predictions.append(predictions)\n",
    "\n",
    "                if loss > prev_loss:\n",
    "                    print('inner lr too high?')\n",
    "                \n",
    "                # Using the computed loss, update the fast parameters.\n",
    "                fast_params, grads = self._update_step(loss, fast_params, j)\n",
    "                prev_loss = loss\n",
    "\n",
    "        return fast_params, intermed_predictions, meta_params\n",
    "\n",
    "    def forward(self, meta_batch, **kwargs):\n",
    "        # The meta_batch conists of the \"context\" set (the observations we're conditioning on)\n",
    "        # and the \"query\" inputs (the points where we want to evaluate the specialized model)\n",
    "        context = meta_batch['context']\n",
    "        query_x = meta_batch['query']['x'].cuda()\n",
    "\n",
    "        # Specialize the model with the \"generate_params\" function.\n",
    "        fast_params, intermed_predictions, meta_params = self.generate_params(context)\n",
    "        pred_specialized_param = self.crossAttHypNet(x=lin2img(context['y']).cuda())#, meta_params=meta_params, gt_specialized_param=fast_params)\n",
    "\n",
    "        pred_specialized_param_corrected = OrderedDict()\n",
    "\n",
    "        crossAttHypNet_loss = 0.\n",
    "        if True:\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "#                 pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "                pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "#                 crossAttHypNet_loss += ((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "                \n",
    "        # Compute the final outputs. \n",
    "        model_output = self.hypo_module(query_x, params=fast_params)\n",
    "        model_output_hypernet = self.hypo_module(query_x, params=pred_specialized_param_corrected)\n",
    "        crossAttHypNet_loss += self.loss(model_output_hypernet, context['y'])\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i0VRikaZobc"
   },
   "source": [
    "<a id='section_1'></a>\n",
    "## Learning to fit images in 3 gradient descent steps\n",
    "\n",
    "By learning an initialization for SIREN, we may fit any image in as few as 3 gradient descent steps! \n",
    "This has also been noted by Tancik et al. in \"Learned Initializations for Optimizing Coordinate-Based Neural Representations\" (2020).\n",
    "\n",
    "We'll demonstrate here with Cifar-10, but it works just as well with CelebA or imagenet - try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gKFj5-FVZobc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR10():\n",
    "    def __init__(self, train=True):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        self.dataset = torchvision.datasets.CIFAR10(root='./data', train=train,\n",
    "                                                download=True, transform=transform)\n",
    "        \n",
    "        self.length = len(self.dataset)\n",
    "        self.meshgrid = get_mgrid(sidelen=32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img, _ = self.dataset[item]\n",
    "        img_flat = img.permute(1,2,0).view(-1, 3)\n",
    "        return {'context':{'x':self.meshgrid, 'y':img_flat}, \n",
    "                'query':{'x':self.meshgrid, 'y':img_flat}}\n",
    "\n",
    "\n",
    "def lin2img(tensor):\n",
    "    batch_size, num_samples, channels = tensor.shape\n",
    "    sidelen = np.sqrt(num_samples).astype(int)\n",
    "    return tensor.view(batch_size, sidelen, sidelen, channels).squeeze(-1)\n",
    "\n",
    "    \n",
    "def plot_sample_image(img_batch, ax):\n",
    "    img = lin2img(img_batch)[0].detach().cpu().numpy()\n",
    "    img += 1\n",
    "    img /= 2.\n",
    "    img = np.clip(img, 0., 1.)\n",
    "#     ax.set_axis_off()\n",
    "#     ax.imshow(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def dict_to_gpu(ob):\n",
    "    if isinstance(ob, Mapping):\n",
    "        return {k: dict_to_gpu(v) for k, v in ob.items()}\n",
    "    else:\n",
    "        return ob.cuda()    \n",
    "\n",
    "\n",
    "# def dict_to_gpu(ob):\n",
    "#     if isinstance(ob, Mapping):\n",
    "#         return {k: dict_to_gpu(v) for k, v in ob.items()}\n",
    "#     else:\n",
    "#         return ob.cuda()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t9tBE4EZobd"
   },
   "source": [
    "Now, let's initialize our models and our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKLBYi25Zobd",
    "outputId": "b6694221-afa4-472e-a572-198500159e14",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "66906387\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import meta_modules\n",
    "img_siren = Siren(in_features=2, hidden_features=256, hidden_layers=3, out_features=3, outermost_linear=True)\n",
    "# img_siren.load_state_dict(torch.load('img_siren_1.pth'))\n",
    "# crossAttHypNet = CrossAttentionHyperNet().cuda()\n",
    "crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                    out_features=3,\n",
    "                                                                    image_resolution=(32, 32))\n",
    "# crossAttHypNet.load_state_dict(torch.load('crossAttHypNet_1.pth'))\n",
    "meta_siren = MAML(num_meta_steps=3, hypo_module=img_siren.cuda(), crossAttHypNet=crossAttHypNet.cuda(), \n",
    "                  loss=l2_loss, init_lr=1e-5, \n",
    "                  lr_type='per_parameter_per_step').cuda()\n",
    "# meta_siren.load_state_dict(torch.load('meta_siren_1.pth'))\n",
    "meta_siren = meta_siren.cuda()\n",
    "if True:\n",
    "    del crossAttHypNet\n",
    "    torch.cuda.empty_cache()\n",
    "    crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                        out_features=3,\n",
    "                                                                        image_resolution=(32, 32))\n",
    "    meta_siren.crossAttHypNet = crossAttHypNet.cuda()\n",
    "meta_siren.train()\n",
    "\n",
    "dataset = CIFAR10()\n",
    "dataloader = DataLoader(dataset, batch_size=16, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lJQ3Fdr-zvIB"
   },
   "outputs": [],
   "source": [
    "# crossAttHypNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5dv0PYqRZlN",
    "outputId": "8a79253e-e5de-4e2b-8c11-147675e8b438"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# img1 = cv2.imread('img1.bmp')\n",
    "# img2 = cv2.imread('img2.bmp')\n",
    "# psnr = cv2.PSNR(img1, img2)\n",
    "# psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install piqa\n",
    "from metrics import psnr, ssim_metric\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_sklearn\n",
    "\n",
    "# print('PSNR:', psnr.psnr(x, y))\n",
    "# print('SSIM:', ssim.ssim(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS7Juv9HZobf"
   },
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "pzmwE0_KZobg",
    "outputId": "96cc34be-e0a5-41f8-f455-1c68d3579309",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0,\tTotal loss: 0.441008,\tHypernet loss: 0.217941\n",
      "\tPSNR: nan \tSSIM: nan\n",
      "[62.01234240954363, 62.012708711470516, 62.01307076133666, 62.01339597401285, 61.885344026343745]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 1000,\tTotal loss: 0.135676,\tHypernet loss: 0.101024\n",
      "\tPSNR: 15.9077691757977 \tSSIM: 0.44811460069869646\n",
      "[61.89276316089767, 65.33812653474429, 66.72663014878312, 67.4210016535136, 64.14568329002252]\n",
      "Epoch 0, Step 2000,\tTotal loss: 0.082741,\tHypernet loss: 0.067622\n",
      "\tPSNR: 16.713669650897383 \tSSIM: 0.48531833636190275\n",
      "[61.644581608558724, 69.43436450517785, 71.81342084662914, 72.63865621605663, 67.09256872054888]\n",
      "Epoch 0, Step 3000,\tTotal loss: 0.063986,\tHypernet loss: 0.055482\n",
      "\tPSNR: 17.274713579942784 \tSSIM: 0.5134680034945874\n",
      "[60.396494732781164, 70.56211274569588, 73.56552076081863, 74.70762606367956, 67.44800587030379]\n",
      "Epoch 1, Step 0,\tTotal loss: 0.062723,\tHypernet loss: 0.054409\n",
      "\tPSNR: 17.333896625013352 \tSSIM: 0.5165101732730121\n",
      "[59.238380251337446, 68.36760688637044, 70.36823356849324, 71.67846896243167, 64.69322641056239]\n",
      "Epoch 1, Step 1000,\tTotal loss: 0.049831,\tHypernet loss: 0.044711\n",
      "\tPSNR: 17.752077210491354 \tSSIM: 0.5385255138476125\n",
      "[58.793735739374206, 73.52288662183214, 76.82603968509116, 78.86417373424243, 68.72677470026014]\n",
      "Epoch 1, Step 2000,\tTotal loss: 0.066917,\tHypernet loss: 0.061063\n",
      "\tPSNR: 18.08161058489288 \tSSIM: 0.5564515114944065\n",
      "[61.04734926931087, 74.2314467834906, 77.18143036140745, 79.32983327613935, 68.78027326347348]\n",
      "Epoch 1, Step 3000,\tTotal loss: 0.062275,\tHypernet loss: 0.057737\n",
      "\tPSNR: 18.349907271098118 \tSSIM: 0.5711746553317548\n",
      "[56.657169045942936, 71.24676804976441, 74.37818416872388, 77.10328668596743, 66.66716794581478]\n",
      "Epoch 2, Step 0,\tTotal loss: 0.049608,\tHypernet loss: 0.045368\n",
      "\tPSNR: 18.37910527258396 \tSSIM: 0.57285624217432\n",
      "[59.360866367243304, 71.07821547586438, 74.05902896522646, 76.44357124185697, 65.48012746141282]\n",
      "Epoch 2, Step 1000,\tTotal loss: 0.045089,\tHypernet loss: 0.042470\n",
      "\tPSNR: 18.612123715347256 \tSSIM: 0.585602920242521\n",
      "[63.43516102433712, 74.53113358305339, 78.12416804854475, 81.31733404044972, 67.70285173574959]\n",
      "Epoch 2, Step 2000,\tTotal loss: 0.043695,\tHypernet loss: 0.041629\n",
      "\tPSNR: 18.810736968455892 \tSSIM: 0.5963849769641899\n",
      "[57.727088207860184, 73.37269103620962, 77.70072455709837, 80.80821399946564, 66.43541871886043]\n",
      "Epoch 2, Step 3000,\tTotal loss: 0.048366,\tHypernet loss: 0.046344\n",
      "\tPSNR: 18.984689927284784 \tSSIM: 0.6059881678713687\n",
      "[60.1463051481351, 75.05281544998527, 79.27832432509213, 83.34024158447266, 67.61790458962611]\n",
      "Epoch 3, Step 0,\tTotal loss: 0.044739,\tHypernet loss: 0.043492\n",
      "\tPSNR: 19.00598928071022 \tSSIM: 0.6071221287040661\n",
      "[59.49046978931706, 75.99800848359837, 80.39233324322679, 83.4013750793239, 68.45666879170436]\n",
      "Epoch 3, Step 1000,\tTotal loss: 0.038376,\tHypernet loss: 0.037546\n",
      "\tPSNR: 19.172178585354104 \tSSIM: 0.6161921234097422\n",
      "[61.31054403030382, 77.52634740652428, 82.50715367888253, 86.60771783224658, 69.74102602177736]\n",
      "Epoch 3, Step 2000,\tTotal loss: 0.036063,\tHypernet loss: 0.035296\n",
      "\tPSNR: 19.321736038420227 \tSSIM: 0.6242936558596339\n",
      "[58.00326864174384, 75.12904700608131, 80.79599616752708, 85.32659004095882, 68.29732716161386]\n",
      "Epoch 3, Step 3000,\tTotal loss: 0.038464,\tHypernet loss: 0.037659\n",
      "\tPSNR: 19.459473118847068 \tSSIM: 0.6317511597304897\n",
      "[60.76634269606163, 77.3669306621689, 81.5781050093665, 85.01033347654385, 69.95710845627643]\n",
      "Epoch 4, Step 0,\tTotal loss: 0.045616,\tHypernet loss: 0.044816\n",
      "\tPSNR: 19.47686599490881 \tSSIM: 0.632642358484324\n",
      "[59.610395436283255, 78.02338316714123, 83.44721503484595, 87.74311353758239, 69.78276578129174]\n",
      "Epoch 4, Step 1000,\tTotal loss: 0.030342,\tHypernet loss: 0.029782\n",
      "\tPSNR: 19.609708509888915 \tSSIM: 0.6397213197185451\n",
      "[61.11305426338427, 77.53642536556364, 83.12561612879244, 87.74282927223702, 69.62433357055792]\n",
      "Epoch 4, Step 2000,\tTotal loss: 0.033473,\tHypernet loss: 0.033008\n",
      "\tPSNR: 19.729938001507314 \tSSIM: 0.6461374165291641\n",
      "[61.657525003590365, 78.66892598589149, 84.07454964360745, 88.60066848583706, 69.48472735408565]\n",
      "Epoch 4, Step 3000,\tTotal loss: 0.027875,\tHypernet loss: 0.027443\n",
      "\tPSNR: 19.843975801700545 \tSSIM: 0.6521399674393207\n",
      "[61.86623985827546, 80.32964236522295, 86.99948935947874, 92.15908391957136, 72.07804173666203]\n",
      "Epoch 5, Step 0,\tTotal loss: 0.034343,\tHypernet loss: 0.033716\n",
      "\tPSNR: 19.858118020730974 \tSSIM: 0.6528791244308203\n",
      "[58.553743020870684, 76.04672950203297, 82.21157612017521, 87.8531366568844, 67.23078254539561]\n",
      "Epoch 5, Step 1000,\tTotal loss: 0.030490,\tHypernet loss: 0.029992\n",
      "\tPSNR: 19.969204409751676 \tSSIM: 0.6587189253783063\n",
      "[57.88601079945917, 82.25270698971094, 88.31267774089676, 92.55363975126932, 75.91670982896964]\n",
      "Epoch 5, Step 2000,\tTotal loss: 0.035675,\tHypernet loss: 0.035293\n",
      "\tPSNR: 20.07304590129007 \tSSIM: 0.664051697478372\n",
      "[62.36947490819328, 78.80197388172233, 84.55114975204795, 88.80424429463122, 68.55417305804997]\n",
      "Epoch 5, Step 3000,\tTotal loss: 0.026512,\tHypernet loss: 0.026266\n",
      "\tPSNR: 20.168724767801745 \tSSIM: 0.6690258821141375\n",
      "[61.03982624456704, 79.26876165444224, 85.97335597620557, 91.44742535610109, 70.38442650512201]\n",
      "Epoch 6, Step 0,\tTotal loss: 0.027042,\tHypernet loss: 0.026802\n",
      "\tPSNR: 20.18028053164641 \tSSIM: 0.6696055881830429\n",
      "[58.65604592352143, 78.21414285774374, 85.33503297202658, 91.07488725482936, 69.06356303337938]\n",
      "Epoch 6, Step 1000,\tTotal loss: 0.029191,\tHypernet loss: 0.028960\n",
      "\tPSNR: 20.276462798482257 \tSSIM: 0.6744823524971151\n",
      "[66.15339643240155, 81.57822444007041, 87.99778352632731, 93.24707562070778, 72.24171735045863]\n",
      "Epoch 6, Step 2000,\tTotal loss: 0.034165,\tHypernet loss: 0.033836\n",
      "\tPSNR: 20.364182568629104 \tSSIM: 0.6789777262230033\n",
      "[62.632849429962015, 79.72589652424733, 86.59840879499231, 92.72864509227489, 70.58503274697914]\n",
      "Epoch 6, Step 3000,\tTotal loss: 0.022370,\tHypernet loss: 0.022240\n",
      "\tPSNR: 20.448981077124333 \tSSIM: 0.6832503862191184\n",
      "[61.435410712262424, 78.4193148084867, 85.15164987285074, 90.68067056288947, 68.25105453036971]\n",
      "Epoch 7, Step 0,\tTotal loss: 0.024229,\tHypernet loss: 0.024066\n",
      "\tPSNR: 20.459228434204373 \tSSIM: 0.683754526394297\n",
      "[59.79169522957921, 80.0406835224947, 86.33013337145553, 91.53894351738467, 71.59007068201076]\n",
      "Epoch 7, Step 1000,\tTotal loss: 0.028966,\tHypernet loss: 0.028785\n",
      "\tPSNR: 20.544023338548474 \tSSIM: 0.6879410824247539\n",
      "[59.96576145685357, 81.38522925123907, 89.53868107892063, 95.8666513214, 71.32885506916469]\n",
      "Epoch 7, Step 2000,\tTotal loss: 0.023527,\tHypernet loss: 0.023413\n",
      "\tPSNR: 20.623646495088856 \tSSIM: 0.6917793340115316\n",
      "[59.87806452922919, 76.72817544918037, 84.54080237737095, 92.20100789007435, 67.0881940758459]\n",
      "Epoch 7, Step 3000,\tTotal loss: 0.028224,\tHypernet loss: 0.028089\n",
      "\tPSNR: 20.695868376975085 \tSSIM: 0.6954531991800302\n",
      "[57.0246551686667, 78.83873202241546, 87.32204018184424, 95.22020576092494, 70.62054122705806]\n",
      "Epoch 8, Step 0,\tTotal loss: 0.028462,\tHypernet loss: 0.028333\n",
      "\tPSNR: 20.704981580485107 \tSSIM: 0.6959004159730208\n",
      "[60.177780906431046, 79.69687790352205, 87.22024179108519, 93.21753345136655, 69.39508564690104]\n",
      "Epoch 8, Step 1000,\tTotal loss: 0.020321,\tHypernet loss: 0.020231\n",
      "\tPSNR: 20.780007615306058 \tSSIM: 0.6995030903736971\n",
      "[64.57543268647503, 87.2685629332195, 95.60686267135986, 100.90560275740549, 78.33265108212245]\n",
      "Epoch 8, Step 2000,\tTotal loss: 0.027649,\tHypernet loss: 0.027546\n",
      "\tPSNR: 20.850707407025276 \tSSIM: 0.7028994327823855\n",
      "[62.071604603508355, 81.98674952917031, 89.89103194796868, 96.46588467023, 72.01256977187589]\n",
      "Epoch 8, Step 3000,\tTotal loss: 0.024734,\tHypernet loss: 0.024617\n",
      "\tPSNR: 20.916793327877564 \tSSIM: 0.7060951096878104\n",
      "[58.67388287951509, 83.84982659236758, 91.88567173146166, 97.6466379728137, 76.07898570447814]\n",
      "Epoch 9, Step 0,\tTotal loss: 0.025946,\tHypernet loss: 0.025863\n",
      "\tPSNR: 20.924383411200843 \tSSIM: 0.7064756547956582\n",
      "[62.67930278903573, 84.12229260233111, 92.90634844530773, 99.92018004803926, 74.01309276613256]\n",
      "Epoch 9, Step 1000,\tTotal loss: 0.022185,\tHypernet loss: 0.022084\n",
      "\tPSNR: 20.992445839076595 \tSSIM: 0.709654188395146\n",
      "[59.020861471984304, 79.80984983232158, 87.44496092603869, 93.45207028590602, 70.44444248974412]\n",
      "Epoch 9, Step 2000,\tTotal loss: 0.022871,\tHypernet loss: 0.022801\n",
      "\tPSNR: 21.05545066456874 \tSSIM: 0.7126358779950369\n",
      "[61.87789868284442, 80.38517558087855, 89.16589211144064, 96.84150916912165, 70.95585837537796]\n",
      "Epoch 9, Step 3000,\tTotal loss: 0.026744,\tHypernet loss: 0.026662\n",
      "\tPSNR: 21.115003039410794 \tSSIM: 0.7154502245150282\n",
      "[59.2245292743617, 79.56122186515533, 88.43782210496448, 96.39679497410802, 69.8259263009019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 21.12205434508991\n",
      "SSIM: 0.7157884437304959\n",
      "500000 50000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 17.333896625013352\n",
      "SSIM: 0.5165101732730121\n",
      "\n",
      "epoch:1\n",
      "PSNR: 19.42431392015457\n",
      "SSIM: 0.6292023110756279\n",
      "\n",
      "epoch:2\n",
      "PSNR: 20.25975729696274\n",
      "SSIM: 0.6756539017635584\n",
      "\n",
      "epoch:3\n",
      "PSNR: 20.889496137504576\n",
      "SSIM: 0.709203047825098\n",
      "\n",
      "epoch:4\n",
      "PSNR: 21.383126124019622\n",
      "SSIM: 0.7338261882168055\n",
      "\n",
      "epoch:5\n",
      "PSNR: 21.7910930862236\n",
      "SSIM: 0.7532379069441557\n",
      "\n",
      "epoch:6\n",
      "PSNR: 22.132915849552155\n",
      "SSIM: 0.7686481556618213\n",
      "\n",
      "epoch:7\n",
      "PSNR: 22.425253604450226\n",
      "SSIM: 0.780921643024087\n",
      "\n",
      "epoch:8\n",
      "PSNR: 22.67959805692673\n",
      "SSIM: 0.7910775653767586\n",
      "\n",
      "epoch:9\n",
      "PSNR: 22.901092750091554\n",
      "SSIM: 0.7996035441440343\n"
     ]
    }
   ],
   "source": [
    "steps_til_summary = 1000\n",
    "\n",
    "# optim = torch.optim.Adam(lr=5e-5, params=meta_siren.parameters())\n",
    "optim = torch.optim.Adam(lr=5e-6, params=meta_siren.parameters())\n",
    "\n",
    "hypernet_loss_multiplier = 1\n",
    "\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "for epoch in range(10):\n",
    "#     if epoch < 10:\n",
    "#         hypernet_loss_multiplier += 100\n",
    "\n",
    "    for step, sample in enumerate(dataloader):\n",
    "        sample = dict_to_gpu(sample)\n",
    "        '''\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param\n",
    "        '''\n",
    "        model_output, fast_params, meta_params, pred_specialized_param = meta_siren(sample)    \n",
    "        loss = ((model_output['model_out'] - sample['query']['y'])**2).mean() + hypernet_loss_multiplier * model_output['crossAttHypNet_loss']\n",
    "        \n",
    "#         for name in pred_specialized_param:\n",
    "#             loss += 10*((pred_specialized_param[name] - fast_params[name].detach()) ** 2).mean()\n",
    "        if False:\n",
    "            pred_specialized_param_corrected = OrderedDict()\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "                loss += 1*((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "       \n",
    "        \n",
    "        if (step % steps_til_summary == 0) and (epoch % 1 == 0): \n",
    "            print(\"Epoch %d, Step %d,\\tTotal loss: %0.6f,\\tHypernet loss: %0.6f\" % (epoch, step, loss, model_output['crossAttHypNet_loss']))\n",
    "            print('\\tPSNR:', np.mean(psnr_list), '\\tSSIM:', np.mean(ssim_list))\n",
    "            fig, axes = [], list(range(6))#plt.subplots(1,6, figsize=(36,6))\n",
    "            ax_titles = ['Learned Initialization', 'Inner step 1 output', \n",
    "                        'Inner step 2 output', 'Inner step 3 output', \n",
    "                        'HyperNet output', ## added by me\n",
    "                        'Ground Truth']\n",
    "            images = []\n",
    "            for i, inner_step_out in enumerate(model_output['intermed_predictions']):\n",
    "                img = plot_sample_image(inner_step_out, ax=axes[i])\n",
    "                images += [img]\n",
    "#                 axes[i].set_title(ax_titles[i], fontsize=25)\n",
    "            images += [plot_sample_image(model_output['model_out'], ax=axes[-3])]\n",
    "#             axes[-3].set_title(ax_titles[-3], fontsize=25)\n",
    "\n",
    "            if True:\n",
    "                images += [plot_sample_image(model_output['model_output_hypernet'], ax=axes[-2])]\n",
    "#                 axes[-2].set_title(ax_titles[-2], fontsize=25)\n",
    "\n",
    "            img_ground_truth = plot_sample_image(sample['query']['y'], ax=axes[-1])\n",
    "#             axes[-1].set_title(ax_titles[-1], fontsize=25)\n",
    "            psnrs = [cv2.PSNR(img_ground_truth, img) for img in images]\n",
    "            print(psnrs)\n",
    "            plt.show()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if True:\n",
    "            x = lin2img(model_output['model_output_hypernet']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            x += 1.\n",
    "            x /= 2.\n",
    "            x = torch.clip(x, 0., 1.)\n",
    "            y = lin2img(sample['query']['y']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            y += 1.\n",
    "            y /= 2.\n",
    "            y = torch.clip(y, 0., 1.)\n",
    "    #         print(x.shape, lin2img(x).shape)\n",
    "#             print(y.min(), y.max())\n",
    "#             print(x.min(), x.max())\n",
    "\n",
    "#             print('PSNR:', psnr(y, x).mean())\n",
    "#             print('psnr_sklearn:', psnr_sklearn(y.numpy(), x.numpy(), data_range=1.))\n",
    "#             print('SSIM:', ssim_metric(y, x))\n",
    "            psnr_list += psnr(y, x).cpu().detach().numpy().tolist()\n",
    "            ssim_list += ssim_metric(y, x).cpu().detach().numpy().tolist()\n",
    "        del model_output, fast_params, meta_params, pred_specialized_param\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "print('PSNR:', np.mean(psnr_list))\n",
    "print('SSIM:', np.mean(ssim_list))\n",
    "\n",
    "l = len(psnr_list) // 10\n",
    "print(len(psnr_list), l)\n",
    "for i in range(10):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(img_siren.state_dict(), 'img_siren_original_02.pth')\n",
    "torch.save(crossAttHypNet.state_dict(), 'crossAttHypNet_original_02.pth')\n",
    "torch.save(meta_siren.state_dict(), 'meta_siren_original_02.pth')\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "import json\n",
    "json.dump({'psnr_list': psnr_list, 'ssim_list': ssim_list}, open('psnr_ssim_list_only_hypernet.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 50000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 17.333896625013352\n",
      "SSIM: 0.5165101732730121\n",
      "\n",
      "epoch:1\n",
      "PSNR: 19.42431392015457\n",
      "SSIM: 0.6292023110756279\n",
      "\n",
      "epoch:2\n",
      "PSNR: 20.25975729696274\n",
      "SSIM: 0.6756539017635584\n",
      "\n",
      "epoch:3\n",
      "PSNR: 20.889496137504576\n",
      "SSIM: 0.709203047825098\n",
      "\n",
      "epoch:4\n",
      "PSNR: 21.383126124019622\n",
      "SSIM: 0.7338261882168055\n",
      "\n",
      "epoch:5\n",
      "PSNR: 21.7910930862236\n",
      "SSIM: 0.7532379069441557\n",
      "\n",
      "epoch:6\n",
      "PSNR: 22.132915849552155\n",
      "SSIM: 0.7686481556618213\n",
      "\n",
      "epoch:7\n",
      "PSNR: 22.425253604450226\n",
      "SSIM: 0.780921643024087\n",
      "\n",
      "epoch:8\n",
      "PSNR: 22.67959805692673\n",
      "SSIM: 0.7910775653767586\n",
      "\n",
      "epoch:9\n",
      "PSNR: 22.901092750091554\n",
      "SSIM: 0.7996035441440343\n"
     ]
    }
   ],
   "source": [
    "l = len(psnr_list) // 10\n",
    "print(len(psnr_list), l)\n",
    "for i in range(10):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150000 50000\n",
    "# PSNR: 20.24929603067398\n",
    "# SSIM: 0.6705767016929388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l4Ca0dY2xr2w"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14096/3591317503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# for key in fast_params:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     print(fast_params[key].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmeta_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfast_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'meta_params' is not defined"
     ]
    }
   ],
   "source": [
    "# fast_params['net.0.linear.weight'].requires_grad\n",
    "# fast_params.keys()\n",
    "# fast_params['net.1.linear.weight'].shape \n",
    "# for key in fast_params:\n",
    "#     print(fast_params[key].shape)\n",
    "for key in meta_params:\n",
    "    print(key, ':\\t', meta_params[key].shape, '\\t', fast_params[key].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT1Qy7983EbD"
   },
   "source": [
    "As you can see, after a few hundred steps of training, we can fit any of the Cifar-10 images in only three gradient descent steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Kryu4N59GVaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66906387\n",
      "Files already downloaded and verified\n",
      "Epoch 0, Step 0,\tTotal loss: 0.288681,\tHypernet loss: 0.026553\n",
      "\tPSNR: nan \tSSIM: nan\n",
      "[61.84728651136861, 61.84759025061747, 61.84789328368177, 61.8481922589773, 68.41545647026857]\n",
      "Epoch 0, Step 100,\tTotal loss: 0.270121,\tHypernet loss: 0.034556\n",
      "\tPSNR: 22.64946443796158 \tSSIM: 0.7927339471876621\n",
      "[61.281240750222125, 61.28139641303367, 61.28155844986469, 61.28172177725524, 70.72001592622827]\n",
      "Epoch 0, Step 200,\tTotal loss: 0.269636,\tHypernet loss: 0.025363\n",
      "\tPSNR: 22.619976660013197 \tSSIM: 0.7910506856441498\n",
      "[62.09936936214433, 62.099636892435356, 62.099929118762105, 62.10023776874901, 71.76641496757851]\n",
      "Epoch 0, Step 300,\tTotal loss: 0.345031,\tHypernet loss: 0.018740\n",
      "\tPSNR: 22.615896931091946 \tSSIM: 0.7909908550977707\n",
      "[55.27643431185871, 55.27738697020473, 55.278354251773116, 55.27934252849751, 73.47403456401801]\n",
      "Epoch 0, Step 400,\tTotal loss: 0.330424,\tHypernet loss: 0.023902\n",
      "\tPSNR: 22.614360447824 \tSSIM: 0.7905274169147014\n",
      "[58.52904617901743, 58.52966989080614, 58.530236993159676, 58.53093557325227, 73.47926742676611]\n",
      "Epoch 0, Step 500,\tTotal loss: 0.282074,\tHypernet loss: 0.023036\n",
      "\tPSNR: 22.616045668363572 \tSSIM: 0.7910155400037765\n",
      "[63.60724472876172, 63.60737828375637, 63.607492804392365, 63.6076212444376, 71.67414978535298]\n",
      "Epoch 0, Step 600,\tTotal loss: 0.296951,\tHypernet loss: 0.027300\n",
      "\tPSNR: 22.597444790005685 \tSSIM: 0.7906029165163636\n",
      "[58.56536478649833, 58.565447315671825, 58.56551484244334, 58.565598600783346, 69.51324330279095]\n",
      "Epoch 0, Step 700,\tTotal loss: 0.257087,\tHypernet loss: 0.022567\n",
      "\tPSNR: 22.59580025758062 \tSSIM: 0.7907058585967336\n",
      "[59.82309909162854, 59.82334109955965, 59.82359581196039, 59.82381574788409, 70.05475562469765]\n",
      "Epoch 0, Step 800,\tTotal loss: 0.256120,\tHypernet loss: 0.025346\n",
      "\tPSNR: 22.600627893358467 \tSSIM: 0.790905655734241\n",
      "[58.693783496650994, 58.694168976990994, 58.69450918960632, 58.69491086858437, 69.90344985249916]\n",
      "Epoch 0, Step 900,\tTotal loss: 0.239497,\tHypernet loss: 0.022193\n",
      "\tPSNR: 22.614380193683836 \tSSIM: 0.7910472982211245\n",
      "[64.80325759162402, 64.80344191376103, 64.80361717604818, 64.80375714825577, 72.25044499648442]\n",
      "Epoch 0, Step 1000,\tTotal loss: 0.382918,\tHypernet loss: 0.035429\n",
      "\tPSNR: 22.61052070224285 \tSSIM: 0.7908406700491906\n",
      "[59.84277144230008, 59.843029739779595, 59.8433362310712, 59.84362828829659, 71.11098695068064]\n",
      "Epoch 0, Step 1100,\tTotal loss: 0.330688,\tHypernet loss: 0.029118\n",
      "\tPSNR: 22.60599361755631 \tSSIM: 0.7906028640947559\n",
      "[57.17042127676792, 57.17131302165843, 57.172325306306824, 57.173248759139305, 75.79557098975002]\n",
      "Epoch 0, Step 1200,\tTotal loss: 0.315374,\tHypernet loss: 0.023838\n",
      "\tPSNR: 22.611140957971415 \tSSIM: 0.7907928264327347\n",
      "[60.02999367336666, 60.030224567736724, 60.030462037158614, 60.03069597915308, 73.77321249177943]\n",
      "PSNR: 22.611143434238432\n",
      "SSIM: 0.790929889279604\n",
      "10000 10000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 22.611143434238432\n",
      "SSIM: 0.790929889279604\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "steps_til_summary = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import meta_modules\n",
    "img_siren = Siren(in_features=2, hidden_features=256, hidden_layers=3, out_features=3, outermost_linear=True)\n",
    "# img_siren.load_state_dict(torch.load('img_siren_original_01.pth'))\n",
    "# crossAttHypNet = CrossAttentionHyperNet().cuda()\n",
    "crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                    out_features=3,\n",
    "                                                                    image_resolution=(32, 32))\n",
    "crossAttHypNet.load_state_dict(torch.load('crossAttHypNet_original_01.pth'))\n",
    "meta_siren = MAML(num_meta_steps=3, hypo_module=img_siren.cuda(), crossAttHypNet=crossAttHypNet.cuda(), \n",
    "                  loss=l2_loss, init_lr=1e-5, \n",
    "                  lr_type='per_parameter_per_step').cuda()\n",
    "# meta_siren.load_state_dict(torch.load('meta_siren_original_01.pth'))\n",
    "meta_siren = meta_siren.cuda()\n",
    "meta_siren.eval()\n",
    "\n",
    "dataset = CIFAR10(train=False)\n",
    "test_dataloader = DataLoader(dataset, batch_size=8, num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "# optim = torch.optim.Adam(lr=5e-5, params=meta_siren.parameters())\n",
    "# optim = torch.optim.Adam(lr=5e-6, params=meta_siren.parameters())\n",
    "\n",
    "hypernet_loss_multiplier = 1\n",
    "\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "for epoch in range(1):\n",
    "#     if epoch < 10:\n",
    "#         hypernet_loss_multiplier += 100\n",
    "\n",
    "    for step, sample in enumerate(test_dataloader):\n",
    "        sample = dict_to_gpu(sample)\n",
    "        '''\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param\n",
    "        '''\n",
    "        model_output, fast_params, meta_params, pred_specialized_param = meta_siren(sample)    \n",
    "        loss = ((model_output['model_out'] - sample['query']['y'])**2).mean() + hypernet_loss_multiplier * model_output['crossAttHypNet_loss']\n",
    "        \n",
    "#         for name in pred_specialized_param:\n",
    "#             loss += 10*((pred_specialized_param[name] - fast_params[name].detach()) ** 2).mean()\n",
    "        if False:\n",
    "            pred_specialized_param_corrected = OrderedDict()\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "                loss += 1*((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "       \n",
    "        \n",
    "        if (step % steps_til_summary == 0) and (epoch % 1 == 0): \n",
    "            print(\"Epoch %d, Step %d,\\tTotal loss: %0.6f,\\tHypernet loss: %0.6f\" % (epoch, step, loss, model_output['crossAttHypNet_loss']))\n",
    "            print('\\tPSNR:', np.mean(psnr_list), '\\tSSIM:', np.mean(ssim_list))\n",
    "            fig, axes = [], list(range(6))#plt.subplots(1,6, figsize=(36,6))\n",
    "            ax_titles = ['Learned Initialization', 'Inner step 1 output', \n",
    "                        'Inner step 2 output', 'Inner step 3 output', \n",
    "                        'HyperNet output', ## added by me\n",
    "                        'Ground Truth']\n",
    "            images = []\n",
    "            for i, inner_step_out in enumerate(model_output['intermed_predictions']):\n",
    "                img = plot_sample_image(inner_step_out, ax=axes[i])\n",
    "                images += [img]\n",
    "#                 axes[i].set_title(ax_titles[i], fontsize=25)\n",
    "            images += [plot_sample_image(model_output['model_out'], ax=axes[-3])]\n",
    "#             axes[-3].set_title(ax_titles[-3], fontsize=25)\n",
    "\n",
    "            if True:\n",
    "                images += [plot_sample_image(model_output['model_output_hypernet'], ax=axes[-2])]\n",
    "#                 axes[-2].set_title(ax_titles[-2], fontsize=25)\n",
    "\n",
    "            img_ground_truth = plot_sample_image(sample['query']['y'], ax=axes[-1])\n",
    "#             axes[-1].set_title(ax_titles[-1], fontsize=25)\n",
    "            psnrs = [cv2.PSNR(img_ground_truth, img) for img in images]\n",
    "            images += [img_ground_truth]\n",
    "            pickle.dump({step: images}, open(f'./images/img_original_{step}', 'wb'))\n",
    "            print(psnrs)\n",
    "            plt.show()\n",
    "\n",
    "#         optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "        if True:\n",
    "            x = lin2img(model_output['model_output_hypernet']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            x += 1.\n",
    "            x /= 2.\n",
    "            x = torch.clip(x, 0., 1.)\n",
    "            y = lin2img(sample['query']['y']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            y += 1.\n",
    "            y /= 2.\n",
    "            y = torch.clip(y, 0., 1.)\n",
    "    #         print(x.shape, lin2img(x).shape)\n",
    "#             print(y.min(), y.max())\n",
    "#             print(x.min(), x.max())\n",
    "\n",
    "#             print('PSNR:', psnr(y, x).mean())\n",
    "#             print('psnr_sklearn:', psnr_sklearn(y.numpy(), x.numpy(), data_range=1.))\n",
    "#             print('SSIM:', ssim_metric(y, x))\n",
    "            psnr_list += psnr(y, x).cpu().detach().numpy().tolist()\n",
    "            ssim_list += ssim_metric(y, x).cpu().detach().numpy().tolist()\n",
    "        del model_output, fast_params, meta_params, pred_specialized_param\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "print('PSNR:', np.mean(psnr_list))\n",
    "print('SSIM:', np.mean(ssim_list))\n",
    "\n",
    "l = len(psnr_list) // 1\n",
    "print(len(psnr_list), l)\n",
    "for i in range(1):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MetaSDF_on_Cifar_10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
