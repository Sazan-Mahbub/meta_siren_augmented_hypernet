{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTwikOyWZobA"
   },
   "source": [
    "# MetaSDF & Meta-SIREN\n",
    "\n",
    "This is a colab to explore MetaSDF, and its applications to rapidly fit neural implicit representations.\n",
    "\n",
    "Make sure to switch the runtime type to \"GPU\" under \"Runtime --> Change Runtime Type\"!\n",
    "\n",
    "We will show you how to run two experiments using gradient-based meta-learning: \n",
    "* [Fitting an image in 3 gradient descent steps with SIREN](#section_1)\n",
    "* [Fitting 2D Signed Distance Functions of MNIST digits](#section_2)\n",
    "\n",
    "Let's go! \n",
    "\n",
    "First, the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ49alfvZobQ",
    "outputId": "627391a5-abd0-4101-8a73-66302c056cbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sazan\\AppData\\Local\\Temp/ipykernel_13548/3975817298.py:12: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import OrderedDict, Mapping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scipy.ndimage\n",
    "from torch import nn \n",
    "from collections import OrderedDict, Mapping \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "from torch.nn.init import _calculate_correct_fan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrsG6X9cDsZe",
    "outputId": "a6e98051-c4ed-4948-8475-5c9775f19c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15]) torch.Size([2, 3]) \n",
      ">>\n",
      " tensor([[ 1,  2,  3,  1,  2,  3,  1,  2,  3,  1,  2,  3,  1,  2,  3],\n",
      "        [10, 20, 30, 10, 20, 30, 10, 20, 30, 10, 20, 30, 10, 20, 30]]) \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3]],\n",
      "\n",
      "        [[10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30]]]) tensor([[ 1,  2,  3],\n",
      "        [10, 20, 30]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [10,20,30]]) \n",
    "print(x.repeat(1, 5).shape, x.shape,'\\n>>\\n', x.repeat(1, 5), '\\n', x.repeat(1, 5).view([-1, 5, 3]), x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2D7783s2tg2"
   },
   "source": [
    "For meta-learning, we're using the excellent \"Torchmeta\" library. We have to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eX_d5hsYZtSq",
    "outputId": "4122e3c1-31d0-4e35-e770-fd46c4a5c8e4"
   },
   "outputs": [],
   "source": [
    "# !pip install torchmeta\n",
    "from torchmeta.modules import (MetaModule, MetaSequential, MetaLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je35HXUPZobV"
   },
   "source": [
    "We're now ready to implement a few neural network layers: Fully connected networks, and SIREN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DlEcUhGiZobX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BatchLinear(nn.Linear, MetaModule):\n",
    "    '''A linear meta-layer that can deal with batched weight matrices and biases, as for instance output by a\n",
    "    hypernetwork.'''\n",
    "    __doc__ = nn.Linear.__doc__\n",
    "\n",
    "    def forward(self, input, params=None):\n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "\n",
    "        bias = params.get('bias', None)\n",
    "        weight = params['weight']\n",
    "\n",
    "        output = input.matmul(weight.permute(*[i for i in range(len(weight.shape)-2)], -1, -2))\n",
    "        output += bias.unsqueeze(-2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MetaFC(MetaModule):\n",
    "    '''A fully connected neural network that allows swapping out the weights, either via a hypernetwork\n",
    "    or via MAML.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 num_hidden_layers, hidden_features,\n",
    "                 outermost_linear=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(MetaSequential(\n",
    "            BatchLinear(in_features, hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ))\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, hidden_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        if outermost_linear:\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, out_features),\n",
    "            ))\n",
    "        else:\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        self.net = MetaSequential(*self.net)\n",
    "        self.net.apply(init_weights_normal)\n",
    "\n",
    "    def forward(self, coords, params=None, **kwargs):\n",
    "        '''Simple forward pass without computation of spatial gradients.'''\n",
    "        output = self.net(coords, params=self.get_subdict(params, 'net'))\n",
    "        return output\n",
    "\n",
    "\n",
    "class SineLayer(MetaModule):\n",
    "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
    "\n",
    "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
    "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
    "    # hyperparameter.\n",
    "\n",
    "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
    "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = float(omega_0)\n",
    "\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.linear = BatchLinear(in_features, out_features, bias=bias)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features,\n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
    "                                            np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "\n",
    "    def forward(self, input, params=None):\n",
    "        intermed = self.linear(input, params=self.get_subdict(params, 'linear'))\n",
    "        return torch.sin(self.omega_0 * intermed)\n",
    "\n",
    "\n",
    "class Siren(MetaModule):\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False,\n",
    "                 first_omega_0=30, hidden_omega_0=30., special_first=True):\n",
    "        super().__init__()\n",
    "        self.hidden_omega_0 = hidden_omega_0\n",
    "\n",
    "        layer = SineLayer\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(layer(in_features, hidden_features,\n",
    "                              is_first=special_first, omega_0=first_omega_0))\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(layer(hidden_features, hidden_features,\n",
    "                                  is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = BatchLinear(hidden_features, out_features)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / 30.,\n",
    "                                             np.sqrt(6 / hidden_features) / 30.)\n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(layer(hidden_features, out_features, is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        self.net = nn.ModuleList(self.net)\n",
    "\n",
    "    def forward(self, coords, params=None):\n",
    "        x = coords\n",
    "\n",
    "        for i, layer in enumerate(self.net):\n",
    "            x = layer(x, params=self.get_subdict(params, f'net.{i}'))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def init_weights_normal(m):\n",
    "    if type(m) == BatchLinear or nn.Linear:\n",
    "        if hasattr(m, 'weight'):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if hasattr(m, 'bias'):\n",
    "            m.bias.data.fill_(0.)\n",
    "            \n",
    "            \n",
    "def get_mgrid(sidelen):\n",
    "    # Generate 2D pixel coordinates from an image of sidelen x sidelen\n",
    "    pixel_coords = np.stack(np.mgrid[:sidelen,:sidelen], axis=-1)[None,...].astype(np.float32)\n",
    "    pixel_coords /= sidelen    \n",
    "    pixel_coords -= 0.5\n",
    "    pixel_coords = torch.Tensor(pixel_coords).view(-1, 2)\n",
    "    return pixel_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRN7_GF4jwyT"
   },
   "source": [
    "Sazan: Now let's implement our Cross-Attention Hypernetwork. It will take the image as input and generate some matrices with the same dimension as the weights of the SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i0ai6Ry7jvmE"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from modules_custom import Conv2dResBlock\n",
    "\n",
    "class CrossAttentionHyperNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        L = 64\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=5//2) # padding=kernel_size//2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=5//2) # padding=kernel_size//2\n",
    "        self.conv3 = nn.Conv2d(64, L, 5, padding=5//2)\n",
    "        self.conv4 = nn.Conv2d(L, L, 5, padding=5//2)\n",
    "        # self.conv4_dim_reduction = nn.Conv2d(16+32+64, 64, 1, padding=0)\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 3, 1, 1),\n",
    "#             nn.ReLU(),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             nn.Conv2d(256, 256, 1, 1, 0)\n",
    "#         )\n",
    "#         self.relu_2 = nn.ReLU(inplace=True)\n",
    "#         self.fc = nn.Linear(1024, 1)\n",
    "\n",
    "        if True:\n",
    "            self.fc0 = nn.Linear(L, 2)\n",
    "            self.fc1 = nn.Linear(L, L)\n",
    "            self.fc2 = nn.Linear(L, L)\n",
    "            self.fc3 = nn.Linear(L, L)\n",
    "            self.fc4_1 = nn.Linear(L, 3)\n",
    "            self.fc4_2 = nn.Linear(3, 3)\n",
    "            self.fc4_bias = nn.Linear(L, 3)\n",
    "\n",
    "        if False:\n",
    "            self.weighted_mean = torch.nn.Conv1d(in_channels=64, out_channels=5, kernel_size=1)\n",
    "\n",
    "            self.bias0_fc = nn.Linear(64+64, 64)\n",
    "            self.bias1_fc = nn.Linear(64+64, 64)\n",
    "            self.bias2_fc = nn.Linear(64+64, 64)\n",
    "            self.bias3_fc = nn.Linear(64+64, 64)\n",
    "            self.bias4_fc = nn.Linear(3+64, 3)\n",
    "            \n",
    "            self.attn_bias0_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias1_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias2_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias3_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias4_fc = nn.Linear(3, 1)\n",
    "\n",
    "\n",
    "        self.wt_cross_attn0 = nn.MultiheadAttention(embed_dim=2, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn3 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn4 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        \n",
    "        self.bias_cross_attn0 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn3 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn4 = nn.MultiheadAttention(embed_dim=3, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    net.0.linear.weight :\t torch.Size([64, 2]) \t torch.Size([16, 64, 2])\n",
    "    net.0.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.1.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.1.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.2.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.2.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.3.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.3.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.4.weight :\t torch.Size([3, 64]) \t torch.Size([16, 3, 64])\n",
    "    net.4.bias :\t torch.Size([3]) \t torch.Size([16, 3])\n",
    "    '''\n",
    "    \n",
    "    def forward_conv(self, x):\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        b = x.shape[0]\n",
    "        # print('1>', x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x))) # bx3x32x32 -> bx32x16x16\n",
    "        # print('2>', x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x))) # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        x = F.relu(self.conv3(x)) # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.permute(0, 2, 3, 1).contiguous() # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        # print('3>', x.shape)\n",
    "        # x3 = self.pool(F.relu(self.conv3(x2)))\n",
    "        # x = torch.cat([x1, x2], -1)\n",
    "        # x = F.relu(self.conv4_dim_reduction(x))\n",
    "#         print('>>', x.shape)\n",
    "        x = x.view([b, 64, 64]) # bx8x8x64 -> bx64x64 ## channel last\n",
    "        # print('4>', x.shape)\n",
    "        \n",
    "        x0 = F.relu(self.fc0(x)) # bx64x64 -> bx64x2\n",
    "        x1 = F.relu(self.fc1(x)) # bx64x64 -> bx64x64\n",
    "        x2 = F.relu(self.fc2(x)) # bx64x64 -> bx64x64\n",
    "        x3 = F.relu(self.fc3(x)) # bx64x64 -> bx64x64\n",
    "        x4 = F.relu(self.fc4_1(x.permute(0,2,1))) # bx64x64 -> bx64x64 -> bx64x3\n",
    "        x4 = F.relu(self.fc4_2(x4)).permute(0,2,1).contiguous() # bx64x3 -> bx64x3 -> bx3x64\n",
    "\n",
    "        # x_biases = F.relu(self.weighted_mean(x)) # bx64x64 -> bx5x64\n",
    "        # x0_bias = F.relu(self.bias0_fc(x_biases[:, 0, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x1_bias = F.relu(self.bias1_fc(x_biases[:, 1, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x2_bias = F.relu(self.bias2_fc(x_biases[:, 2, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x3_bias = F.relu(self.bias3_fc(x_biases[:, 3, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x4_bias = F.relu(self.bias4_fc(x_biases[:, 4, :])) # bx5x64 ->  bx64 -> bx3\n",
    "\n",
    "        return x, x0, x1, x2, x3, x4#, x0_bias, x1_bias, x2_bias, x3_bias, x4_bias\n",
    "    \n",
    "    def bias_attention(self, x, meta_param_bias, bias_fc, attn_bias_fc):\n",
    "        b, c = meta_param_bias.shape\n",
    "        meta_param_bias = meta_param_bias.view(b, 1, c)\n",
    "        param_bias = torch.cat([meta_param_bias.repeat(1,64,1), x], -1) # [bx1xc, bx64x64] -> [bx64xc, bx64x64] -> bx64x(c+64)\n",
    "        param_bias = F.relu(bias_fc(param_bias)) # bx64x(c+64) -> bx64xc\n",
    "        attention_scores = F.relu(attn_bias_fc(param_bias)).permute(0,2,1) # bx64xc -> bx64x1 -> bx1x64\n",
    "        attention_scores = F.softmax(attention_scores, -1) # bx1x64 -> bx1x64\n",
    "        param_bias = torch.bmm(attention_scores, param_bias).view(b, c) # [bx1x64, bx64xc] -> bx1xc -> bxc\n",
    "        return param_bias\n",
    "\n",
    "    def compute_biases(self, x, meta_params):\n",
    "        x0_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.0.linear.bias'], bias_fc=self.bias0_fc, attn_bias_fc=self.attn_bias0_fc)\n",
    "        x1_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.1.linear.bias'], bias_fc=self.bias1_fc, attn_bias_fc=self.attn_bias1_fc)\n",
    "        x2_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.2.linear.bias'], bias_fc=self.bias2_fc, attn_bias_fc=self.attn_bias2_fc)\n",
    "        x3_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.3.linear.bias'], bias_fc=self.bias3_fc, attn_bias_fc=self.attn_bias3_fc)\n",
    "        x4_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.4.bias'], bias_fc=self.bias4_fc, attn_bias_fc=self.attn_bias4_fc)\n",
    "        return x0_bias, x1_bias, x2_bias, x3_bias, x4_bias \n",
    "\n",
    "    def compute_loss(self, specialized_param, gt_specialized_param):\n",
    "        loss = 0.\n",
    "        for key in specialized_param:\n",
    "            loss += F.mse_loss(input=specialized_param[key], target=gt_specialized_param[key])\n",
    "        loss /= 10 # not sure if it will help\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, meta_params):\n",
    "        x, x0, x1, x2, x3, x4 = self.forward_conv(x) \n",
    "        # x0_bias, x1_bias, x2_bias, x3_bias, x4_bias = self.compute_biases(x, meta_params)\n",
    "\n",
    "        # x = self.forward_conv(x) # bx32x32x3 -> bx64x64\n",
    "        b, l, c = x.shape\n",
    "        # query -> from meta model ==> meta_params\n",
    "        # key and value -> from this model ==> x\n",
    "        specialized_param = OrderedDict()\n",
    "\n",
    "        # print('1>', meta_params['net.4.bias'].shape, x.shape)\n",
    "        # x_in = self.fc0(x)\n",
    "        x_out = F.relu(self.fc4_bias(x))\n",
    "        specialized_param['net.0.linear.weight']  = self.wt_cross_attn0(query=meta_params['net.0.linear.weight'], key=x0, value=x0)[0]\n",
    "        specialized_param['net.0.linear.bias']  = self.bias_cross_attn0(query=meta_params['net.0.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.1.linear.weight']  = self.wt_cross_attn1(query=meta_params['net.1.linear.weight'], key=x1, value=x1)[0]\n",
    "        specialized_param['net.1.linear.bias']  = self.bias_cross_attn1(query=meta_params['net.1.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.2.linear.weight']  = self.wt_cross_attn2(query=meta_params['net.2.linear.weight'], key=x2, value=x2)[0]\n",
    "        specialized_param['net.2.linear.bias']  = self.bias_cross_attn2(query=meta_params['net.2.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.3.linear.weight']  = self.wt_cross_attn3(query=meta_params['net.3.linear.weight'], key=x3, value=x3)[0]\n",
    "        specialized_param['net.3.linear.bias']  = self.bias_cross_attn3(query=meta_params['net.3.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.4.weight']  = self.wt_cross_attn4(query=meta_params['net.4.weight'], key=x4, value=x4)[0]\n",
    "        specialized_param['net.4.bias']  = self.bias_cross_attn4(query=meta_params['net.4.bias'].view(b, 1, -1), key=x_out, value=x_out)[0].view(b, -1)\n",
    "\n",
    "#         loss = self.compute_loss(specialized_param, gt_specialized_param)\n",
    "\n",
    "#         return loss, specialized_param\n",
    "        return specialized_param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhPvd2TUkZC8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cPO4PWDZobZ"
   },
   "source": [
    "Now, we implement MAML. The important parts of the code are commented, so it's easy to understand how each part works! Start by looking at the \"forward\" function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OL4dgzRiZoba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(prediction, gt):\n",
    "    return ((prediction - gt)**2).mean()\n",
    "\n",
    "\n",
    "class MAML(nn.Module):\n",
    "    def __init__(self, num_meta_steps, hypo_module, crossAttHypNet, loss, init_lr,\n",
    "                 lr_type='static', first_order=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hypo_module = hypo_module # The module who's weights we want to meta-learn.\n",
    "        self.crossAttHypNet = crossAttHypNet\n",
    "        self.first_order = first_order\n",
    "        self.loss = loss\n",
    "        self.lr_type = lr_type\n",
    "        self.log = []\n",
    "\n",
    "        self.register_buffer('num_meta_steps', torch.Tensor([num_meta_steps]).int())\n",
    "\n",
    "        if self.lr_type == 'static': \n",
    "            self.register_buffer('lr', torch.Tensor([init_lr]))\n",
    "        elif self.lr_type == 'global':\n",
    "            self.lr = nn.Parameter(torch.Tensor([init_lr]))\n",
    "        elif self.lr_type == 'per_step':\n",
    "            self.lr = nn.ParameterList([nn.Parameter(torch.Tensor([init_lr]))\n",
    "                                        for _ in range(num_meta_steps)])\n",
    "        elif self.lr_type == 'per_parameter': # As proposed in \"Meta-SGD\".\n",
    "            self.lr = nn.ParameterList([])\n",
    "            hypo_parameters = hypo_module.parameters()\n",
    "            for param in hypo_parameters:\n",
    "                self.lr.append(nn.Parameter(torch.ones(param.size()) * init_lr))\n",
    "        elif self.lr_type == 'per_parameter_per_step':\n",
    "            self.lr = nn.ModuleList([])\n",
    "            for name, param in hypo_module.meta_named_parameters():\n",
    "                self.lr.append(nn.ParameterList([nn.Parameter(torch.ones(param.size()) * init_lr)\n",
    "                                                 for _ in range(num_meta_steps)]))\n",
    "\n",
    "        param_count = 0\n",
    "        for param in self.parameters():\n",
    "            param_count += np.prod(param.shape)\n",
    "\n",
    "        print(param_count)\n",
    "\n",
    "    def _update_step(self, loss, param_dict, step):\n",
    "        grads = torch.autograd.grad(loss, param_dict.values(),\n",
    "                                    create_graph=False if self.first_order else True)\n",
    "        params = OrderedDict()\n",
    "        for i, ((name, param), grad) in enumerate(zip(param_dict.items(), grads)):\n",
    "            if self.lr_type in ['static', 'global']:\n",
    "                lr = self.lr\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_step']:\n",
    "                lr = self.lr[step]\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_parameter']:\n",
    "                lr = self.lr[i]\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_parameter_per_step']:\n",
    "                lr = self.lr[i][step]\n",
    "                params[name] = param - lr * grad\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return params, grads\n",
    "\n",
    "    def forward_with_params(self, query_x, fast_params, **kwargs):\n",
    "        output = self.hypo_module(query_x, params=fast_params)\n",
    "        return output\n",
    "\n",
    "    def generate_params(self, context_dict):\n",
    "        \"\"\"Specializes the model\"\"\"\n",
    "        x = context_dict.get('x').cuda()\n",
    "        y = context_dict.get('y').cuda()\n",
    "\n",
    "        meta_batch_size = x.shape[0]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            # First, replicate the initialization for each batch item.\n",
    "            # This is the learned initialization, i.e., in the outer loop,\n",
    "            # the gradients are backpropagated all the way into the \n",
    "            # \"meta_named_parameters\" of the hypo_module.\n",
    "            fast_params = OrderedDict()\n",
    "            meta_params = OrderedDict()\n",
    "            for name, param in self.hypo_module.meta_named_parameters():\n",
    "                fast_params[name] = param[None, ...].repeat((meta_batch_size,) + (1,) * len(param.shape))\n",
    "                meta_params[name] = param[None, ...].repeat((meta_batch_size,) + (1,) * len(param.shape))\n",
    "\n",
    "            prev_loss = 1e6\n",
    "            intermed_predictions = []\n",
    "            for j in range(self.num_meta_steps):\n",
    "                # Using the current set of parameters, perform a forward pass with the context inputs.\n",
    "                predictions = self.hypo_module(x, params=fast_params)\n",
    "\n",
    "                # Compute the loss on the context labels.\n",
    "                loss = self.loss(predictions, y)\n",
    "                intermed_predictions.append(predictions)\n",
    "\n",
    "                if loss > prev_loss:\n",
    "                    print('inner lr too high?')\n",
    "                \n",
    "                # Using the computed loss, update the fast parameters.\n",
    "                fast_params, grads = self._update_step(loss, fast_params, j)\n",
    "                prev_loss = loss\n",
    "\n",
    "        return fast_params, intermed_predictions, meta_params\n",
    "\n",
    "    def forward(self, meta_batch, **kwargs):\n",
    "        # The meta_batch conists of the \"context\" set (the observations we're conditioning on)\n",
    "        # and the \"query\" inputs (the points where we want to evaluate the specialized model)\n",
    "        context = meta_batch['context']\n",
    "        query_x = meta_batch['query']['x'].cuda()\n",
    "\n",
    "        # Specialize the model with the \"generate_params\" function.\n",
    "        fast_params, intermed_predictions, meta_params = self.generate_params(context)\n",
    "        pred_specialized_param = self.crossAttHypNet(x=lin2img(context['y']).cuda())#, meta_params=meta_params, gt_specialized_param=fast_params)\n",
    "\n",
    "        pred_specialized_param_corrected = OrderedDict()\n",
    "\n",
    "        crossAttHypNet_loss = 0.\n",
    "        if True:\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "#                 crossAttHypNet_loss += ((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "                \n",
    "        # Compute the final outputs. \n",
    "        model_output = self.hypo_module(query_x, params=fast_params)\n",
    "        model_output_hypernet = self.hypo_module(query_x, params=pred_specialized_param_corrected)\n",
    "        crossAttHypNet_loss += self.loss(model_output_hypernet, context['y'])\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i0VRikaZobc"
   },
   "source": [
    "<a id='section_1'></a>\n",
    "## Learning to fit images in 3 gradient descent steps\n",
    "\n",
    "By learning an initialization for SIREN, we may fit any image in as few as 3 gradient descent steps! \n",
    "This has also been noted by Tancik et al. in \"Learned Initializations for Optimizing Coordinate-Based Neural Representations\" (2020).\n",
    "\n",
    "We'll demonstrate here with Cifar-10, but it works just as well with CelebA or imagenet - try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gKFj5-FVZobc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR10():\n",
    "    def __init__(self, train=True):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        self.dataset = torchvision.datasets.CIFAR10(root='./data', train=train,\n",
    "                                                download=True, transform=transform)\n",
    "        \n",
    "        self.length = len(self.dataset)\n",
    "        self.meshgrid = get_mgrid(sidelen=32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img, _ = self.dataset[item]\n",
    "        img_flat = img.permute(1,2,0).view(-1, 3)\n",
    "        return {'context':{'x':self.meshgrid, 'y':img_flat}, \n",
    "                'query':{'x':self.meshgrid, 'y':img_flat}}\n",
    "\n",
    "\n",
    "def lin2img(tensor):\n",
    "    batch_size, num_samples, channels = tensor.shape\n",
    "    sidelen = np.sqrt(num_samples).astype(int)\n",
    "    return tensor.view(batch_size, sidelen, sidelen, channels).squeeze(-1)\n",
    "\n",
    "    \n",
    "def plot_sample_image(img_batch, ax):\n",
    "    img = lin2img(img_batch)[0].detach().cpu().numpy()\n",
    "    img += 1\n",
    "    img /= 2.\n",
    "    img = np.clip(img, 0., 1.)\n",
    "#     ax.set_axis_off()\n",
    "#     ax.imshow(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def dict_to_gpu(ob):\n",
    "    if isinstance(ob, Mapping):\n",
    "        return {k: dict_to_gpu(v) for k, v in ob.items()}\n",
    "    else:\n",
    "        return ob.cuda()    \n",
    "\n",
    "\n",
    "# def dict_to_gpu(ob):\n",
    "#     if isinstance(ob, Mapping):\n",
    "#         return {k: dict_to_gpu(v) for k, v in ob.items()}\n",
    "#     else:\n",
    "#         return ob.cuda()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t9tBE4EZobd"
   },
   "source": [
    "Now, let's initialize our models and our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKLBYi25Zobd",
    "outputId": "b6694221-afa4-472e-a572-198500159e14",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66906387\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import meta_modules\n",
    "img_siren = Siren(in_features=2, hidden_features=256, hidden_layers=3, out_features=3, outermost_linear=True)\n",
    "img_siren.load_state_dict(torch.load('img_siren_1.pth'))\n",
    "# crossAttHypNet = CrossAttentionHyperNet().cuda()\n",
    "crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                    out_features=3,\n",
    "                                                                    image_resolution=(32, 32))\n",
    "crossAttHypNet.load_state_dict(torch.load('crossAttHypNet_1.pth'))\n",
    "meta_siren = MAML(num_meta_steps=3, hypo_module=img_siren.cuda(), crossAttHypNet=crossAttHypNet.cuda(), \n",
    "                  loss=l2_loss, init_lr=1e-5, \n",
    "                  lr_type='per_parameter_per_step').cuda()\n",
    "meta_siren.load_state_dict(torch.load('meta_siren_1.pth'))\n",
    "meta_siren = meta_siren.cuda()\n",
    "if True:\n",
    "    del crossAttHypNet\n",
    "    torch.cuda.empty_cache()\n",
    "    crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                        out_features=3,\n",
    "                                                                        image_resolution=(32, 32))\n",
    "    meta_siren.crossAttHypNet = crossAttHypNet.cuda()\n",
    "meta_siren.train()\n",
    "\n",
    "dataset = CIFAR10()\n",
    "dataloader = DataLoader(dataset, batch_size=16, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lJQ3Fdr-zvIB"
   },
   "outputs": [],
   "source": [
    "# crossAttHypNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5dv0PYqRZlN",
    "outputId": "8a79253e-e5de-4e2b-8c11-147675e8b438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "img1 = cv2.imread('img1.bmp')\n",
    "img2 = cv2.imread('img2.bmp')\n",
    "psnr = cv2.PSNR(img1, img2)\n",
    "psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install piqa\n",
    "from metrics import psnr, ssim_metric\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_sklearn\n",
    "\n",
    "# print('PSNR:', psnr.psnr(x, y))\n",
    "# print('SSIM:', ssim.ssim(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS7Juv9HZobf"
   },
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "pzmwE0_KZobg",
    "outputId": "96cc34be-e0a5-41f8-f455-1c68d3579309",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0,\tTotal loss: 0.518580,\tHypernet loss: 0.518012\n",
      "\tPSNR: nan \tSSIM: nan\n",
      "[59.876506636505404, 78.7269381470984, 83.68297725917375, 87.588963965828, 56.75914218848668]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 1000,\tTotal loss: 0.074652,\tHypernet loss: 0.073969\n",
      "\tPSNR: 15.927075093120337 \tSSIM: 0.4403425267010462\n",
      "[58.82033031420056, 78.864712070074, 83.99351684846891, 87.63760817980142, 69.20961457105368]\n",
      "Epoch 0, Step 2000,\tTotal loss: 0.060942,\tHypernet loss: 0.060344\n",
      "\tPSNR: 16.904958082303406 \tSSIM: 0.4892079942239216\n",
      "[61.60207640332832, 80.45978591791422, 85.95120572071065, 90.89382674972292, 70.57629758508355]\n",
      "Epoch 0, Step 3000,\tTotal loss: 0.063204,\tHypernet loss: 0.062580\n",
      "\tPSNR: 17.479792918016514 \tSSIM: 0.5194242985604797\n",
      "[62.272151653707795, 79.84503590742415, 84.88721689869931, 88.77962133574624, 68.55148974541669]\n",
      "Epoch 1, Step 0,\tTotal loss: 0.056490,\tHypernet loss: 0.055666\n",
      "\tPSNR: 17.5385835608387 \tSSIM: 0.5225042154034227\n",
      "[60.045392617014926, 75.46822899094363, 79.26613330773102, 82.30939543957538, 66.79757259342227]\n",
      "Epoch 1, Step 1000,\tTotal loss: 0.053231,\tHypernet loss: 0.052596\n",
      "\tPSNR: 17.94962364489382 \tSSIM: 0.5452010524351376\n",
      "[58.69990915316586, 77.23110724523954, 82.59363624784734, 87.9267413410834, 68.58764694155845]\n",
      "Epoch 1, Step 2000,\tTotal loss: 0.050572,\tHypernet loss: 0.049685\n",
      "\tPSNR: 18.2855748199428 \tSSIM: 0.5635563019061143\n",
      "[60.96260149627785, 75.41093315046555, 79.69064686661854, 83.70996994465024, 66.19550805386245]\n",
      "Epoch 1, Step 3000,\tTotal loss: 0.039471,\tHypernet loss: 0.038977\n",
      "\tPSNR: 18.560026558472185 \tSSIM: 0.5790694385585661\n",
      "[59.547270117200966, 79.40500294324478, 85.30211000784483, 90.65773896384285, 69.49743918803287]\n",
      "Epoch 2, Step 0,\tTotal loss: 0.041135,\tHypernet loss: 0.040589\n",
      "\tPSNR: 18.590679569745063 \tSSIM: 0.580935008537881\n",
      "[59.7903125741033, 76.4787524600659, 81.36445491188016, 85.64133931778824, 67.50255756400607]\n",
      "Epoch 2, Step 1000,\tTotal loss: 0.038252,\tHypernet loss: 0.037708\n",
      "\tPSNR: 18.833752794763136 \tSSIM: 0.5946264373825477\n",
      "[60.117100736373466, 75.0107659463369, 79.4343121432106, 83.19837705841935, 67.89519975731396]\n",
      "Epoch 2, Step 2000,\tTotal loss: 0.036796,\tHypernet loss: 0.036287\n",
      "\tPSNR: 19.040881831664027 \tSSIM: 0.606203508909312\n",
      "[60.96115031510579, 78.38335971548725, 85.01154771159273, 90.79334701080428, 68.3430401411645]\n",
      "Epoch 2, Step 3000,\tTotal loss: 0.040815,\tHypernet loss: 0.040203\n",
      "\tPSNR: 19.224738427307155 \tSSIM: 0.6165867537274906\n",
      "[65.27530303586265, 79.70170805337638, 84.68425697329573, 89.50096354247663, 71.62288219655976]\n",
      "Epoch 3, Step 0,\tTotal loss: 0.040150,\tHypernet loss: 0.039644\n",
      "\tPSNR: 19.2462318750604 \tSSIM: 0.6177957852241148\n",
      "[62.311090904996746, 80.41685365618166, 85.90802750262638, 90.64206164984412, 71.34443224955605]\n",
      "Epoch 3, Step 1000,\tTotal loss: 0.030987,\tHypernet loss: 0.030401\n",
      "\tPSNR: 19.421126972046242 \tSSIM: 0.627481112620984\n",
      "[63.84600485090819, 81.06537796478688, 86.67376950371701, 91.35507766586872, 71.42584335535828]\n",
      "Epoch 3, Step 2000,\tTotal loss: 0.034071,\tHypernet loss: 0.033666\n",
      "\tPSNR: 19.577866147835177 \tSSIM: 0.6360501163600852\n",
      "[58.56133800486781, 73.32692392054122, 79.86367906871463, 86.23039736597953, 64.26754817343324]\n",
      "Epoch 3, Step 3000,\tTotal loss: 0.026067,\tHypernet loss: 0.025655\n",
      "\tPSNR: 19.717952016577577 \tSSIM: 0.6437542186372471\n",
      "[61.834085828436784, 82.39324653389289, 87.60566149413206, 91.95581617472541, 73.80689358731259]\n",
      "Epoch 4, Step 0,\tTotal loss: 0.035805,\tHypernet loss: 0.035172\n",
      "\tPSNR: 19.735176231582166 \tSSIM: 0.6446920476547442\n",
      "[57.65301933390434, 77.65693791436512, 83.90087631036592, 89.09610927543342, 69.79794260613221]\n",
      "Epoch 4, Step 1000,\tTotal loss: 0.029462,\tHypernet loss: 0.029020\n",
      "\tPSNR: 19.874178789604592 \tSSIM: 0.6521526964521208\n",
      "[56.899868876734246, 75.4527995564064, 81.7711235687372, 87.04459749952157, 71.01144760880084]\n",
      "Epoch 4, Step 2000,\tTotal loss: 0.028883,\tHypernet loss: 0.028508\n",
      "\tPSNR: 19.9987080058661 \tSSIM: 0.6587886295646654\n",
      "[58.89066048837336, 76.68443017565268, 82.5929048362342, 87.4421694030395, 68.7654253619276]\n",
      "Epoch 4, Step 3000,\tTotal loss: 0.023600,\tHypernet loss: 0.023238\n",
      "\tPSNR: 20.114757493244063 \tSSIM: 0.6649559303760498\n",
      "[61.60376788125075, 76.42210699290035, 79.94430323043778, 83.0782129289935, 69.11853243209838]\n",
      "Epoch 5, Step 0,\tTotal loss: 0.030263,\tHypernet loss: 0.029801\n",
      "\tPSNR: 20.128834104066847 \tSSIM: 0.6656978119571656\n",
      "[61.868549968227605, 78.84270520513743, 85.14427888028959, 90.60439284743374, 71.14807410422233]\n",
      "Epoch 5, Step 1000,\tTotal loss: 0.025693,\tHypernet loss: 0.025325\n",
      "\tPSNR: 20.242145289663085 \tSSIM: 0.671595622712102\n",
      "[66.16912335116407, 83.15369259684331, 87.78901039631927, 90.84575721104657, 76.28723624017576]\n",
      "Epoch 5, Step 2000,\tTotal loss: 0.030360,\tHypernet loss: 0.029831\n",
      "\tPSNR: 20.347785204398715 \tSSIM: 0.6770249665089295\n",
      "[60.754290232043566, 79.7031029406648, 85.88952111916713, 90.82711100494299, 71.56774486033356]\n",
      "Epoch 5, Step 3000,\tTotal loss: 0.025255,\tHypernet loss: 0.024976\n",
      "\tPSNR: 20.444724077619963 \tSSIM: 0.6820366403761902\n",
      "[61.605280112467426, 78.73031636402561, 85.21793780374269, 90.45023544212702, 69.50016945908745]\n",
      "Epoch 6, Step 0,\tTotal loss: 0.027766,\tHypernet loss: 0.027215\n",
      "\tPSNR: 20.456503459865253 \tSSIM: 0.6826469202688709\n",
      "[59.29058856704123, 77.26326893731388, 82.16695832203933, 86.76137286469512, 69.93538344043064]\n",
      "Epoch 6, Step 1000,\tTotal loss: 0.031563,\tHypernet loss: 0.031081\n",
      "\tPSNR: 20.55408470854729 \tSSIM: 0.6875479882127853\n",
      "[59.59154911703554, 76.25197606653191, 83.06461854150905, 89.93235679651201, 68.2135155520076]\n",
      "Epoch 6, Step 2000,\tTotal loss: 0.028942,\tHypernet loss: 0.028561\n",
      "\tPSNR: 20.643062512313026 \tSSIM: 0.692003828453046\n",
      "[59.29306693846728, 76.55455170855035, 82.90098604522406, 89.36906310579133, 68.77664414072116]\n",
      "Epoch 6, Step 3000,\tTotal loss: 0.021828,\tHypernet loss: 0.021557\n",
      "\tPSNR: 20.726289929407766 \tSSIM: 0.6961675128425825\n",
      "[63.503942810903034, 85.96163245111416, 93.26454990704892, 98.18997154436958, 77.32758175608717]\n",
      "Epoch 7, Step 0,\tTotal loss: 0.027571,\tHypernet loss: 0.027245\n",
      "\tPSNR: 20.736136629007884 \tSSIM: 0.6966621318406186\n",
      "[59.798103253214634, 75.8536528879599, 81.96079453806519, 87.72759968794752, 68.50147768959735]\n",
      "Epoch 7, Step 1000,\tTotal loss: 0.022615,\tHypernet loss: 0.022268\n",
      "\tPSNR: 20.82099446870851 \tSSIM: 0.700758574233141\n",
      "[61.131705652145385, 78.98680117363818, 85.2017885205529, 90.09026099941741, 70.12519905441874]\n",
      "Epoch 7, Step 2000,\tTotal loss: 0.031237,\tHypernet loss: 0.030495\n",
      "\tPSNR: 20.898821452354262 \tSSIM: 0.7045562574133492\n",
      "[59.58847884757002, 77.15144502284163, 82.9136341019088, 87.50901447212635, 69.77540032171832]\n",
      "Epoch 7, Step 3000,\tTotal loss: 0.020083,\tHypernet loss: 0.019850\n",
      "\tPSNR: 20.96997021446156 \tSSIM: 0.7080802158743573\n",
      "[58.258363564767876, 76.8544742021596, 83.3335589812348, 89.07723322922125, 69.27317309689498]\n",
      "Epoch 8, Step 0,\tTotal loss: 0.023948,\tHypernet loss: 0.023654\n",
      "\tPSNR: 20.97905617168069 \tSSIM: 0.7085111737614405\n",
      "[63.23362800074966, 78.8029559249597, 84.42800519766624, 88.60196535916185, 70.77832943147983]\n",
      "Epoch 8, Step 1000,\tTotal loss: 0.025329,\tHypernet loss: 0.024910\n",
      "\tPSNR: 21.054105874690872 \tSSIM: 0.7120205758460708\n",
      "[62.785997633645906, 81.5116701143813, 87.4032330942485, 91.96192325677336, 73.79529171967289]\n",
      "Epoch 8, Step 2000,\tTotal loss: 0.019861,\tHypernet loss: 0.019627\n",
      "\tPSNR: 21.122023291248965 \tSSIM: 0.7152830946611293\n",
      "[60.37677434055044, 80.48956087003008, 86.5624592225468, 91.82479504255025, 72.56747171235585]\n",
      "Epoch 8, Step 3000,\tTotal loss: 0.021753,\tHypernet loss: 0.021461\n",
      "\tPSNR: 21.18639932975705 \tSSIM: 0.7183512219043166\n",
      "[58.72786650403561, 77.38193002248593, 83.73923153214454, 89.44334238104842, 69.9744782395367]\n",
      "Epoch 9, Step 0,\tTotal loss: 0.024896,\tHypernet loss: 0.024488\n",
      "\tPSNR: 21.194160231866835 \tSSIM: 0.7187297952680042\n",
      "[60.073359024534724, 80.90546489494677, 87.86181532668994, 94.01863302467986, 72.80880767037326]\n",
      "Epoch 9, Step 1000,\tTotal loss: 0.021217,\tHypernet loss: 0.020878\n",
      "\tPSNR: 21.260973121283904 \tSSIM: 0.7217977951761463\n",
      "[62.510135971413604, 82.85412502712875, 89.96657315091007, 95.82220563119327, 74.03003231566915]\n",
      "Epoch 9, Step 2000,\tTotal loss: 0.021751,\tHypernet loss: 0.021448\n",
      "\tPSNR: 21.32251754560807 \tSSIM: 0.724655277796391\n",
      "[63.48901046041938, 80.70326904470336, 86.8216048631088, 91.77482196947834, 74.1415343197771]\n",
      "Epoch 9, Step 3000,\tTotal loss: 0.015844,\tHypernet loss: 0.015629\n",
      "\tPSNR: 21.380870554942202 \tSSIM: 0.7273540682132809\n",
      "[64.3987129257458, 80.62409266194507, 87.64186876512761, 92.7581189958978, 71.48036005258868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 21.38791110755825\n",
      "SSIM: 0.7276882169741019\n",
      "500000 50000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 17.5385835608387\n",
      "SSIM: 0.5225042154034227\n",
      "\n",
      "epoch:1\n",
      "PSNR: 19.642775578651428\n",
      "SSIM: 0.6393658016723395\n",
      "\n",
      "epoch:2\n",
      "PSNR: 20.55733648569107\n",
      "SSIM: 0.6915173385965824\n",
      "\n",
      "epoch:3\n",
      "PSNR: 21.20200930114746\n",
      "SSIM: 0.7253808349466324\n",
      "\n",
      "epoch:4\n",
      "PSNR: 21.703465594005586\n",
      "SSIM: 0.749720869166851\n",
      "\n",
      "epoch:5\n",
      "PSNR: 22.09485023885727\n",
      "SSIM: 0.7673924618273974\n",
      "\n",
      "epoch:6\n",
      "PSNR: 22.413935643863677\n",
      "SSIM: 0.7807534012711048\n",
      "\n",
      "epoch:7\n",
      "PSNR: 22.67949297039032\n",
      "SSIM: 0.7914544672071934\n",
      "\n",
      "epoch:8\n",
      "PSNR: 22.914992713356018\n",
      "SSIM: 0.8004787673205137\n",
      "\n",
      "epoch:9\n",
      "PSNR: 23.131668988780977\n",
      "SSIM: 0.8083140123289824\n"
     ]
    }
   ],
   "source": [
    "steps_til_summary = 1000\n",
    "\n",
    "# optim = torch.optim.Adam(lr=5e-5, params=meta_siren.parameters())\n",
    "optim = torch.optim.Adam(lr=5e-6, params=meta_siren.parameters())\n",
    "\n",
    "hypernet_loss_multiplier = 1\n",
    "\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "for epoch in range(10):\n",
    "#     if epoch < 10:\n",
    "#         hypernet_loss_multiplier += 100\n",
    "\n",
    "    for step, sample in enumerate(dataloader):\n",
    "        sample = dict_to_gpu(sample)\n",
    "        '''\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param\n",
    "        '''\n",
    "        model_output, fast_params, meta_params, pred_specialized_param = meta_siren(sample)    \n",
    "        loss = ((model_output['model_out'] - sample['query']['y'])**2).mean() + hypernet_loss_multiplier * model_output['crossAttHypNet_loss']\n",
    "        \n",
    "#         for name in pred_specialized_param:\n",
    "#             loss += 10*((pred_specialized_param[name] - fast_params[name].detach()) ** 2).mean()\n",
    "        if False:\n",
    "            pred_specialized_param_corrected = OrderedDict()\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "                loss += 1*((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "       \n",
    "        \n",
    "        if (step % steps_til_summary == 0) and (epoch % 1 == 0): \n",
    "            print(\"Epoch %d, Step %d,\\tTotal loss: %0.6f,\\tHypernet loss: %0.6f\" % (epoch, step, loss, model_output['crossAttHypNet_loss']))\n",
    "            print('\\tPSNR:', np.mean(psnr_list), '\\tSSIM:', np.mean(ssim_list))\n",
    "            fig, axes = [], list(range(6))#plt.subplots(1,6, figsize=(36,6))\n",
    "            ax_titles = ['Learned Initialization', 'Inner step 1 output', \n",
    "                        'Inner step 2 output', 'Inner step 3 output', \n",
    "                        'HyperNet output', ## added by me\n",
    "                        'Ground Truth']\n",
    "            images = []\n",
    "            for i, inner_step_out in enumerate(model_output['intermed_predictions']):\n",
    "                img = plot_sample_image(inner_step_out, ax=axes[i])\n",
    "                images += [img]\n",
    "#                 axes[i].set_title(ax_titles[i], fontsize=25)\n",
    "            images += [plot_sample_image(model_output['model_out'], ax=axes[-3])]\n",
    "#             axes[-3].set_title(ax_titles[-3], fontsize=25)\n",
    "\n",
    "            if True:\n",
    "                images += [plot_sample_image(model_output['model_output_hypernet'], ax=axes[-2])]\n",
    "#                 axes[-2].set_title(ax_titles[-2], fontsize=25)\n",
    "\n",
    "            img_ground_truth = plot_sample_image(sample['query']['y'], ax=axes[-1])\n",
    "#             axes[-1].set_title(ax_titles[-1], fontsize=25)\n",
    "            psnrs = [cv2.PSNR(img_ground_truth, img) for img in images]\n",
    "            print(psnrs)\n",
    "            plt.show()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if True:\n",
    "            x = lin2img(model_output['model_output_hypernet']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            x += 1.\n",
    "            x /= 2.\n",
    "            x = torch.clip(x, 0., 1.)\n",
    "            y = lin2img(sample['query']['y']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            y += 1.\n",
    "            y /= 2.\n",
    "            y = torch.clip(y, 0., 1.)\n",
    "    #         print(x.shape, lin2img(x).shape)\n",
    "#             print(y.min(), y.max())\n",
    "#             print(x.min(), x.max())\n",
    "\n",
    "#             print('PSNR:', psnr(y, x).mean())\n",
    "#             print('psnr_sklearn:', psnr_sklearn(y.numpy(), x.numpy(), data_range=1.))\n",
    "#             print('SSIM:', ssim_metric(y, x))\n",
    "            psnr_list += psnr(y, x).cpu().detach().numpy().tolist()\n",
    "            ssim_list += ssim_metric(y, x).cpu().detach().numpy().tolist()\n",
    "        del model_output, fast_params, meta_params, pred_specialized_param\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "print('PSNR:', np.mean(psnr_list))\n",
    "print('SSIM:', np.mean(ssim_list))\n",
    "\n",
    "l = len(psnr_list) // 10\n",
    "print(len(psnr_list), l)\n",
    "for i in range(10):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(img_siren.state_dict(), 'img_siren_03.pth')\n",
    "torch.save(crossAttHypNet.state_dict(), 'crossAttHypNet_03.pth')\n",
    "torch.save(meta_siren.state_dict(), 'meta_siren_03.pth')\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "import json\n",
    "json.dump({'psnr_list': psnr_list, 'ssim_list': ssim_list}, open('psnr_ssim_list_hypernet+meta.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8553827404975891,\n",
       " 0.7498713731765747,\n",
       " 0.871080756187439,\n",
       " 0.8204772472381592,\n",
       " 0.831011950969696,\n",
       " 0.8545085191726685,\n",
       " 0.7410856485366821,\n",
       " 0.7953417301177979,\n",
       " 0.8518069982528687,\n",
       " 0.8062344789505005,\n",
       " 0.8738279342651367,\n",
       " 0.9311460256576538,\n",
       " 0.870961606502533,\n",
       " 0.8833256959915161,\n",
       " 0.8735157251358032,\n",
       " 0.8559781908988953,\n",
       " 0.8251357078552246,\n",
       " 0.7713413834571838,\n",
       " 0.7799944877624512,\n",
       " 0.8738343119621277]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim_list[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 50000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 17.5385835608387\n",
      "SSIM: 0.5225042154034227\n",
      "\n",
      "epoch:1\n",
      "PSNR: 19.642775578651428\n",
      "SSIM: 0.6393658016723395\n",
      "\n",
      "epoch:2\n",
      "PSNR: 20.55733648569107\n",
      "SSIM: 0.6915173385965824\n",
      "\n",
      "epoch:3\n",
      "PSNR: 21.20200930114746\n",
      "SSIM: 0.7253808349466324\n",
      "\n",
      "epoch:4\n",
      "PSNR: 21.703465594005586\n",
      "SSIM: 0.749720869166851\n",
      "\n",
      "epoch:5\n",
      "PSNR: 22.09485023885727\n",
      "SSIM: 0.7673924618273974\n",
      "\n",
      "epoch:6\n",
      "PSNR: 22.413935643863677\n",
      "SSIM: 0.7807534012711048\n",
      "\n",
      "epoch:7\n",
      "PSNR: 22.67949297039032\n",
      "SSIM: 0.7914544672071934\n",
      "\n",
      "epoch:8\n",
      "PSNR: 22.914992713356018\n",
      "SSIM: 0.8004787673205137\n",
      "\n",
      "epoch:9\n",
      "PSNR: 23.131668988780977\n",
      "SSIM: 0.8083140123289824\n"
     ]
    }
   ],
   "source": [
    "l = len(psnr_list) // 10\n",
    "print(len(psnr_list), l)\n",
    "for i in range(10):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l4Ca0dY2xr2w"
   },
   "outputs": [],
   "source": [
    "# # fast_params['net.0.linear.weight'].requires_grad\n",
    "# # fast_params.keys()\n",
    "# # fast_params['net.1.linear.weight'].shape \n",
    "# # for key in fast_params:\n",
    "# #     print(fast_params[key].shape)\n",
    "# for key in meta_params:\n",
    "#     print(key, ':\\t', meta_params[key].shape, '\\t', fast_params[key].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT1Qy7983EbD"
   },
   "source": [
    "As you can see, after a few hundred steps of training, we can fit any of the Cifar-10 images in only three gradient descent steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Kryu4N59GVaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66906387\n",
      "Files already downloaded and verified\n",
      "Epoch 0, Step 0,\tTotal loss: 0.372558,\tHypernet loss: 0.024942\n",
      "\tPSNR: nan \tSSIM: nan\n",
      "[60.984939224555, 64.74048486281639, 67.01535192800142, 59.88635155207875, 68.74323976166585]\n",
      "Epoch 0, Step 1000,\tTotal loss: 0.327509,\tHypernet loss: 0.034427\n",
      "\tPSNR: 22.79414855825901 \tSSIM: 0.7994172387942672\n",
      "[59.59527655391675, 64.24556548595234, 66.61828433254318, 60.9969283620229, 71.23795078491838]\n",
      "PSNR: 22.793975235652923\n",
      "SSIM: 0.7995462300360203\n",
      "10000 10000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 22.793975235652923\n",
      "SSIM: 0.7995462300360203\n"
     ]
    }
   ],
   "source": [
    "steps_til_summary = 1000\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import meta_modules\n",
    "img_siren = Siren(in_features=2, hidden_features=256, hidden_layers=3, out_features=3, outermost_linear=True)\n",
    "img_siren.load_state_dict(torch.load('img_siren_03.pth'))\n",
    "# crossAttHypNet = CrossAttentionHyperNet().cuda()\n",
    "crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                    out_features=3,\n",
    "                                                                    image_resolution=(32, 32))\n",
    "crossAttHypNet.load_state_dict(torch.load('crossAttHypNet_03.pth'))\n",
    "meta_siren = MAML(num_meta_steps=3, hypo_module=img_siren.cuda(), crossAttHypNet=crossAttHypNet.cuda(), \n",
    "                  loss=l2_loss, init_lr=1e-5, \n",
    "                  lr_type='per_parameter_per_step').cuda()\n",
    "meta_siren.load_state_dict(torch.load('meta_siren_03.pth'))\n",
    "meta_siren = meta_siren.cuda()\n",
    "meta_siren.eval()\n",
    "\n",
    "dataset = CIFAR10(train=False)\n",
    "test_dataloader = DataLoader(dataset, batch_size=8, num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "# optim = torch.optim.Adam(lr=5e-5, params=meta_siren.parameters())\n",
    "# optim = torch.optim.Adam(lr=5e-6, params=meta_siren.parameters())\n",
    "\n",
    "hypernet_loss_multiplier = 1\n",
    "\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "for epoch in range(1):\n",
    "#     if epoch < 10:\n",
    "#         hypernet_loss_multiplier += 100\n",
    "\n",
    "    for step, sample in enumerate(test_dataloader):\n",
    "        sample = dict_to_gpu(sample)\n",
    "        '''\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param\n",
    "        '''\n",
    "        model_output, fast_params, meta_params, pred_specialized_param = meta_siren(sample)    \n",
    "        loss = ((model_output['model_out'] - sample['query']['y'])**2).mean() + hypernet_loss_multiplier * model_output['crossAttHypNet_loss']\n",
    "        \n",
    "#         for name in pred_specialized_param:\n",
    "#             loss += 10*((pred_specialized_param[name] - fast_params[name].detach()) ** 2).mean()\n",
    "        if False:\n",
    "            pred_specialized_param_corrected = OrderedDict()\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "                loss += 1*((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "       \n",
    "        \n",
    "        if (step % steps_til_summary == 0) and (epoch % 1 == 0): \n",
    "            print(\"Epoch %d, Step %d,\\tTotal loss: %0.6f,\\tHypernet loss: %0.6f\" % (epoch, step, loss, model_output['crossAttHypNet_loss']))\n",
    "            print('\\tPSNR:', np.mean(psnr_list), '\\tSSIM:', np.mean(ssim_list))\n",
    "            fig, axes = [], list(range(6))#plt.subplots(1,6, figsize=(36,6))\n",
    "            ax_titles = ['Learned Initialization', 'Inner step 1 output', \n",
    "                        'Inner step 2 output', 'Inner step 3 output', \n",
    "                        'HyperNet output', ## added by me\n",
    "                        'Ground Truth']\n",
    "            images = []\n",
    "            for i, inner_step_out in enumerate(model_output['intermed_predictions']):\n",
    "                img = plot_sample_image(inner_step_out, ax=axes[i])\n",
    "                images += [img]\n",
    "#                 axes[i].set_title(ax_titles[i], fontsize=25)\n",
    "            images += [plot_sample_image(model_output['model_out'], ax=axes[-3])]\n",
    "#             axes[-3].set_title(ax_titles[-3], fontsize=25)\n",
    "\n",
    "            if True:\n",
    "                images += [plot_sample_image(model_output['model_output_hypernet'], ax=axes[-2])]\n",
    "#                 axes[-2].set_title(ax_titles[-2], fontsize=25)\n",
    "\n",
    "            img_ground_truth = plot_sample_image(sample['query']['y'], ax=axes[-1])\n",
    "#             axes[-1].set_title(ax_titles[-1], fontsize=25)\n",
    "            psnrs = [cv2.PSNR(img_ground_truth, img) for img in images]\n",
    "            print(psnrs)\n",
    "            plt.show()\n",
    "\n",
    "#         optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "        if True:\n",
    "            x = lin2img(model_output['model_output_hypernet']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            x += 1.\n",
    "            x /= 2.\n",
    "            x = torch.clip(x, 0., 1.)\n",
    "            y = lin2img(sample['query']['y']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            y += 1.\n",
    "            y /= 2.\n",
    "            y = torch.clip(y, 0., 1.)\n",
    "    #         print(x.shape, lin2img(x).shape)\n",
    "#             print(y.min(), y.max())\n",
    "#             print(x.min(), x.max())\n",
    "\n",
    "#             print('PSNR:', psnr(y, x).mean())\n",
    "#             print('psnr_sklearn:', psnr_sklearn(y.numpy(), x.numpy(), data_range=1.))\n",
    "#             print('SSIM:', ssim_metric(y, x))\n",
    "            psnr_list += psnr(y, x).cpu().detach().numpy().tolist()\n",
    "            ssim_list += ssim_metric(y, x).cpu().detach().numpy().tolist()\n",
    "        del model_output, fast_params, meta_params, pred_specialized_param\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "print('PSNR:', np.mean(psnr_list))\n",
    "print('SSIM:', np.mean(ssim_list))\n",
    "\n",
    "l = len(psnr_list) // 1\n",
    "print(len(psnr_list), l)\n",
    "for i in range(1):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MetaSDF_on_Cifar_10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
