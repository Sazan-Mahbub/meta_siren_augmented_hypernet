{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTwikOyWZobA"
   },
   "source": [
    "# MetaSDF & Meta-SIREN\n",
    "\n",
    "This is a colab to explore MetaSDF, and its applications to rapidly fit neural implicit representations.\n",
    "\n",
    "Make sure to switch the runtime type to \"GPU\" under \"Runtime --> Change Runtime Type\"!\n",
    "\n",
    "We will show you how to run two experiments using gradient-based meta-learning: \n",
    "* [Fitting an image in 3 gradient descent steps with SIREN](#section_1)\n",
    "* [Fitting 2D Signed Distance Functions of MNIST digits](#section_2)\n",
    "\n",
    "Let's go! \n",
    "\n",
    "First, the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ49alfvZobQ",
    "outputId": "627391a5-abd0-4101-8a73-66302c056cbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sazan\\AppData\\Local\\Temp/ipykernel_14568/3975817298.py:12: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import OrderedDict, Mapping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scipy.ndimage\n",
    "from torch import nn \n",
    "from collections import OrderedDict, Mapping \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "from torch.nn.init import _calculate_correct_fan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrsG6X9cDsZe",
    "outputId": "a6e98051-c4ed-4948-8475-5c9775f19c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15]) torch.Size([2, 3]) \n",
      ">>\n",
      " tensor([[ 1,  2,  3,  1,  2,  3,  1,  2,  3,  1,  2,  3,  1,  2,  3],\n",
      "        [10, 20, 30, 10, 20, 30, 10, 20, 30, 10, 20, 30, 10, 20, 30]]) \n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3],\n",
      "         [ 1,  2,  3]],\n",
      "\n",
      "        [[10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30],\n",
      "         [10, 20, 30]]]) tensor([[ 1,  2,  3],\n",
      "        [10, 20, 30]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [10,20,30]]) \n",
    "print(x.repeat(1, 5).shape, x.shape,'\\n>>\\n', x.repeat(1, 5), '\\n', x.repeat(1, 5).view([-1, 5, 3]), x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2D7783s2tg2"
   },
   "source": [
    "For meta-learning, we're using the excellent \"Torchmeta\" library. We have to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eX_d5hsYZtSq",
    "outputId": "4122e3c1-31d0-4e35-e770-fd46c4a5c8e4"
   },
   "outputs": [],
   "source": [
    "# !pip install torchmeta\n",
    "from torchmeta.modules import (MetaModule, MetaSequential, MetaLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je35HXUPZobV"
   },
   "source": [
    "We're now ready to implement a few neural network layers: Fully connected networks, and SIREN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DlEcUhGiZobX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BatchLinear(nn.Linear, MetaModule):\n",
    "    '''A linear meta-layer that can deal with batched weight matrices and biases, as for instance output by a\n",
    "    hypernetwork.'''\n",
    "    __doc__ = nn.Linear.__doc__\n",
    "\n",
    "    def forward(self, input, params=None):\n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "\n",
    "        bias = params.get('bias', None)\n",
    "        weight = params['weight']\n",
    "\n",
    "        output = input.matmul(weight.permute(*[i for i in range(len(weight.shape)-2)], -1, -2))\n",
    "        output += bias.unsqueeze(-2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MetaFC(MetaModule):\n",
    "    '''A fully connected neural network that allows swapping out the weights, either via a hypernetwork\n",
    "    or via MAML.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 num_hidden_layers, hidden_features,\n",
    "                 outermost_linear=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(MetaSequential(\n",
    "            BatchLinear(in_features, hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ))\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, hidden_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        if outermost_linear:\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, out_features),\n",
    "            ))\n",
    "        else:\n",
    "            self.net.append(MetaSequential(\n",
    "                BatchLinear(hidden_features, out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        self.net = MetaSequential(*self.net)\n",
    "        self.net.apply(init_weights_normal)\n",
    "\n",
    "    def forward(self, coords, params=None, **kwargs):\n",
    "        '''Simple forward pass without computation of spatial gradients.'''\n",
    "        output = self.net(coords, params=self.get_subdict(params, 'net'))\n",
    "        return output\n",
    "\n",
    "\n",
    "class SineLayer(MetaModule):\n",
    "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
    "\n",
    "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
    "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
    "    # hyperparameter.\n",
    "\n",
    "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
    "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = float(omega_0)\n",
    "\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.linear = BatchLinear(in_features, out_features, bias=bias)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features,\n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
    "                                            np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "\n",
    "    def forward(self, input, params=None):\n",
    "        intermed = self.linear(input, params=self.get_subdict(params, 'linear'))\n",
    "        return torch.sin(self.omega_0 * intermed)\n",
    "\n",
    "\n",
    "class Siren(MetaModule):\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False,\n",
    "                 first_omega_0=30, hidden_omega_0=30., special_first=True):\n",
    "        super().__init__()\n",
    "        self.hidden_omega_0 = hidden_omega_0\n",
    "\n",
    "        layer = SineLayer\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(layer(in_features, hidden_features,\n",
    "                              is_first=special_first, omega_0=first_omega_0))\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(layer(hidden_features, hidden_features,\n",
    "                                  is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = BatchLinear(hidden_features, out_features)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / 30.,\n",
    "                                             np.sqrt(6 / hidden_features) / 30.)\n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(layer(hidden_features, out_features, is_first=False, omega_0=hidden_omega_0))\n",
    "\n",
    "        self.net = nn.ModuleList(self.net)\n",
    "\n",
    "    def forward(self, coords, params=None):\n",
    "        x = coords\n",
    "\n",
    "        for i, layer in enumerate(self.net):\n",
    "            x = layer(x, params=self.get_subdict(params, f'net.{i}'))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def init_weights_normal(m):\n",
    "    if type(m) == BatchLinear or nn.Linear:\n",
    "        if hasattr(m, 'weight'):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if hasattr(m, 'bias'):\n",
    "            m.bias.data.fill_(0.)\n",
    "            \n",
    "            \n",
    "def get_mgrid(sidelen):\n",
    "    # Generate 2D pixel coordinates from an image of sidelen x sidelen\n",
    "    pixel_coords = np.stack(np.mgrid[:sidelen,:sidelen], axis=-1)[None,...].astype(np.float32)\n",
    "    pixel_coords /= sidelen    \n",
    "    pixel_coords -= 0.5\n",
    "    pixel_coords = torch.Tensor(pixel_coords).view(-1, 2)\n",
    "    return pixel_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRN7_GF4jwyT"
   },
   "source": [
    "Sazan: Now let's implement our Cross-Attention Hypernetwork. It will take the image as input and generate some matrices with the same dimension as the weights of the SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i0ai6Ry7jvmE"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from modules_custom import Conv2dResBlock\n",
    "\n",
    "class CrossAttentionHyperNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        L = 64\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=5//2) # padding=kernel_size//2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=5//2) # padding=kernel_size//2\n",
    "        self.conv3 = nn.Conv2d(64, L, 5, padding=5//2)\n",
    "        self.conv4 = nn.Conv2d(L, L, 5, padding=5//2)\n",
    "        # self.conv4_dim_reduction = nn.Conv2d(16+32+64, 64, 1, padding=0)\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 3, 1, 1),\n",
    "#             nn.ReLU(),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             Conv2dResBlock(256, 256),\n",
    "#             nn.Conv2d(256, 256, 1, 1, 0)\n",
    "#         )\n",
    "#         self.relu_2 = nn.ReLU(inplace=True)\n",
    "#         self.fc = nn.Linear(1024, 1)\n",
    "\n",
    "        if True:\n",
    "            self.fc0 = nn.Linear(L, 2)\n",
    "            self.fc1 = nn.Linear(L, L)\n",
    "            self.fc2 = nn.Linear(L, L)\n",
    "            self.fc3 = nn.Linear(L, L)\n",
    "            self.fc4_1 = nn.Linear(L, 3)\n",
    "            self.fc4_2 = nn.Linear(3, 3)\n",
    "            self.fc4_bias = nn.Linear(L, 3)\n",
    "\n",
    "        if False:\n",
    "            self.weighted_mean = torch.nn.Conv1d(in_channels=64, out_channels=5, kernel_size=1)\n",
    "\n",
    "            self.bias0_fc = nn.Linear(64+64, 64)\n",
    "            self.bias1_fc = nn.Linear(64+64, 64)\n",
    "            self.bias2_fc = nn.Linear(64+64, 64)\n",
    "            self.bias3_fc = nn.Linear(64+64, 64)\n",
    "            self.bias4_fc = nn.Linear(3+64, 3)\n",
    "            \n",
    "            self.attn_bias0_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias1_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias2_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias3_fc = nn.Linear(64, 1)\n",
    "            self.attn_bias4_fc = nn.Linear(3, 1)\n",
    "\n",
    "\n",
    "        self.wt_cross_attn0 = nn.MultiheadAttention(embed_dim=2, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn3 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.wt_cross_attn4 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        \n",
    "        self.bias_cross_attn0 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn3 = nn.MultiheadAttention(embed_dim=64, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        self.bias_cross_attn4 = nn.MultiheadAttention(embed_dim=3, num_heads=1, dropout=0.1, bias=True)#, batch_first=True)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    net.0.linear.weight :\t torch.Size([64, 2]) \t torch.Size([16, 64, 2])\n",
    "    net.0.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.1.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.1.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.2.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.2.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.3.linear.weight :\t torch.Size([64, 64]) \t torch.Size([16, 64, 64])\n",
    "    net.3.linear.bias :\t torch.Size([64]) \t torch.Size([16, 64])\n",
    "    net.4.weight :\t torch.Size([3, 64]) \t torch.Size([16, 3, 64])\n",
    "    net.4.bias :\t torch.Size([3]) \t torch.Size([16, 3])\n",
    "    '''\n",
    "    \n",
    "    def forward_conv(self, x):\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        b = x.shape[0]\n",
    "        # print('1>', x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x))) # bx3x32x32 -> bx32x16x16\n",
    "        # print('2>', x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x))) # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        x = F.relu(self.conv3(x)) # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.permute(0, 2, 3, 1).contiguous() # bx32x16x16 -> bx64x8x8 -> bx8x8x64\n",
    "        # print('3>', x.shape)\n",
    "        # x3 = self.pool(F.relu(self.conv3(x2)))\n",
    "        # x = torch.cat([x1, x2], -1)\n",
    "        # x = F.relu(self.conv4_dim_reduction(x))\n",
    "#         print('>>', x.shape)\n",
    "        x = x.view([b, 64, 64]) # bx8x8x64 -> bx64x64 ## channel last\n",
    "        # print('4>', x.shape)\n",
    "        \n",
    "        x0 = F.relu(self.fc0(x)) # bx64x64 -> bx64x2\n",
    "        x1 = F.relu(self.fc1(x)) # bx64x64 -> bx64x64\n",
    "        x2 = F.relu(self.fc2(x)) # bx64x64 -> bx64x64\n",
    "        x3 = F.relu(self.fc3(x)) # bx64x64 -> bx64x64\n",
    "        x4 = F.relu(self.fc4_1(x.permute(0,2,1))) # bx64x64 -> bx64x64 -> bx64x3\n",
    "        x4 = F.relu(self.fc4_2(x4)).permute(0,2,1).contiguous() # bx64x3 -> bx64x3 -> bx3x64\n",
    "\n",
    "        # x_biases = F.relu(self.weighted_mean(x)) # bx64x64 -> bx5x64\n",
    "        # x0_bias = F.relu(self.bias0_fc(x_biases[:, 0, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x1_bias = F.relu(self.bias1_fc(x_biases[:, 1, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x2_bias = F.relu(self.bias2_fc(x_biases[:, 2, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x3_bias = F.relu(self.bias3_fc(x_biases[:, 3, :])) # bx5x64 ->  bx64 -> bx64\n",
    "        # x4_bias = F.relu(self.bias4_fc(x_biases[:, 4, :])) # bx5x64 ->  bx64 -> bx3\n",
    "\n",
    "        return x, x0, x1, x2, x3, x4#, x0_bias, x1_bias, x2_bias, x3_bias, x4_bias\n",
    "    \n",
    "    def bias_attention(self, x, meta_param_bias, bias_fc, attn_bias_fc):\n",
    "        b, c = meta_param_bias.shape\n",
    "        meta_param_bias = meta_param_bias.view(b, 1, c)\n",
    "        param_bias = torch.cat([meta_param_bias.repeat(1,64,1), x], -1) # [bx1xc, bx64x64] -> [bx64xc, bx64x64] -> bx64x(c+64)\n",
    "        param_bias = F.relu(bias_fc(param_bias)) # bx64x(c+64) -> bx64xc\n",
    "        attention_scores = F.relu(attn_bias_fc(param_bias)).permute(0,2,1) # bx64xc -> bx64x1 -> bx1x64\n",
    "        attention_scores = F.softmax(attention_scores, -1) # bx1x64 -> bx1x64\n",
    "        param_bias = torch.bmm(attention_scores, param_bias).view(b, c) # [bx1x64, bx64xc] -> bx1xc -> bxc\n",
    "        return param_bias\n",
    "\n",
    "    def compute_biases(self, x, meta_params):\n",
    "        x0_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.0.linear.bias'], bias_fc=self.bias0_fc, attn_bias_fc=self.attn_bias0_fc)\n",
    "        x1_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.1.linear.bias'], bias_fc=self.bias1_fc, attn_bias_fc=self.attn_bias1_fc)\n",
    "        x2_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.2.linear.bias'], bias_fc=self.bias2_fc, attn_bias_fc=self.attn_bias2_fc)\n",
    "        x3_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.3.linear.bias'], bias_fc=self.bias3_fc, attn_bias_fc=self.attn_bias3_fc)\n",
    "        x4_bias = self.bias_attention(x=x, meta_param_bias=meta_params['net.4.bias'], bias_fc=self.bias4_fc, attn_bias_fc=self.attn_bias4_fc)\n",
    "        return x0_bias, x1_bias, x2_bias, x3_bias, x4_bias \n",
    "\n",
    "    def compute_loss(self, specialized_param, gt_specialized_param):\n",
    "        loss = 0.\n",
    "        for key in specialized_param:\n",
    "            loss += F.mse_loss(input=specialized_param[key], target=gt_specialized_param[key])\n",
    "        loss /= 10 # not sure if it will help\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, meta_params):\n",
    "        x, x0, x1, x2, x3, x4 = self.forward_conv(x) \n",
    "        # x0_bias, x1_bias, x2_bias, x3_bias, x4_bias = self.compute_biases(x, meta_params)\n",
    "\n",
    "        # x = self.forward_conv(x) # bx32x32x3 -> bx64x64\n",
    "        b, l, c = x.shape\n",
    "        # query -> from meta model ==> meta_params\n",
    "        # key and value -> from this model ==> x\n",
    "        specialized_param = OrderedDict()\n",
    "\n",
    "        # print('1>', meta_params['net.4.bias'].shape, x.shape)\n",
    "        # x_in = self.fc0(x)\n",
    "        x_out = F.relu(self.fc4_bias(x))\n",
    "        specialized_param['net.0.linear.weight']  = self.wt_cross_attn0(query=meta_params['net.0.linear.weight'], key=x0, value=x0)[0]\n",
    "        specialized_param['net.0.linear.bias']  = self.bias_cross_attn0(query=meta_params['net.0.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.1.linear.weight']  = self.wt_cross_attn1(query=meta_params['net.1.linear.weight'], key=x1, value=x1)[0]\n",
    "        specialized_param['net.1.linear.bias']  = self.bias_cross_attn1(query=meta_params['net.1.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.2.linear.weight']  = self.wt_cross_attn2(query=meta_params['net.2.linear.weight'], key=x2, value=x2)[0]\n",
    "        specialized_param['net.2.linear.bias']  = self.bias_cross_attn2(query=meta_params['net.2.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.3.linear.weight']  = self.wt_cross_attn3(query=meta_params['net.3.linear.weight'], key=x3, value=x3)[0]\n",
    "        specialized_param['net.3.linear.bias']  = self.bias_cross_attn3(query=meta_params['net.3.linear.bias'].view(b, 1, -1), key=x, value=x)[0].view(b, -1)\n",
    "        specialized_param['net.4.weight']  = self.wt_cross_attn4(query=meta_params['net.4.weight'], key=x4, value=x4)[0]\n",
    "        specialized_param['net.4.bias']  = self.bias_cross_attn4(query=meta_params['net.4.bias'].view(b, 1, -1), key=x_out, value=x_out)[0].view(b, -1)\n",
    "\n",
    "#         loss = self.compute_loss(specialized_param, gt_specialized_param)\n",
    "\n",
    "#         return loss, specialized_param\n",
    "        return specialized_param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhPvd2TUkZC8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cPO4PWDZobZ"
   },
   "source": [
    "Now, we implement MAML. The important parts of the code are commented, so it's easy to understand how each part works! Start by looking at the \"forward\" function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OL4dgzRiZoba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def l2_loss(prediction, gt):\n",
    "    return ((prediction - gt)**2).mean()\n",
    "\n",
    "\n",
    "class MAML(nn.Module):\n",
    "    def __init__(self, num_meta_steps, hypo_module, crossAttHypNet, loss, init_lr,\n",
    "                 lr_type='static', first_order=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hypo_module = hypo_module # The module who's weights we want to meta-learn.\n",
    "        self.crossAttHypNet = crossAttHypNet\n",
    "        self.first_order = first_order\n",
    "        self.loss = loss\n",
    "        self.lr_type = lr_type\n",
    "        self.log = []\n",
    "\n",
    "        self.register_buffer('num_meta_steps', torch.Tensor([num_meta_steps]).int())\n",
    "\n",
    "        if self.lr_type == 'static': \n",
    "            self.register_buffer('lr', torch.Tensor([init_lr]))\n",
    "        elif self.lr_type == 'global':\n",
    "            self.lr = nn.Parameter(torch.Tensor([init_lr]))\n",
    "        elif self.lr_type == 'per_step':\n",
    "            self.lr = nn.ParameterList([nn.Parameter(torch.Tensor([init_lr]))\n",
    "                                        for _ in range(num_meta_steps)])\n",
    "        elif self.lr_type == 'per_parameter': # As proposed in \"Meta-SGD\".\n",
    "            self.lr = nn.ParameterList([])\n",
    "            hypo_parameters = hypo_module.parameters()\n",
    "            for param in hypo_parameters:\n",
    "                self.lr.append(nn.Parameter(torch.ones(param.size()) * init_lr))\n",
    "        elif self.lr_type == 'per_parameter_per_step':\n",
    "            self.lr = nn.ModuleList([])\n",
    "            for name, param in hypo_module.meta_named_parameters():\n",
    "                self.lr.append(nn.ParameterList([nn.Parameter(torch.ones(param.size()) * init_lr)\n",
    "                                                 for _ in range(num_meta_steps)]))\n",
    "\n",
    "        param_count = 0\n",
    "        for param in self.parameters():\n",
    "            param_count += np.prod(param.shape)\n",
    "\n",
    "        print(param_count)\n",
    "\n",
    "    def _update_step(self, loss, param_dict, step):\n",
    "        grads = torch.autograd.grad(loss, param_dict.values(),\n",
    "                                    create_graph=False if self.first_order else True)\n",
    "        params = OrderedDict()\n",
    "        for i, ((name, param), grad) in enumerate(zip(param_dict.items(), grads)):\n",
    "            if self.lr_type in ['static', 'global']:\n",
    "                lr = self.lr\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_step']:\n",
    "                lr = self.lr[step]\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_parameter']:\n",
    "                lr = self.lr[i]\n",
    "                params[name] = param - lr * grad\n",
    "            elif self.lr_type in ['per_parameter_per_step']:\n",
    "                lr = self.lr[i][step]\n",
    "                params[name] = param - lr * grad\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return params, grads\n",
    "\n",
    "    def forward_with_params(self, query_x, fast_params, **kwargs):\n",
    "        output = self.hypo_module(query_x, params=fast_params)\n",
    "        return output\n",
    "\n",
    "    def generate_params(self, context_dict):\n",
    "        \"\"\"Specializes the model\"\"\"\n",
    "        x = context_dict.get('x').cuda()\n",
    "        y = context_dict.get('y').cuda()\n",
    "\n",
    "        meta_batch_size = x.shape[0]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            # First, replicate the initialization for each batch item.\n",
    "            # This is the learned initialization, i.e., in the outer loop,\n",
    "            # the gradients are backpropagated all the way into the \n",
    "            # \"meta_named_parameters\" of the hypo_module.\n",
    "            fast_params = OrderedDict()\n",
    "            meta_params = OrderedDict()\n",
    "            for name, param in self.hypo_module.meta_named_parameters():\n",
    "                fast_params[name] = param[None, ...].repeat((meta_batch_size,) + (1,) * len(param.shape))\n",
    "                meta_params[name] = param[None, ...].repeat((meta_batch_size,) + (1,) * len(param.shape))\n",
    "\n",
    "            prev_loss = 1e6\n",
    "            intermed_predictions = []\n",
    "            for j in range(self.num_meta_steps):\n",
    "                # Using the current set of parameters, perform a forward pass with the context inputs.\n",
    "                predictions = self.hypo_module(x, params=fast_params)\n",
    "\n",
    "                # Compute the loss on the context labels.\n",
    "                loss = self.loss(predictions, y)\n",
    "                intermed_predictions.append(predictions)\n",
    "\n",
    "                if loss > prev_loss:\n",
    "                    print('inner lr too high?')\n",
    "                \n",
    "                # Using the computed loss, update the fast parameters.\n",
    "                fast_params, grads = self._update_step(loss, fast_params, j)\n",
    "                prev_loss = loss\n",
    "\n",
    "        return fast_params, intermed_predictions, meta_params\n",
    "\n",
    "    def forward(self, meta_batch, **kwargs):\n",
    "        # The meta_batch conists of the \"context\" set (the observations we're conditioning on)\n",
    "        # and the \"query\" inputs (the points where we want to evaluate the specialized model)\n",
    "        t0 = time.time()\n",
    "        context = meta_batch['context']\n",
    "        query_x = meta_batch['query']['x'].cuda()\n",
    "        \n",
    "        t1 = time.time()\n",
    "        # Specialize the model with the \"generate_params\" function.\n",
    "        fast_params, intermed_predictions, meta_params = self.generate_params(context)\n",
    "        t2 = time.time()\n",
    "        pred_specialized_param = self.crossAttHypNet(x=lin2img(context['y']).cuda())#, meta_params=meta_params, gt_specialized_param=fast_params)\n",
    "        t3 = time.time()\n",
    "        pred_specialized_param_corrected = OrderedDict()\n",
    "        \n",
    "        crossAttHypNet_loss = 0.\n",
    "        if True:\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "#                 crossAttHypNet_loss += ((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "                \n",
    "        # Compute the final outputs. \n",
    "        model_output = self.hypo_module(query_x, params=fast_params)\n",
    "        model_output_hypernet = self.hypo_module(query_x, params=pred_specialized_param_corrected)\n",
    "        crossAttHypNet_loss += self.loss(model_output_hypernet, context['y'])\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "        t4 = time.time()\n",
    "#         print(f'meta: {t2-t1}  {t4-t0 - t3+t2}, hyper: {t3-t2} {t4-t0 - t2+t1}, all: {t4 - t0}')\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i0VRikaZobc"
   },
   "source": [
    "<a id='section_1'></a>\n",
    "## Learning to fit images in 3 gradient descent steps\n",
    "\n",
    "By learning an initialization for SIREN, we may fit any image in as few as 3 gradient descent steps! \n",
    "This has also been noted by Tancik et al. in \"Learned Initializations for Optimizing Coordinate-Based Neural Representations\" (2020).\n",
    "\n",
    "We'll demonstrate here with Cifar-10, but it works just as well with CelebA or imagenet - try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gKFj5-FVZobc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR10():\n",
    "    def __init__(self, train=True):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        self.dataset = torchvision.datasets.CIFAR10(root='./data', train=train,\n",
    "                                                download=True, transform=transform)\n",
    "        \n",
    "        self.length = len(self.dataset)\n",
    "        self.meshgrid = get_mgrid(sidelen=32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img, _ = self.dataset[item]\n",
    "        img_flat = img.permute(1,2,0).view(-1, 3)\n",
    "        return {'context':{'x':self.meshgrid, 'y':img_flat}, \n",
    "                'query':{'x':self.meshgrid, 'y':img_flat}}\n",
    "\n",
    "\n",
    "def lin2img(tensor):\n",
    "    batch_size, num_samples, channels = tensor.shape\n",
    "    sidelen = np.sqrt(num_samples).astype(int)\n",
    "    return tensor.view(batch_size, sidelen, sidelen, channels).squeeze(-1)\n",
    "\n",
    "    \n",
    "def plot_sample_image(img_batch, ax):\n",
    "    img = lin2img(img_batch)[0].detach().cpu().numpy()\n",
    "    img += 1\n",
    "    img /= 2.\n",
    "    img = np.clip(img, 0., 1.)\n",
    "#     ax.set_axis_off()\n",
    "#     ax.imshow(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def dict_to_gpu(ob):\n",
    "    if isinstance(ob, Mapping):\n",
    "        return {k: dict_to_gpu(v) for k, v in ob.items()}\n",
    "    else:\n",
    "        return ob.cuda()    \n",
    "\n",
    "\n",
    "# def dict_to_gpu(ob):\n",
    "#     if isinstance(ob, Mapping):\n",
    "#         return {k: dict_to_gpu(v) for k, v in ob.items()}\n",
    "#     else:\n",
    "#         return ob.cuda()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t9tBE4EZobd"
   },
   "source": [
    "Now, let's initialize our models and our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKLBYi25Zobd",
    "outputId": "b6694221-afa4-472e-a572-198500159e14",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66906387\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import meta_modules\n",
    "img_siren = Siren(in_features=2, hidden_features=256, hidden_layers=3, out_features=3, outermost_linear=True)\n",
    "img_siren.load_state_dict(torch.load('img_siren_1.pth'))\n",
    "# crossAttHypNet = CrossAttentionHyperNet().cuda()\n",
    "crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                    out_features=3,\n",
    "                                                                    image_resolution=(32, 32))\n",
    "crossAttHypNet.load_state_dict(torch.load('crossAttHypNet_1.pth'))\n",
    "meta_siren = MAML(num_meta_steps=3, hypo_module=img_siren.cuda(), crossAttHypNet=crossAttHypNet.cuda(), \n",
    "                  loss=l2_loss, init_lr=1e-5, \n",
    "                  lr_type='per_parameter_per_step').cuda()\n",
    "meta_siren.load_state_dict(torch.load('meta_siren_1.pth'))\n",
    "meta_siren = meta_siren.cuda()\n",
    "if True:\n",
    "    del crossAttHypNet\n",
    "    torch.cuda.empty_cache()\n",
    "    crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                        out_features=3,\n",
    "                                                                        image_resolution=(32, 32))\n",
    "    meta_siren.crossAttHypNet = crossAttHypNet.cuda()\n",
    "meta_siren.train()\n",
    "\n",
    "dataset = CIFAR10()\n",
    "dataloader = DataLoader(dataset, batch_size=16, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lJQ3Fdr-zvIB"
   },
   "outputs": [],
   "source": [
    "# crossAttHypNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5dv0PYqRZlN",
    "outputId": "8a79253e-e5de-4e2b-8c11-147675e8b438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "img1 = cv2.imread('img1.bmp')\n",
    "img2 = cv2.imread('img2.bmp')\n",
    "psnr = cv2.PSNR(img1, img2)\n",
    "psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install piqa\n",
    "from metrics import psnr, ssim_metric\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr_sklearn\n",
    "\n",
    "# print('PSNR:', psnr.psnr(x, y))\n",
    "# print('SSIM:', ssim.ssim(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS7Juv9HZobf"
   },
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "pzmwE0_KZobg",
    "outputId": "96cc34be-e0a5-41f8-f455-1c68d3579309",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0,\tTotal loss: 0.459648,\tHypernet loss: 0.459164\n",
      "\tPSNR: nan \tSSIM: nan\n",
      "[61.970139418908076, 79.45781866874914, 83.65020168564416, 87.23265898416778, 57.74506572405127]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 1000,\tTotal loss: 0.080177,\tHypernet loss: 0.079696\n",
      "\tPSNR: 15.634923721522092 \tSSIM: 0.46937411817233077\n",
      "[60.61849277678819, 77.01214189453661, 82.43402461441967, 87.7616340670144, 64.78318890138061]\n",
      "Epoch 0, Step 2000,\tTotal loss: 0.058338,\tHypernet loss: 0.057823\n",
      "\tPSNR: 16.820098195508123 \tSSIM: 0.533869205305702\n",
      "[65.81378262269466, 82.40283570054422, 86.52193422411605, 89.89350623160762, 73.36657019560946]\n",
      "Epoch 0, Step 3000,\tTotal loss: 0.065868,\tHypernet loss: 0.065047\n",
      "\tPSNR: 17.483636867652336 \tSSIM: 0.5716190961322282\n",
      "[59.73142375891341, 76.45133269791023, 81.57609823712068, 86.34791100869846, 66.12086240606239]\n",
      "Epoch 1, Step 0,\tTotal loss: 0.061886,\tHypernet loss: 0.061123\n",
      "\tPSNR: 17.547758929891586 \tSSIM: 0.5754110272672027\n",
      "[57.53321154439581, 76.95803386871577, 83.41530550078932, 89.28270045197345, 66.40715829087782]\n",
      "Epoch 1, Step 1000,\tTotal loss: 0.042106,\tHypernet loss: 0.041530\n",
      "\tPSNR: 17.988852461431964 \tSSIM: 0.6016455089130975\n",
      "[62.60576955073834, 77.34705704931427, 81.84388532646517, 85.71673042024656, 66.71157328088177]\n",
      "Epoch 1, Step 2000,\tTotal loss: 0.044383,\tHypernet loss: 0.043621\n",
      "\tPSNR: 18.334734107104744 \tSSIM: 0.6215367921988321\n",
      "[60.00774512599541, 76.67200089193987, 81.30748506650153, 85.51720839809755, 67.81158327281327]\n",
      "Epoch 1, Step 3000,\tTotal loss: 0.048178,\tHypernet loss: 0.047428\n",
      "\tPSNR: 18.61266636605652 \tSSIM: 0.6377395319751255\n",
      "[67.22022091716681, 82.33373513370908, 86.30065740024551, 89.45683325639855, 74.00708065682295]\n",
      "Epoch 2, Step 0,\tTotal loss: 0.040340,\tHypernet loss: 0.039757\n",
      "\tPSNR: 18.645038433480263 \tSSIM: 0.6395870238850638\n",
      "[60.39362096682794, 76.04112385410914, 80.57167940185721, 84.32207669615835, 67.66996036433399]\n",
      "Epoch 2, Step 1000,\tTotal loss: 0.039414,\tHypernet loss: 0.038730\n",
      "\tPSNR: 18.892211039045762 \tSSIM: 0.6535155186240464\n",
      "[57.65880391221877, 76.1023405696261, 82.39744322169152, 88.37773565010511, 69.03823911425516]\n",
      "Epoch 2, Step 2000,\tTotal loss: 0.041311,\tHypernet loss: 0.040599\n",
      "\tPSNR: 19.10597420101816 \tSSIM: 0.6652571290224283\n",
      "[57.32688129262911, 75.91184685880033, 81.4379396732324, 85.92598767773069, 68.80427505664781]\n",
      "Epoch 2, Step 3000,\tTotal loss: 0.046610,\tHypernet loss: 0.045730\n",
      "\tPSNR: 19.292591267930494 \tSSIM: 0.6755262715150728\n",
      "[58.82415174272366, 73.0704321210957, 77.11710595553761, 80.58678140433416, 63.945439363965555]\n",
      "Epoch 3, Step 0,\tTotal loss: 0.030436,\tHypernet loss: 0.029983\n",
      "\tPSNR: 19.315428274552026 \tSSIM: 0.6767543546750893\n",
      "[62.36758361054336, 79.58579879044729, 85.69871608035909, 90.80454734245983, 71.01533472395012]\n",
      "Epoch 3, Step 1000,\tTotal loss: 0.039677,\tHypernet loss: 0.038760\n",
      "\tPSNR: 19.49135502719592 \tSSIM: 0.6861762506982244\n",
      "[59.384952619947875, 74.78837154968282, 78.95366207498024, 82.22451342724149, 66.15517773164083]\n",
      "Epoch 3, Step 2000,\tTotal loss: 0.035264,\tHypernet loss: 0.034626\n",
      "\tPSNR: 19.647886476094907 \tSSIM: 0.6945222614784776\n",
      "[59.99585883600308, 80.50406886831306, 86.85565836555838, 91.89690501303878, 72.54315305192097]\n",
      "Epoch 3, Step 3000,\tTotal loss: 0.024787,\tHypernet loss: 0.024331\n",
      "\tPSNR: 19.790757712612248 \tSSIM: 0.7020375378720951\n",
      "[59.79561554489522, 77.66743277038296, 83.8246769335956, 89.12087432869544, 69.70754624507796]\n",
      "Epoch 4, Step 0,\tTotal loss: 0.027425,\tHypernet loss: 0.026892\n",
      "\tPSNR: 19.808215345680715 \tSSIM: 0.7029182711072824\n",
      "[59.30664126880424, 75.66183251224602, 79.99311645669533, 83.84996396658966, 68.7452814508294]\n",
      "Epoch 4, Step 1000,\tTotal loss: 0.029342,\tHypernet loss: 0.028670\n",
      "\tPSNR: 19.94360319388575 \tSSIM: 0.7099713375092898\n",
      "[60.80262504705505, 75.96310637211398, 79.90055256280972, 82.58472601620836, 69.79939108439298]\n",
      "Epoch 4, Step 2000,\tTotal loss: 0.030028,\tHypernet loss: 0.029521\n",
      "\tPSNR: 20.069126819386565 \tSSIM: 0.7163130829744523\n",
      "[56.850697051776606, 73.9323108352415, 79.903798380908, 85.85449850112118, 66.89875487366108]\n",
      "Epoch 4, Step 3000,\tTotal loss: 0.029395,\tHypernet loss: 0.028776\n",
      "\tPSNR: 20.182558592552137 \tSSIM: 0.7220891585998207\n",
      "[59.42342343758838, 76.47829336385993, 81.90845524447867, 86.55518938962253, 67.853340037095]\n",
      "Epoch 5, Step 0,\tTotal loss: 0.029525,\tHypernet loss: 0.029037\n",
      "\tPSNR: 20.19588274894905 \tSSIM: 0.7227709842867106\n",
      "[62.219898623174515, 79.58304342315684, 84.32998499987295, 88.58493557730955, 71.52758889716725]\n",
      "Epoch 5, Step 1000,\tTotal loss: 0.031368,\tHypernet loss: 0.030589\n",
      "\tPSNR: 20.307932715607766 \tSSIM: 0.7283157988889073\n",
      "[62.91055400653554, 80.34894465176362, 84.92460366406598, 88.3575376808058, 72.78010829525738]\n",
      "Epoch 5, Step 2000,\tTotal loss: 0.028009,\tHypernet loss: 0.027552\n",
      "\tPSNR: 20.40934881632548 \tSSIM: 0.7333502101489601\n",
      "[58.692356268137935, 78.39657790671585, 83.90058838535222, 88.03547247448824, 73.55806202074221]\n",
      "Epoch 5, Step 3000,\tTotal loss: 0.029259,\tHypernet loss: 0.028486\n",
      "\tPSNR: 20.50458030706284 \tSSIM: 0.7379570783926112\n",
      "[60.48091968305266, 70.96675644643594, 74.94641756694537, 81.15771893247681, 66.00410366838852]\n",
      "Epoch 6, Step 0,\tTotal loss: 0.024772,\tHypernet loss: 0.024189\n",
      "\tPSNR: 20.516415234163603 \tSSIM: 0.7385090021643415\n",
      "[61.083861822772, 79.19504227071067, 85.11333506477769, 90.33019715716712, 71.96183528440454]\n",
      "Epoch 6, Step 1000,\tTotal loss: 0.029418,\tHypernet loss: 0.028700\n",
      "\tPSNR: 20.610331819831572 \tSSIM: 0.7429770884349164\n",
      "[59.09780395163941, 72.8309972425236, 78.13429400727284, 83.52370498441046, 65.64106824257601]\n",
      "Epoch 6, Step 2000,\tTotal loss: 0.021670,\tHypernet loss: 0.021230\n",
      "\tPSNR: 20.696321840643883 \tSSIM: 0.7470867071132321\n",
      "[59.99584805631691, 79.02725902607615, 85.26045774758998, 91.231635606458, 71.91720061640488]\n",
      "Epoch 6, Step 3000,\tTotal loss: 0.020706,\tHypernet loss: 0.020367\n",
      "\tPSNR: 20.77820381833904 \tSSIM: 0.7509149659564076\n",
      "[59.019662002090655, 77.66278337149797, 84.09446381068052, 90.06133990205383, 70.78698959662489]\n",
      "Epoch 7, Step 0,\tTotal loss: 0.027528,\tHypernet loss: 0.026839\n",
      "\tPSNR: 20.787913963229315 \tSSIM: 0.7513678655311359\n",
      "[60.995770350708696, 77.19359442164358, 81.26155995484275, 84.41687147684814, 70.81795838653058]\n",
      "Epoch 7, Step 1000,\tTotal loss: 0.027525,\tHypernet loss: 0.027022\n",
      "\tPSNR: 20.86975043162492 \tSSIM: 0.7551047681298392\n",
      "[60.881182635568464, 75.30629136264865, 80.43064280416945, 85.4175673035929, 67.90155656123757]\n",
      "Epoch 7, Step 2000,\tTotal loss: 0.027395,\tHypernet loss: 0.026679\n",
      "\tPSNR: 20.945340995480255 \tSSIM: 0.7585476254989632\n",
      "[61.264757054626045, 80.00716101873502, 85.82466241685408, 90.6094747122183, 72.71011316397687]\n",
      "Epoch 7, Step 3000,\tTotal loss: 0.022421,\tHypernet loss: 0.021892\n",
      "\tPSNR: 21.015115475334714 \tSSIM: 0.7617596579895228\n",
      "[59.43550953794664, 82.02371004307611, 88.64624772472021, 94.60238971442087, 75.41292385073275]\n",
      "Epoch 8, Step 0,\tTotal loss: 0.025048,\tHypernet loss: 0.024523\n",
      "\tPSNR: 21.023394017311336 \tSSIM: 0.7621413066523056\n",
      "[57.00119568149689, 73.08420660350751, 78.70668388652956, 83.86856709942907, 66.96777748761488]\n",
      "Epoch 8, Step 1000,\tTotal loss: 0.024300,\tHypernet loss: 0.023645\n",
      "\tPSNR: 21.094016295606128 \tSSIM: 0.7653034409214653\n",
      "[61.81299527676341, 76.76998478449237, 80.08670889930653, 82.41288014588442, 71.0434453276222]\n",
      "Epoch 8, Step 2000,\tTotal loss: 0.027289,\tHypernet loss: 0.026643\n",
      "\tPSNR: 21.160244873123037 \tSSIM: 0.7682455410440193\n",
      "[57.92129836919705, 77.47006486371217, 82.99256816676686, 87.36929167192713, 71.133075342727]\n",
      "Epoch 8, Step 3000,\tTotal loss: 0.023888,\tHypernet loss: 0.023429\n",
      "\tPSNR: 21.22426359071157 \tSSIM: 0.7710232971302883\n",
      "[59.970104919149534, 76.26804792156011, 81.7435852181505, 87.16132909704928, 69.27952352377076]\n",
      "Epoch 9, Step 0,\tTotal loss: 0.022277,\tHypernet loss: 0.021846\n",
      "\tPSNR: 21.23195511104266 \tSSIM: 0.7713622404388835\n",
      "[61.31476915589537, 80.08903709628905, 85.44587964772259, 90.27054508672745, 73.72656333401896]\n",
      "Epoch 9, Step 1000,\tTotal loss: 0.024341,\tHypernet loss: 0.023890\n",
      "\tPSNR: 21.295780814902464 \tSSIM: 0.7741211104622199\n",
      "[64.85397202688193, 83.35610440261674, 88.7384908433822, 93.29561182949247, 75.41408789082692]\n",
      "Epoch 9, Step 2000,\tTotal loss: 0.028644,\tHypernet loss: 0.028119\n",
      "\tPSNR: 21.355196885373086 \tSSIM: 0.7766739890562038\n",
      "[60.03746195259633, 76.57310474088463, 82.63958166869136, 88.60433184227057, 70.06230046251756]\n",
      "Epoch 9, Step 3000,\tTotal loss: 0.021839,\tHypernet loss: 0.021426\n",
      "\tPSNR: 21.41147520305737 \tSSIM: 0.779078184719089\n",
      "[60.198496611510784, 78.18212633655068, 83.87839907846235, 89.27150290398986, 72.02064658809712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 21.418693832255364\n",
      "SSIM: 0.7793747039702162\n",
      "500000 50000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 17.547758929891586\n",
      "SSIM: 0.5754110272672027\n",
      "\n",
      "epoch:1\n",
      "PSNR: 19.74231793706894\n",
      "SSIM: 0.7037630205029249\n",
      "\n",
      "epoch:2\n",
      "PSNR: 20.656207956695557\n",
      "SSIM: 0.7510890162551404\n",
      "\n",
      "epoch:3\n",
      "PSNR: 21.286576559066773\n",
      "SSIM: 0.781410020403862\n",
      "\n",
      "epoch:4\n",
      "PSNR: 21.7465523620224\n",
      "SSIM: 0.8021818370044231\n",
      "\n",
      "epoch:5\n",
      "PSNR: 22.119077660236357\n",
      "SSIM: 0.8171990915524959\n",
      "\n",
      "epoch:6\n",
      "PSNR: 22.416906337623598\n",
      "SSIM: 0.8285210457319021\n",
      "\n",
      "epoch:7\n",
      "PSNR: 22.67175439588547\n",
      "SSIM: 0.837555394500494\n",
      "\n",
      "epoch:8\n",
      "PSNR: 22.90044386089325\n",
      "SSIM: 0.8451297107315063\n",
      "\n",
      "epoch:9\n",
      "PSNR: 23.099342323169708\n",
      "SSIM: 0.8514868757522106\n"
     ]
    }
   ],
   "source": [
    "steps_til_summary = 1000\n",
    "\n",
    "# optim = torch.optim.Adam(lr=5e-5, params=meta_siren.parameters())\n",
    "optim = torch.optim.Adam(lr=5e-6, params=meta_siren.parameters())\n",
    "\n",
    "hypernet_loss_multiplier = 1\n",
    "\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "for epoch in range(10):\n",
    "#     if epoch < 10:\n",
    "#         hypernet_loss_multiplier += 100\n",
    "\n",
    "    for step, sample in enumerate(dataloader):\n",
    "        sample = dict_to_gpu(sample)\n",
    "        '''\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param\n",
    "        '''\n",
    "        model_output, fast_params, meta_params, pred_specialized_param = meta_siren(sample)    \n",
    "        loss = ((model_output['model_out'] - sample['query']['y'])**2).mean() + hypernet_loss_multiplier * model_output['crossAttHypNet_loss']\n",
    "        \n",
    "#         for name in pred_specialized_param:\n",
    "#             loss += 10*((pred_specialized_param[name] - fast_params[name].detach()) ** 2).mean()\n",
    "        if False:\n",
    "            pred_specialized_param_corrected = OrderedDict()\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "                loss += 1*((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "       \n",
    "        \n",
    "        if (step % steps_til_summary == 0) and (epoch % 1 == 0): \n",
    "            print(\"Epoch %d, Step %d,\\tTotal loss: %0.6f,\\tHypernet loss: %0.6f\" % (epoch, step, loss, model_output['crossAttHypNet_loss']))\n",
    "            print('\\tPSNR:', np.mean(psnr_list), '\\tSSIM:', np.mean(ssim_list))\n",
    "            fig, axes = [], list(range(6))#plt.subplots(1,6, figsize=(36,6))\n",
    "            ax_titles = ['Learned Initialization', 'Inner step 1 output', \n",
    "                        'Inner step 2 output', 'Inner step 3 output', \n",
    "                        'HyperNet output', ## added by me\n",
    "                        'Ground Truth']\n",
    "            images = []\n",
    "            for i, inner_step_out in enumerate(model_output['intermed_predictions']):\n",
    "                img = plot_sample_image(inner_step_out, ax=axes[i])\n",
    "                images += [img]\n",
    "#                 axes[i].set_title(ax_titles[i], fontsize=25)\n",
    "            images += [plot_sample_image(model_output['model_out'], ax=axes[-3])]\n",
    "#             axes[-3].set_title(ax_titles[-3], fontsize=25)\n",
    "\n",
    "            if True:\n",
    "                images += [plot_sample_image(model_output['model_output_hypernet'], ax=axes[-2])]\n",
    "#                 axes[-2].set_title(ax_titles[-2], fontsize=25)\n",
    "\n",
    "            img_ground_truth = plot_sample_image(sample['query']['y'], ax=axes[-1])\n",
    "#             axes[-1].set_title(ax_titles[-1], fontsize=25)\n",
    "            psnrs = [cv2.PSNR(img_ground_truth, img) for img in images]\n",
    "            print(psnrs)\n",
    "            plt.show()\n",
    "\n",
    "        if True:\n",
    "            x = lin2img(model_output['model_output_hypernet']).permute(0,3,1,2).contiguous()#.cpu().detach()\n",
    "            x += 1.\n",
    "            x /= 2.\n",
    "            x = torch.clip(x, 0., 1.)\n",
    "            y = lin2img(sample['query']['y']).permute(0,3,1,2).contiguous()#.cpu().detach()\n",
    "            y += 1.\n",
    "            y /= 2.\n",
    "            y = torch.clip(y, 0., 1.)\n",
    "    #         print(x.shape, lin2img(x).shape)\n",
    "#             print(y.min(), y.max())\n",
    "#             print(x.min(), x.max())\n",
    "\n",
    "#             print('PSNR:', psnr(y, x).mean())\n",
    "#             print('psnr_sklearn:', psnr_sklearn(y.numpy(), x.numpy(), data_range=1.))\n",
    "#             print('SSIM:', ssim_metric(y, x))\n",
    "            psnr_temp = psnr(y, x)\n",
    "            ssim_temp = ssim_metric(y, x)\n",
    "            psnr_list += psnr_temp.cpu().detach().numpy().tolist()\n",
    "            ssim_list += ssim_temp.cpu().detach().numpy().tolist()\n",
    "#             psnr_temp = psnr_temp.mean()\n",
    "#             ssim_temp = ssim_temp.mean()\n",
    "#             loss += 1 * (torch.exp(-1. * psnr_temp)).mean()\n",
    "            loss += 1 * (1. - ssim_temp).mean()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        del model_output, fast_params, meta_params, pred_specialized_param\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "print('PSNR:', np.mean(psnr_list))\n",
    "print('SSIM:', np.mean(ssim_list))\n",
    "\n",
    "l = len(psnr_list) // 10\n",
    "print(len(psnr_list), l)\n",
    "for i in range(10):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(img_siren.state_dict(), 'img_siren_ssim_loss_1.pth')\n",
    "torch.save(crossAttHypNet.state_dict(), 'crossAttHypNet_ssim_loss_1.pth')\n",
    "torch.save(meta_siren.state_dict(), 'meta_siren_ssim_ssim_1.pth')\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "import json\n",
    "json.dump({'psnr_list': psnr_list, 'ssim_list': ssim_list}, open('psnr_ssim_list_hypernet+ssim_loss.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9344650506973267,\n",
       " 0.9029501676559448,\n",
       " 0.907914400100708,\n",
       " 0.748037576675415,\n",
       " 0.9377844333648682,\n",
       " 0.8202025890350342,\n",
       " 0.8790472745895386,\n",
       " 0.8672150373458862,\n",
       " 0.8413331508636475,\n",
       " 0.8708105087280273,\n",
       " 0.9003541469573975,\n",
       " 0.8264485597610474,\n",
       " 0.8508522510528564,\n",
       " 0.75550377368927,\n",
       " 0.8427578210830688,\n",
       " 0.8951382637023926,\n",
       " 0.868256688117981,\n",
       " 0.8538165092468262,\n",
       " 0.8058369159698486,\n",
       " 0.8353908061981201]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim_list[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 50000\n",
      "\n",
      "epoch:0\n",
      "PSNR: 17.547758929891586\n",
      "SSIM: 0.5754110272672027\n",
      "\n",
      "epoch:1\n",
      "PSNR: 19.74231793706894\n",
      "SSIM: 0.7037630205029249\n",
      "\n",
      "epoch:2\n",
      "PSNR: 20.656207956695557\n",
      "SSIM: 0.7510890162551404\n",
      "\n",
      "epoch:3\n",
      "PSNR: 21.286576559066773\n",
      "SSIM: 0.781410020403862\n",
      "\n",
      "epoch:4\n",
      "PSNR: 21.7465523620224\n",
      "SSIM: 0.8021818370044231\n",
      "\n",
      "epoch:5\n",
      "PSNR: 22.119077660236357\n",
      "SSIM: 0.8171990915524959\n",
      "\n",
      "epoch:6\n",
      "PSNR: 22.416906337623598\n",
      "SSIM: 0.8285210457319021\n",
      "\n",
      "epoch:7\n",
      "PSNR: 22.67175439588547\n",
      "SSIM: 0.837555394500494\n",
      "\n",
      "epoch:8\n",
      "PSNR: 22.90044386089325\n",
      "SSIM: 0.8451297107315063\n",
      "\n",
      "epoch:9\n",
      "PSNR: 23.099342323169708\n",
      "SSIM: 0.8514868757522106\n"
     ]
    }
   ],
   "source": [
    "l = len(psnr_list) // 10\n",
    "print(len(psnr_list), l)\n",
    "for i in range(10):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l4Ca0dY2xr2w"
   },
   "outputs": [],
   "source": [
    "# # fast_params['net.0.linear.weight'].requires_grad\n",
    "# # fast_params.keys()\n",
    "# # fast_params['net.1.linear.weight'].shape \n",
    "# # for key in fast_params:\n",
    "# #     print(fast_params[key].shape)\n",
    "# for key in meta_params:\n",
    "#     print(key, ':\\t', meta_params[key].shape, '\\t', fast_params[key].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT1Qy7983EbD"
   },
   "source": [
    "As you can see, after a few hundred steps of training, we can fit any of the Cifar-10 images in only three gradient descent steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Kryu4N59GVaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SingleBVPNet(\n",
      "  (image_downsampling): ImageDownsampling()\n",
      "  (net): FCBlock(\n",
      "    (net): MetaSequential(\n",
      "      (0): MetaSequential(\n",
      "        (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (1): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (2): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (3): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Sine()\n",
      "      )\n",
      "      (4): MetaSequential(\n",
      "        (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvolutionalNeuralProcessImplicit2DHypernet(\n",
      "  (encoder): ConvImgEncoder(\n",
      "    (conv_theta): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (3): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (4): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (5): Conv2dResBlock(\n",
      "        (convs): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (final_relu): ReLU()\n",
      "      )\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (hypo_net): SingleBVPNet(\n",
      "    (image_downsampling): ImageDownsampling()\n",
      "    (net): FCBlock(\n",
      "      (net): MetaSequential(\n",
      "        (0): MetaSequential(\n",
      "          (0): BatchLinear(in_features=2, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (1): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (2): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (3): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Sine()\n",
      "        )\n",
      "        (4): MetaSequential(\n",
      "          (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_net): HyperNetwork(\n",
      "    (nets): ModuleList(\n",
      "      (0): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): FCBlock(\n",
      "        (net): MetaSequential(\n",
      "          (0): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): MetaSequential(\n",
      "            (0): BatchLinear(in_features=256, out_features=3, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66906387\n",
      "Files already downloaded and verified\n",
      "meta: 0.012998580932617188  0.01599860191345215, hyper: 0.006999969482421875 0.009999990463256836, all: 0.022998571395874023\n",
      "0.023998737335205078\n",
      "Epoch 0, Step 0,\tTotal loss: 0.381722,\tHypernet loss: 0.025654\n",
      "\tPSNR: nan \tSSIM: nan\n",
      "[60.92382756007669, 64.9129061220766, 66.835854163684, 59.38227988700565, 68.22732945132158]\n",
      "meta: 0.013069629669189453  0.015069961547851562, hyper: 0.006064653396606445 0.008064985275268555, all: 0.021134614944458008\n",
      "0.02213430404663086\n",
      "meta: 0.012187480926513672  0.013185739517211914, hyper: 0.007002115249633789 0.008000373840332031, all: 0.020187854766845703\n",
      "0.021189451217651367\n",
      "meta: 0.011001348495483398  0.013000011444091797, hyper: 0.006997346878051758 0.008996009826660156, all: 0.019997358322143555\n",
      "0.020998001098632812\n",
      "meta: 0.012048721313476562  0.014049768447875977, hyper: 0.005075216293334961 0.007076263427734375, all: 0.019124984741210938\n",
      "0.020125150680541992\n",
      "meta: 0.011075735092163086  0.012075662612915039, hyper: 0.0060007572174072266 0.00700068473815918, all: 0.018076419830322266\n",
      "0.019122838973999023\n",
      "meta: 0.010999917984008789  0.013000011444091797, hyper: 0.00599980354309082 0.007999897003173828, all: 0.018999814987182617\n",
      "0.018999814987182617\n",
      "meta: 0.010073184967041016  0.012073040008544922, hyper: 0.005112409591674805 0.007112264633178711, all: 0.017185449600219727\n",
      "0.01918482780456543\n",
      "meta: 0.01111912727355957  0.012115478515625, hyper: 0.00600433349609375 0.00700068473815918, all: 0.01811981201171875\n",
      "0.01912236213684082\n",
      "meta: 0.012072086334228516  0.014073610305786133, hyper: 0.006064891815185547 0.008066415786743164, all: 0.02013850212097168\n",
      "0.02113795280456543\n",
      "meta: 0.010076761245727539  0.012106895446777344, hyper: 0.006111860275268555 0.00814199447631836, all: 0.0182187557220459\n",
      "0.01921367645263672\n",
      "meta: 0.011997699737548828  0.013998746871948242, hyper: 0.006000041961669922 0.008001089096069336, all: 0.019998788833618164\n",
      "0.020998239517211914\n",
      "meta: 0.011000394821166992  0.013000011444091797, hyper: 0.004999876022338867 0.006999492645263672, all: 0.017999887466430664\n",
      "0.01900005340576172\n",
      "meta: 0.011198043823242188  0.01219797134399414, hyper: 0.006051540374755859 0.0070514678955078125, all: 0.01824951171875\n",
      "0.019249439239501953\n",
      "meta: 0.011000871658325195  0.013001441955566406, hyper: 0.006069660186767578 0.008070230484008789, all: 0.019071102142333984\n",
      "0.020073890686035156\n",
      "meta: 0.012061595916748047  0.01406097412109375, hyper: 0.0060007572174072266 0.00800013542175293, all: 0.020061731338500977\n",
      "0.021061182022094727\n",
      "meta: 0.012109041213989258  0.01410984992980957, hyper: 0.0060040950775146484 0.008004903793334961, all: 0.02011394500732422\n",
      "0.021113157272338867\n",
      "meta: 0.012044906616210938  0.013043403625488281, hyper: 0.005999326705932617 0.006997823715209961, all: 0.0190427303314209\n",
      "0.02004265785217285\n",
      "meta: 0.011085033416748047  0.012083053588867188, hyper: 0.0060122013092041016 0.007010221481323242, all: 0.01809525489807129\n",
      "0.019084930419921875\n",
      "meta: 0.011075735092163086  0.012076139450073242, hyper: 0.005072593688964844 0.006072998046875, all: 0.017148733139038086\n",
      "0.018148183822631836\n",
      "meta: 0.011995315551757812  0.013013601303100586, hyper: 0.0069997310638427734 0.008018016815185547, all: 0.02001333236694336\n",
      "0.020996809005737305\n",
      "meta: 0.011001110076904297  0.015999555587768555, hyper: 0.007001161575317383 0.01199960708618164, all: 0.023000717163085938\n",
      "0.0240018367767334\n",
      "meta: 0.011095523834228516  0.013096094131469727, hyper: 0.00610661506652832 0.008107185363769531, all: 0.019202709197998047\n",
      "0.020199298858642578\n",
      "meta: 0.011121988296508789  0.012121200561523438, hyper: 0.007000923156738281 0.00800013542175293, all: 0.01912212371826172\n",
      "0.020185470581054688\n",
      "meta: 0.012056589126586914  0.014070987701416016, hyper: 0.005000114440917969 0.00701451301574707, all: 0.019071102142333984\n",
      "0.020056962966918945\n",
      "meta: 0.01110386848449707  0.012102603912353516, hyper: 0.0060977935791015625 0.007096529006958008, all: 0.018200397491455078\n",
      "0.019202470779418945\n",
      "meta: 0.012340307235717773  0.013339996337890625, hyper: 0.0060770511627197266 0.007076740264892578, all: 0.01941704750061035\n",
      "0.02041769027709961\n",
      "meta: 0.011083602905273438  0.01208353042602539, hyper: 0.00599980354309082 0.0069997310638427734, all: 0.01808333396911621\n",
      "0.019083499908447266\n",
      "meta: 0.011999368667602539  0.013998270034790039, hyper: 0.00700068473815918 0.00899958610534668, all: 0.02099895477294922\n",
      "0.021999120712280273\n",
      "meta: 0.012137413024902344  0.014124155044555664, hyper: 0.006012916564941406 0.007999658584594727, all: 0.02013707160949707\n",
      "0.021142005920410156\n",
      "meta: 0.010998249053955078  0.012997150421142578, hyper: 0.006002187728881836 0.008001089096069336, all: 0.018999338150024414\n",
      "0.019998788833618164\n",
      "meta: 0.012388467788696289  0.013397455215454102, hyper: 0.005086660385131836 0.0060956478118896484, all: 0.018484115600585938\n",
      "0.019473791122436523\n",
      "meta: 0.011978626251220703  0.013979196548461914, hyper: 0.006860494613647461 0.008861064910888672, all: 0.020839691162109375\n",
      "0.020839691162109375\n",
      "meta: 0.011138916015625  0.012139081954956055, hyper: 0.005995988845825195 0.00699615478515625, all: 0.01813507080078125\n",
      "0.019134998321533203\n",
      "meta: 0.010999202728271484  0.012999296188354492, hyper: 0.006000041961669922 0.00800013542175293, all: 0.018999338150024414\n",
      "0.019997596740722656\n",
      "meta: 0.011996746063232422  0.01399683952331543, hyper: 0.006000041961669922 0.00800013542175293, all: 0.01999688148498535\n",
      "0.02099895477294922\n",
      "meta: 0.01100015640258789  0.013014554977416992, hyper: 0.005999565124511719 0.00801396369934082, all: 0.01901412010192871\n",
      "0.01999831199645996\n",
      "meta: 0.011070489883422852  0.013070344924926758, hyper: 0.005054473876953125 0.007054328918457031, all: 0.018124818801879883\n",
      "0.018124818801879883\n",
      "meta: 0.01012563705444336  0.011125326156616211, hyper: 0.006165027618408203 0.007164716720581055, all: 0.017290353775024414\n",
      "0.018300294876098633\n",
      "meta: 0.011098384857177734  0.013098001480102539, hyper: 0.0050547122955322266 0.007054328918457031, all: 0.018152713775634766\n",
      "0.01915287971496582\n",
      "meta: 0.010042190551757812  0.011042118072509766, hyper: 0.00599980354309082 0.0069997310638427734, all: 0.017041921615600586\n",
      "0.018042325973510742\n",
      "meta: 0.010071039199829102  0.012070894241333008, hyper: 0.005999326705932617 0.007999181747436523, all: 0.018070220947265625\n",
      "0.0190889835357666\n",
      "meta: 0.011090755462646484  0.01308131217956543, hyper: 0.005009174346923828 0.0069997310638427734, all: 0.018090486526489258\n",
      "0.019090652465820312\n",
      "meta: 0.011142253875732422  0.013143539428710938, hyper: 0.004998445510864258 0.0069997310638427734, all: 0.018141984939575195\n",
      "0.01914238929748535\n",
      "meta: 0.011089563369750977  0.013089418411254883, hyper: 0.005057811737060547 0.007057666778564453, all: 0.01814723014831543\n",
      "0.019147634506225586\n",
      "meta: 0.013124704360961914  0.014398574829101562, hyper: 0.0060350894927978516 0.0073089599609375, all: 0.020433664321899414\n",
      "0.022230148315429688\n",
      "meta: 0.012086868286132812  0.014075517654418945, hyper: 0.0050051212310791016 0.006993770599365234, all: 0.019080638885498047\n",
      "0.020142316818237305\n",
      "meta: 0.01109004020690918  0.012089967727661133, hyper: 0.004999876022338867 0.00599980354309082, all: 0.01708984375\n",
      "0.018147706985473633\n",
      "meta: 0.011083841323852539  0.013083934783935547, hyper: 0.0070002079010009766 0.009000301361083984, all: 0.020084142684936523\n",
      "0.021084308624267578\n",
      "meta: 0.012074947357177734  0.013854026794433594, hyper: 0.006077766418457031 0.00785684585571289, all: 0.019931793212890625\n",
      "0.020954370498657227\n",
      "meta: 0.01104593276977539  0.013099908828735352, hyper: 0.004999637603759766 0.0070536136627197266, all: 0.018099546432495117\n",
      "0.01909947395324707\n",
      "meta: 0.010999679565429688  0.012987375259399414, hyper: 0.006011247634887695 0.007998943328857422, all: 0.01899862289428711\n",
      "0.019998550415039062\n",
      "meta: 0.010999917984008789  0.012000560760498047, hyper: 0.006000041961669922 0.00700068473815918, all: 0.01800060272216797\n",
      "0.019002199172973633\n",
      "meta: 0.011174440383911133  0.013173341751098633, hyper: 0.005045652389526367 0.007044553756713867, all: 0.018218994140625\n",
      "0.018218994140625\n",
      "meta: 0.009998798370361328  0.010998964309692383, hyper: 0.00599980354309082 0.006999969482421875, all: 0.016998767852783203\n",
      "0.017998695373535156\n",
      "meta: 0.010059356689453125  0.011059284210205078, hyper: 0.00604701042175293 0.007046937942504883, all: 0.017106294631958008\n",
      "0.018106698989868164\n",
      "meta: 0.011066675186157227  0.012066364288330078, hyper: 0.00500035285949707 0.006000041961669922, all: 0.01706671714782715\n",
      "0.01810145378112793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta: 0.011101722717285156  0.012101411819458008, hyper: 0.006001949310302734 0.007001638412475586, all: 0.018103361129760742\n",
      "0.0191037654876709\n",
      "meta: 0.011313438415527344  0.012313127517700195, hyper: 0.00599980354309082 0.006999492645263672, all: 0.018312931060791016\n",
      "0.019313812255859375\n",
      "meta: 0.011151552200317383  0.013150453567504883, hyper: 0.006002187728881836 0.008001089096069336, all: 0.01915264129638672\n",
      "0.02015209197998047\n",
      "meta: 0.01110982894897461  0.013170957565307617, hyper: 0.006000995635986328 0.008062124252319336, all: 0.019171953201293945\n",
      "0.020175933837890625\n",
      "meta: 0.01102900505065918  0.013030290603637695, hyper: 0.006047964096069336 0.008049249649047852, all: 0.01907825469970703\n",
      "0.020074129104614258\n",
      "meta: 0.011067628860473633  0.01312112808227539, hyper: 0.005001068115234375 0.007054567337036133, all: 0.018122196197509766\n",
      "0.019122600555419922\n",
      "meta: 0.010067462921142578  0.011065483093261719, hyper: 0.007002592086791992 0.008000612258911133, all: 0.01806807518005371\n",
      "0.019069433212280273\n",
      "meta: 0.010097980499267578  0.011109113693237305, hyper: 0.006000041961669922 0.0070111751556396484, all: 0.017109155654907227\n",
      "0.018097639083862305\n",
      "meta: 0.011142730712890625  0.013142824172973633, hyper: 0.005013704299926758 0.007013797760009766, all: 0.01815652847290039\n",
      "0.019156694412231445\n",
      "meta: 0.011074304580688477  0.013075113296508789, hyper: 0.006053924560546875 0.008054733276367188, all: 0.019129037857055664\n",
      "0.020128726959228516\n",
      "meta: 0.01009678840637207  0.011096715927124023, hyper: 0.006069660186767578 0.007069587707519531, all: 0.0171663761138916\n",
      "0.018166542053222656\n",
      "meta: 0.01110982894897461  0.012109756469726562, hyper: 0.005040168762207031 0.006040096282958984, all: 0.017149925231933594\n",
      "0.01815009117126465\n",
      "meta: 0.011046171188354492  0.013084650039672852, hyper: 0.005000591278076172 0.007039070129394531, all: 0.018085241317749023\n",
      "0.019083738327026367\n",
      "meta: 0.010058164596557617  0.012096881866455078, hyper: 0.00500035285949707 0.007039070129394531, all: 0.01709723472595215\n",
      "0.01809859275817871\n",
      "meta: 0.011081695556640625  0.013081789016723633, hyper: 0.0060596466064453125 0.00805974006652832, all: 0.019141435623168945\n",
      "0.020140886306762695\n",
      "meta: 0.010056734085083008  0.01105809211730957, hyper: 0.006058692932128906 0.007060050964355469, all: 0.017116785049438477\n",
      "0.01811695098876953\n",
      "meta: 0.010043621063232422  0.011044979095458984, hyper: 0.0060575008392333984 0.007058858871459961, all: 0.017102479934692383\n",
      "0.018102645874023438\n",
      "meta: 0.011083364486694336  0.013083219528198242, hyper: 0.005042552947998047 0.007042407989501953, all: 0.01812577247619629\n",
      "0.019125938415527344\n",
      "meta: 0.010102510452270508  0.012102365493774414, hyper: 0.006064891815185547 0.008064746856689453, all: 0.01816725730895996\n",
      "0.01816725730895996\n",
      "meta: 0.010063409805297852  0.012078285217285156, hyper: 0.006050586700439453 0.008065462112426758, all: 0.01812887191772461\n",
      "0.023127317428588867\n",
      "meta: 0.011049032211303711  0.012048721313476562, hyper: 0.007052898406982422 0.008052587509155273, all: 0.019101619720458984\n",
      "0.020101547241210938\n",
      "meta: 0.011042594909667969  0.013042449951171875, hyper: 0.0060002803802490234 0.00800013542175293, all: 0.0190427303314209\n",
      "0.02004265785217285\n",
      "meta: 0.012063741683959961  0.014065027236938477, hyper: 0.005053281784057617 0.007054567337036133, all: 0.019118309020996094\n",
      "0.020117759704589844\n",
      "meta: 0.011115312576293945  0.012114524841308594, hyper: 0.006054878234863281 0.00705409049987793, all: 0.018169403076171875\n",
      "0.02016615867614746\n",
      "meta: 0.012085199356079102  0.014084577560424805, hyper: 0.006055593490600586 0.008054971694946289, all: 0.02014017105102539\n",
      "0.02014017105102539\n",
      "meta: 0.010073184967041016  0.011072874069213867, hyper: 0.006150245666503906 0.007149934768676758, all: 0.017223119735717773\n",
      "0.01822185516357422\n",
      "meta: 0.010072469711303711  0.01107168197631836, hyper: 0.00607752799987793 0.007076740264892578, all: 0.01714920997619629\n",
      "0.018149852752685547\n",
      "meta: 0.011080503463745117  0.012080907821655273, hyper: 0.006001949310302734 0.007002353668212891, all: 0.018082857131958008\n",
      "0.01908254623413086\n",
      "meta: 0.01005864143371582  0.011058330535888672, hyper: 0.006067752838134766 0.007067441940307617, all: 0.017126083374023438\n",
      "0.018125057220458984\n",
      "meta: 0.011078834533691406  0.013079643249511719, hyper: 0.006239652633666992 0.008240461349487305, all: 0.01931929588317871\n",
      "0.020318269729614258\n",
      "meta: 0.01015782356262207  0.012157678604125977, hyper: 0.0070607662200927734 0.00906062126159668, all: 0.01921844482421875\n",
      "0.020218372344970703\n",
      "meta: 0.011131763458251953  0.012142658233642578, hyper: 0.005000114440917969 0.006011009216308594, all: 0.017142772674560547\n",
      "0.018208026885986328\n",
      "meta: 0.011085033416748047  0.013085365295410156, hyper: 0.004999399185180664 0.0069997310638427734, all: 0.01808476448059082\n",
      "0.019135713577270508\n",
      "meta: 0.011059284210205078  0.013112545013427734, hyper: 0.004999637603759766 0.007052898406982422, all: 0.0181121826171875\n",
      "0.0181121826171875\n",
      "meta: 0.011089801788330078  0.013212919235229492, hyper: 0.004998445510864258 0.007121562957763672, all: 0.01821136474609375\n",
      "0.019213199615478516\n",
      "meta: 0.01105952262878418  0.013059377670288086, hyper: 0.00500035285949707 0.0070002079010009766, all: 0.018059730529785156\n",
      "0.018059730529785156\n",
      "meta: 0.011076688766479492  0.013074874877929688, hyper: 0.005002260208129883 0.007000446319580078, all: 0.01807713508605957\n",
      "0.01912093162536621\n",
      "meta: 0.011067628860473633  0.012067317962646484, hyper: 0.006000041961669922 0.0069997310638427734, all: 0.018067359924316406\n",
      "0.01906609535217285\n",
      "meta: 0.01116490364074707  0.013166666030883789, hyper: 0.004999876022338867 0.007001638412475586, all: 0.018166542053222656\n",
      "0.018166542053222656\n",
      "meta: 0.011119604110717773  0.012118339538574219, hyper: 0.006001472473144531 0.0070002079010009766, all: 0.01811981201171875\n",
      "0.01912212371826172\n",
      "meta: 0.011087179183959961  0.012087106704711914, hyper: 0.00603795051574707 0.0070378780364990234, all: 0.018125057220458984\n",
      "0.019125699996948242\n",
      "meta: 0.010140419006347656  0.012131452560424805, hyper: 0.005069255828857422 0.00706028938293457, all: 0.017200708389282227\n",
      "0.01820063591003418\n",
      "meta: 0.010065555572509766  0.011065483093261719, hyper: 0.006135702133178711 0.007135629653930664, all: 0.01720118522644043\n",
      "0.018201589584350586\n",
      "meta: 0.011006355285644531  0.012007474899291992, hyper: 0.006053447723388672 0.007054567337036133, all: 0.018060922622680664\n",
      "0.01906132698059082\n",
      "Epoch 0, Step 100,\tTotal loss: 0.392661,\tHypernet loss: 0.035017\n",
      "\tPSNR: 22.88543518066406 \tSSIM: 0.8436399166285992\n",
      "[60.55264922272812, 64.96425176890551, 65.90498206116204, 59.15029911045525, 71.10792556178819]\n",
      "meta: 0.011050701141357422  0.014098882675170898, hyper: 0.005999326705932617 0.009047508239746094, all: 0.020098209381103516\n",
      "0.021097898483276367\n",
      "meta: 0.011080503463745117  0.013080120086669922, hyper: 0.00500035285949707 0.006999969482421875, all: 0.018080472946166992\n",
      "0.019080162048339844\n",
      "meta: 0.011089324951171875  0.013127803802490234, hyper: 0.005001068115234375 0.007039546966552734, all: 0.01812887191772461\n",
      "0.019129037857055664\n",
      "meta: 0.011075258255004883  0.013077735900878906, hyper: 0.006059169769287109 0.008061647415161133, all: 0.019136905670166016\n",
      "0.020138025283813477\n",
      "meta: 0.011097908020019531  0.012150287628173828, hyper: 0.007001399993896484 0.008053779602050781, all: 0.019151687622070312\n",
      "0.021151065826416016\n",
      "meta: 0.011112689971923828  0.013121604919433594, hyper: 0.005198955535888672 0.0072078704833984375, all: 0.018320560455322266\n",
      "0.018320560455322266\n",
      "meta: 0.011081457138061523  0.01308441162109375, hyper: 0.005060434341430664 0.007063388824462891, all: 0.018144845962524414\n",
      "0.01914501190185547\n",
      "meta: 0.011100530624389648  0.013100624084472656, hyper: 0.005032539367675781 0.007032632827758789, all: 0.018133163452148438\n",
      "0.01913309097290039\n",
      "meta: 0.010184049606323242  0.01218414306640625, hyper: 0.004999876022338867 0.006999969482421875, all: 0.017184019088745117\n",
      "0.018184185028076172\n",
      "meta: 0.011087894439697266  0.012087821960449219, hyper: 0.006000041961669922 0.006999969482421875, all: 0.01808786392211914\n",
      "0.019088029861450195\n",
      "meta: 0.010121345520019531  0.011121273040771484, hyper: 0.006001472473144531 0.007001399993896484, all: 0.017122745513916016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01812291145324707\n",
      "meta: 0.011090993881225586  0.01309061050415039, hyper: 0.006000518798828125 0.00800013542175293, all: 0.019091129302978516\n",
      "0.019091129302978516\n",
      "meta: 0.011063575744628906  0.01206350326538086, hyper: 0.0060405731201171875 0.007040500640869141, all: 0.018104076385498047\n",
      "0.0191042423248291\n",
      "meta: 0.011108160018920898  0.012108564376831055, hyper: 0.006045341491699219 0.007045745849609375, all: 0.018153905868530273\n",
      "0.01915287971496582\n",
      "meta: 0.011105775833129883  0.013105630874633789, hyper: 0.006063699722290039 0.008063554763793945, all: 0.019169330596923828\n",
      "0.02017068862915039\n",
      "meta: 0.011082887649536133  0.014082670211791992, hyper: 0.006056547164916992 0.009056329727172852, all: 0.020139217376708984\n",
      "0.021138429641723633\n",
      "meta: 0.01132655143737793  0.013325691223144531, hyper: 0.006036996841430664 0.008036136627197266, all: 0.019362688064575195\n",
      "0.02036428451538086\n",
      "meta: 0.011066675186157227  0.013066768646240234, hyper: 0.0050008296966552734 0.007000923156738281, all: 0.018067598342895508\n",
      "0.018067598342895508\n",
      "meta: 0.011214971542358398  0.012224197387695312, hyper: 0.00601959228515625 0.007028818130493164, all: 0.018243789672851562\n",
      "0.019242525100708008\n",
      "meta: 0.012078523635864258  0.014102697372436523, hyper: 0.005087137222290039 0.007111310958862305, all: 0.019189834594726562\n",
      "0.020191669464111328\n",
      "meta: 0.011180400848388672  0.013237714767456055, hyper: 0.004999637603759766 0.0070569515228271484, all: 0.01823735237121582\n",
      "0.01923513412475586\n",
      "meta: 0.011072158813476562  0.013072490692138672, hyper: 0.004999637603759766 0.006999969482421875, all: 0.018072128295898438\n",
      "0.019144773483276367\n",
      "meta: 0.010070323944091797  0.011070013046264648, hyper: 0.00600123405456543 0.007000923156738281, all: 0.017071247100830078\n",
      "0.018071889877319336\n",
      "meta: 0.01208806037902832  0.013133049011230469, hyper: 0.00500178337097168 0.006046772003173828, all: 0.01813483238220215\n",
      "0.0191347599029541\n",
      "meta: 0.010064125061035156  0.011064529418945312, hyper: 0.006047248840332031 0.0070476531982421875, all: 0.017111778259277344\n",
      "0.018111467361450195\n",
      "meta: 0.012059926986694336  0.014060020446777344, hyper: 0.0051479339599609375 0.007148027420043945, all: 0.01920795440673828\n",
      "0.01920795440673828\n",
      "meta: 0.010079383850097656  0.01107931137084961, hyper: 0.006079912185668945 0.0070798397064208984, all: 0.017159223556518555\n",
      "0.018158912658691406\n",
      "meta: 0.01107025146484375  0.013069868087768555, hyper: 0.005078792572021484 0.007078409194946289, all: 0.01814866065979004\n",
      "0.01814866065979004\n",
      "meta: 0.011168956756591797  0.01316976547241211, hyper: 0.0060002803802490234 0.008001089096069336, all: 0.019170045852661133\n",
      "0.021220684051513672\n",
      "meta: 0.01106882095336914  0.013059139251708984, hyper: 0.005009651184082031 0.006999969482421875, all: 0.018068790435791016\n",
      "0.019078969955444336\n",
      "meta: 0.011094093322753906  0.012091875076293945, hyper: 0.006093263626098633 0.007091045379638672, all: 0.018185138702392578\n",
      "0.01918649673461914\n",
      "meta: 0.012000322341918945  0.01399993896484375, hyper: 0.007997512817382812 0.009997129440307617, all: 0.021997451782226562\n",
      "0.022996902465820312\n",
      "meta: 0.010066509246826172  0.011129379272460938, hyper: 0.00600123405456543 0.007064104080200195, all: 0.017130613327026367\n",
      "0.018135547637939453\n",
      "meta: 0.011074304580688477  0.012085676193237305, hyper: 0.0049991607666015625 0.006010532379150391, all: 0.017084836959838867\n",
      "0.018072843551635742\n",
      "meta: 0.011175394058227539  0.012175559997558594, hyper: 0.006003379821777344 0.0070035457611083984, all: 0.018178939819335938\n",
      "0.020231008529663086\n",
      "meta: 0.011062383651733398  0.012115716934204102, hyper: 0.006009101867675781 0.007062435150146484, all: 0.018124818801879883\n",
      "0.019153594970703125\n",
      "meta: 0.011066675186157227  0.012076616287231445, hyper: 0.005070924758911133 0.0060808658599853516, all: 0.017147541046142578\n",
      "0.018146514892578125\n",
      "meta: 0.012049436569213867  0.014049768447875977, hyper: 0.006073951721191406 0.008074283599853516, all: 0.020123720169067383\n",
      "0.02112293243408203\n",
      "meta: 0.0101165771484375  0.012128353118896484, hyper: 0.005054950714111328 0.0070667266845703125, all: 0.017183303833007812\n",
      "0.018183469772338867\n",
      "meta: 0.012059450149536133  0.014127731323242188, hyper: 0.005011558532714844 0.0070798397064208984, all: 0.01913928985595703\n",
      "0.020141124725341797\n",
      "meta: 0.01106715202331543  0.013065338134765625, hyper: 0.004999876022338867 0.0069980621337890625, all: 0.018065214157104492\n",
      "0.019066810607910156\n",
      "meta: 0.011062860488891602  0.012063026428222656, hyper: 0.00599980354309082 0.006999969482421875, all: 0.018062829971313477\n",
      "0.01910543441772461\n",
      "meta: 0.011163949966430664  0.013204574584960938, hyper: 0.0050013065338134766 0.00704193115234375, all: 0.018205881118774414\n",
      "0.019205808639526367\n",
      "meta: 0.011085748672485352  0.013086080551147461, hyper: 0.0060002803802490234 0.008000612258911133, all: 0.019086360931396484\n",
      "0.019086360931396484\n",
      "meta: 0.011066913604736328  0.013067483901977539, hyper: 0.006073951721191406 0.008074522018432617, all: 0.019141435623168945\n",
      "0.0201416015625\n",
      "meta: 0.011081933975219727  0.012081623077392578, hyper: 0.0060558319091796875 0.007055521011352539, all: 0.018137454986572266\n",
      "0.01913762092590332\n",
      "meta: 0.010059595108032227  0.012059688568115234, hyper: 0.0053195953369140625 0.00731968879699707, all: 0.017379283905029297\n",
      "0.018378257751464844\n",
      "meta: 0.011208057403564453  0.01320791244506836, hyper: 0.004998683929443359 0.006998538970947266, all: 0.01820659637451172\n",
      "0.019206762313842773\n",
      "meta: 0.012126684188842773  0.014127016067504883, hyper: 0.006055116653442383 0.008055448532104492, all: 0.020182132720947266\n",
      "0.02118372917175293\n",
      "meta: 0.010996103286743164  0.011996984481811523, hyper: 0.006001949310302734 0.007002830505371094, all: 0.017998933792114258\n",
      "0.018995046615600586\n",
      "meta: 0.01210474967956543  0.014104366302490234, hyper: 0.005002021789550781 0.007001638412475586, all: 0.019106388092041016\n",
      "0.02010512351989746\n",
      "meta: 0.011128902435302734  0.013128519058227539, hyper: 0.00504612922668457 0.007045745849609375, all: 0.01817464828491211\n",
      "0.019173383712768555\n",
      "meta: 0.012068033218383789  0.014067649841308594, hyper: 0.005997657775878906 0.007997274398803711, all: 0.0200653076171875\n",
      "0.02106499671936035\n",
      "meta: 0.01107168197631836  0.012071609497070312, hyper: 0.006011486053466797 0.00701141357421875, all: 0.01808309555053711\n",
      "0.019070148468017578\n",
      "meta: 0.011052846908569336  0.012051820755004883, hyper: 0.006000995635986328 0.006999969482421875, all: 0.01805281639099121\n",
      "0.01911163330078125\n",
      "meta: 0.013070821762084961  0.015073776245117188, hyper: 0.004998207092285156 0.007001161575317383, all: 0.020071983337402344\n",
      "0.021070241928100586\n",
      "meta: 0.011095285415649414  0.01315450668334961, hyper: 0.006999492645263672 0.009058713912963867, all: 0.02015399932861328\n",
      "0.02015399932861328\n",
      "meta: 0.010150671005249023  0.011150360107421875, hyper: 0.006044626235961914 0.007044315338134766, all: 0.01719498634338379\n",
      "0.018195629119873047\n",
      "meta: 0.009999752044677734  0.012010574340820312, hyper: 0.00500035285949707 0.0070111751556396484, all: 0.017010927200317383\n",
      "0.01800680160522461\n",
      "meta: 0.011057376861572266  0.013053178787231445, hyper: 0.0060577392578125 0.00805354118347168, all: 0.019110918045043945\n",
      "0.020113229751586914\n",
      "meta: 0.012115955352783203  0.014116048812866211, hyper: 0.006177663803100586 0.008177757263183594, all: 0.020293712615966797\n",
      "0.021292448043823242\n",
      "meta: 0.010121583938598633  0.011121511459350586, hyper: 0.006075143814086914 0.007075071334838867, all: 0.0171966552734375\n",
      "0.018196821212768555\n",
      "meta: 0.011074066162109375  0.013062238693237305, hyper: 0.006092071533203125 0.008080244064331055, all: 0.01915431022644043\n",
      "0.0201568603515625\n",
      "meta: 0.011072874069213867  0.012071847915649414, hyper: 0.0060040950775146484 0.007003068923950195, all: 0.018075942993164062\n",
      "0.01915264129638672\n",
      "meta: 0.011173009872436523  0.012173652648925781, hyper: 0.0060002803802490234 0.007000923156738281, all: 0.018173933029174805\n",
      "0.019173622131347656\n",
      "meta: 0.010284900665283203  0.01227259635925293, hyper: 0.005038261413574219 0.007025957107543945, all: 0.01731085777282715\n",
      "0.018323898315429688\n",
      "meta: 0.012099504470825195  0.013102293014526367, hyper: 0.0060503482818603516 0.0070531368255615234, all: 0.01915264129638672\n",
      "0.020150184631347656\n",
      "meta: 0.011035919189453125  0.013036727905273438, hyper: 0.006056070327758789 0.008056879043579102, all: 0.019092798233032227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02008819580078125\n",
      "meta: 0.011064529418945312  0.012064456939697266, hyper: 0.0060422420501708984 0.0070421695709228516, all: 0.018106698989868164\n",
      "0.01910686492919922\n",
      "meta: 0.01110386848449707  0.013105630874633789, hyper: 0.0050506591796875 0.007052421569824219, all: 0.01815629005432129\n",
      "0.019156455993652344\n",
      "meta: 0.011081695556640625  0.012083053588867188, hyper: 0.006070852279663086 0.0070722103118896484, all: 0.018153905868530273\n",
      "0.020154237747192383\n",
      "meta: 0.011092901229858398  0.012104034423828125, hyper: 0.005300045013427734 0.006311178207397461, all: 0.01740407943725586\n",
      "0.018404006958007812\n",
      "meta: 0.011076927185058594  0.012102842330932617, hyper: 0.006121397018432617 0.007147312164306641, all: 0.018224239349365234\n",
      "0.020224571228027344\n",
      "meta: 0.010998725891113281  0.011998653411865234, hyper: 0.006000995635986328 0.007000923156738281, all: 0.017999649047851562\n",
      "0.018999814987182617\n",
      "meta: 0.011088132858276367  0.01308894157409668, hyper: 0.004998922348022461 0.0069997310638427734, all: 0.01808786392211914\n",
      "0.01820540428161621\n",
      "meta: 0.011081695556640625  0.013135671615600586, hyper: 0.005001068115234375 0.007055044174194336, all: 0.01813673973083496\n",
      "0.01915740966796875\n",
      "meta: 0.012127161026000977  0.014197826385498047, hyper: 0.0049974918365478516 0.007068157196044922, all: 0.0191953182220459\n",
      "0.0191953182220459\n",
      "meta: 0.012054204940795898  0.014112234115600586, hyper: 0.005000114440917969 0.007058143615722656, all: 0.019112348556518555\n",
      "0.0201108455657959\n",
      "meta: 0.011076927185058594  0.01314234733581543, hyper: 0.0050013065338134766 0.0070667266845703125, all: 0.018143653869628906\n",
      "0.01914358139038086\n",
      "meta: 0.01105046272277832  0.01304936408996582, hyper: 0.004999637603759766 0.006998538970947266, all: 0.018049001693725586\n",
      "0.019092082977294922\n",
      "meta: 0.011091470718383789  0.013092994689941406, hyper: 0.007053852081298828 0.009055376052856445, all: 0.020146846771240234\n",
      "0.020146846771240234\n",
      "meta: 0.011104345321655273  0.012103796005249023, hyper: 0.0050013065338134766 0.0060007572174072266, all: 0.0171051025390625\n",
      "0.01816082000732422\n",
      "meta: 0.01105642318725586  0.014056682586669922, hyper: 0.004999876022338867 0.00800013542175293, all: 0.01905655860900879\n",
      "0.01905655860900879\n",
      "meta: 0.011094093322753906  0.013093948364257812, hyper: 0.0050013065338134766 0.007001161575317383, all: 0.01809525489807129\n",
      "0.0191495418548584\n",
      "meta: 0.011097192764282227  0.013097047805786133, hyper: 0.00500178337097168 0.007001638412475586, all: 0.018098831176757812\n",
      "0.018098831176757812\n",
      "meta: 0.011215925216674805  0.013206958770751953, hyper: 0.006010532379150391 0.008001565933227539, all: 0.019217491149902344\n",
      "0.020217418670654297\n",
      "meta: 0.011080503463745117  0.013080120086669922, hyper: 0.00505375862121582 0.007053375244140625, all: 0.018133878707885742\n",
      "0.019134044647216797\n",
      "meta: 0.011276960372924805  0.013275384902954102, hyper: 0.005015373229980469 0.007013797760009766, all: 0.01829075813293457\n",
      "0.019292593002319336\n",
      "meta: 0.012084245681762695  0.014084100723266602, hyper: 0.004990339279174805 0.006990194320678711, all: 0.019074440002441406\n",
      "0.019074440002441406\n",
      "meta: 0.010058164596557617  0.012058019638061523, hyper: 0.00500035285949707 0.0070002079010009766, all: 0.017058372497558594\n",
      "0.018099546432495117\n",
      "meta: 0.011064529418945312  0.013075828552246094, hyper: 0.004999399185180664 0.007010698318481445, all: 0.018075227737426758\n",
      "0.019128084182739258\n",
      "meta: 0.011172294616699219  0.012171745300292969, hyper: 0.007001399993896484 0.008000850677490234, all: 0.019173145294189453\n",
      "0.021223783493041992\n",
      "meta: 0.011070966720581055  0.012070655822753906, hyper: 0.007055759429931641 0.008055448532104492, all: 0.019126415252685547\n",
      "0.0201265811920166\n",
      "meta: 0.011046409606933594  0.012047290802001953, hyper: 0.005998849868774414 0.0069997310638427734, all: 0.018046140670776367\n",
      "0.019083499908447266\n",
      "meta: 0.01107931137084961  0.012078046798706055, hyper: 0.005999565124511719 0.006998300552368164, all: 0.018077611923217773\n",
      "0.01907944679260254\n",
      "meta: 0.01104879379272461  0.013037443161010742, hyper: 0.005011081695556641 0.0069997310638427734, all: 0.018048524856567383\n",
      "0.018048524856567383\n",
      "meta: 0.010061979293823242  0.012061834335327148, hyper: 0.005042314529418945 0.0070421695709228516, all: 0.017104148864746094\n",
      "0.01810288429260254\n",
      "meta: 0.011045694351196289  0.012045145034790039, hyper: 0.006043910980224609 0.007043361663818359, all: 0.01808905601501465\n",
      "0.020088672637939453\n",
      "meta: 0.011056661605834961  0.012054920196533203, hyper: 0.006005287170410156 0.0070035457611083984, all: 0.01806020736694336\n",
      "0.01911020278930664\n",
      "meta: 0.011083364486694336  0.01216268539428711, hyper: 0.00599980354309082 0.007079124450683594, all: 0.01816248893737793\n",
      "0.0191650390625\n",
      "Epoch 0, Step 200,\tTotal loss: 0.393678,\tHypernet loss: 0.023961\n",
      "\tPSNR: 22.873499203324318 \tSSIM: 0.8426015321537852\n",
      "[61.278059062016254, 65.21890813622741, 65.67925657506913, 58.832347040770784, 71.51544850793468]\n",
      "meta: 0.011077404022216797  0.012077093124389648, hyper: 0.005999326705932617 0.006999015808105469, all: 0.018076419830322266\n",
      "0.01907658576965332\n",
      "meta: 0.011077165603637695  0.013077259063720703, hyper: 0.006078004837036133 0.00807809829711914, all: 0.019155263900756836\n",
      "0.020157337188720703\n",
      "meta: 0.012089967727661133  0.01409149169921875, hyper: 0.005167245864868164 0.007168769836425781, all: 0.019258737564086914\n",
      "0.020258665084838867\n",
      "meta: 0.012042760848999023  0.014056921005249023, hyper: 0.007016181945800781 0.009030342102050781, all: 0.021073102951049805\n",
      "0.021073102951049805\n",
      "meta: 0.011066675186157227  0.01206660270690918, hyper: 0.005001068115234375 0.006000995635986328, all: 0.017067670822143555\n",
      "0.01806783676147461\n",
      "meta: 0.011271476745605469  0.013269901275634766, hyper: 0.006005287170410156 0.008003711700439453, all: 0.019275188446044922\n",
      "0.02027583122253418\n",
      "meta: 0.011062145233154297  0.012061834335327148, hyper: 0.006055593490600586 0.0070552825927734375, all: 0.018117427825927734\n",
      "0.019117355346679688\n",
      "meta: 0.011075258255004883  0.013072967529296875, hyper: 0.0050046443939208984 0.007002353668212891, all: 0.018077611923217773\n",
      "0.019078493118286133\n",
      "meta: 0.011103391647338867  0.01309061050415039, hyper: 0.005083799362182617 0.007071018218994141, all: 0.018174409866333008\n",
      "0.01917433738708496\n",
      "meta: 0.011055469512939453  0.012053728103637695, hyper: 0.006107330322265625 0.007105588912963867, all: 0.01816105842590332\n",
      "0.019162893295288086\n",
      "meta: 0.011107444763183594  0.01310873031616211, hyper: 0.0050563812255859375 0.007057666778564453, all: 0.018165111541748047\n",
      "0.018165111541748047\n",
      "meta: 0.010998010635375977  0.01299738883972168, hyper: 0.005000591278076172 0.006999969482421875, all: 0.01799798011779785\n",
      "0.018997907638549805\n",
      "meta: 0.011057853698730469  0.013057947158813477, hyper: 0.006056070327758789 0.008056163787841797, all: 0.019114017486572266\n",
      "0.020114421844482422\n",
      "meta: 0.011062145233154297  0.012061834335327148, hyper: 0.006092071533203125 0.0070917606353759766, all: 0.018153905868530273\n",
      "0.01915264129638672\n",
      "meta: 0.011091232299804688  0.013090848922729492, hyper: 0.005000114440917969 0.0069997310638427734, all: 0.01809096336364746\n",
      "0.01809096336364746\n",
      "meta: 0.011229515075683594  0.013290643692016602, hyper: 0.00500178337097168 0.0070629119873046875, all: 0.01829242706298828\n",
      "0.019292593002319336\n",
      "meta: 0.010070562362670898  0.011118173599243164, hyper: 0.005999565124511719 0.007047176361083984, all: 0.017117738723754883\n",
      "0.018116474151611328\n",
      "meta: 0.011092424392700195  0.012143850326538086, hyper: 0.005002021789550781 0.006053447723388672, all: 0.017145872116088867\n",
      "0.018160343170166016\n",
      "meta: 0.011062860488891602  0.01306295394897461, hyper: 0.006003618240356445 0.008003711700439453, all: 0.019066572189331055\n",
      "0.020064830780029297\n",
      "meta: 0.011048078536987305  0.012049674987792969, hyper: 0.005052804946899414 0.006054401397705078, all: 0.017102479934692383\n",
      "0.018100738525390625\n",
      "meta: 0.010999202728271484  0.012001752853393555, hyper: 0.0060007572174072266 0.007003307342529297, all: 0.01800251007080078\n",
      "0.018999338150024414\n",
      "meta: 0.011999130249023438  0.013999700546264648, hyper: 0.004999637603759766 0.0070002079010009766, all: 0.018999338150024414\n",
      "0.01999974250793457\n",
      "meta: 0.011083602905273438  0.013083457946777344, hyper: 0.00504302978515625 0.007042884826660156, all: 0.018126487731933594\n",
      "0.018126487731933594\n",
      "meta: 0.01107478141784668  0.013074398040771484, hyper: 0.005143404006958008 0.0071430206298828125, all: 0.018217802047729492\n",
      "0.018217802047729492\n",
      "meta: 0.011071920394897461  0.012073516845703125, hyper: 0.006040811538696289 0.007042407989501953, all: 0.018114328384399414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019114255905151367\n",
      "meta: 0.012283563613891602  0.014284610748291016, hyper: 0.005049943923950195 0.007050991058349609, all: 0.01933455467224121\n",
      "0.020333051681518555\n",
      "meta: 0.010999441146850586  0.012999296188354492, hyper: 0.0059986114501953125 0.007998466491699219, all: 0.018997907638549805\n",
      "0.018997907638549805\n",
      "meta: 0.011095762252807617  0.01309514045715332, hyper: 0.0050008296966552734 0.0070002079010009766, all: 0.018095970153808594\n",
      "0.018095970153808594\n",
      "meta: 0.01100015640258789  0.012998580932617188, hyper: 0.006001710891723633 0.00800013542175293, all: 0.01900029182434082\n",
      "0.020000219345092773\n",
      "meta: 0.011008262634277344  0.01300811767578125, hyper: 0.005991458892822266 0.007991313934326172, all: 0.018999576568603516\n",
      "0.018999576568603516\n",
      "meta: 0.011000633239746094  0.012000322341918945, hyper: 0.005999565124511719 0.00699925422668457, all: 0.017999887466430664\n",
      "0.018999576568603516\n",
      "meta: 0.011574745178222656  0.013631582260131836, hyper: 0.005000114440917969 0.0070569515228271484, all: 0.018631696701049805\n",
      "0.01963329315185547\n",
      "meta: 0.010942459106445312  0.011942386627197266, hyper: 0.006058454513549805 0.007058382034301758, all: 0.01800084114074707\n",
      "0.019000768661499023\n",
      "meta: 0.01100015640258789  0.01300048828125, hyper: 0.004999637603759766 0.006999969482421875, all: 0.018000125885009766\n",
      "0.01900005340576172\n",
      "meta: 0.011144876480102539  0.012144804000854492, hyper: 0.005000114440917969 0.006000041961669922, all: 0.01714491844177246\n",
      "0.018145084381103516\n",
      "meta: 0.01116323471069336  0.012162208557128906, hyper: 0.006033420562744141 0.0070323944091796875, all: 0.018195629119873047\n",
      "0.019196748733520508\n",
      "meta: 0.011316061019897461  0.01231694221496582, hyper: 0.007033586502075195 0.008034467697143555, all: 0.019350528717041016\n",
      "0.021351099014282227\n",
      "meta: 0.011044502258300781  0.013044595718383789, hyper: 0.00500035285949707 0.007000446319580078, all: 0.01804494857788086\n",
      "0.01904439926147461\n",
      "meta: 0.011176586151123047  0.013176918029785156, hyper: 0.005013227462768555 0.007013559341430664, all: 0.01819014549255371\n",
      "0.01819014549255371\n",
      "meta: 0.011099576950073242  0.013091564178466797, hyper: 0.006081342697143555 0.00807332992553711, all: 0.01917290687561035\n",
      "0.01917290687561035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14568/2781117538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfast_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_specialized_param\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PSNR:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsnr_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "steps_til_summary = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import meta_modules\n",
    "img_siren = Siren(in_features=2, hidden_features=256, hidden_layers=3, out_features=3, outermost_linear=True)\n",
    "img_siren.load_state_dict(torch.load('img_siren_ssim_loss_0.pth'))\n",
    "# crossAttHypNet = CrossAttentionHyperNet().cuda()\n",
    "crossAttHypNet = meta_modules.ConvolutionalNeuralProcessImplicit2DHypernet(in_features=3,\n",
    "                                                                    out_features=3,\n",
    "                                                                    image_resolution=(32, 32))\n",
    "crossAttHypNet.load_state_dict(torch.load('crossAttHypNet_ssim_loss_0.pth'))\n",
    "meta_siren = MAML(num_meta_steps=3, hypo_module=img_siren.cuda(), crossAttHypNet=crossAttHypNet.cuda(), \n",
    "                  loss=l2_loss, init_lr=1e-5, \n",
    "                  lr_type='per_parameter_per_step').cuda()\n",
    "meta_siren.load_state_dict(torch.load('meta_siren_ssim_ssim_0.pth'))\n",
    "meta_siren = meta_siren.cuda()\n",
    "meta_siren.eval()\n",
    "\n",
    "dataset = CIFAR10(train=False)\n",
    "test_dataloader = DataLoader(dataset, batch_size=8, num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "# optim = torch.optim.Adam(lr=5e-5, params=meta_siren.parameters())\n",
    "# optim = torch.optim.Adam(lr=5e-6, params=meta_siren.parameters())\n",
    "\n",
    "hypernet_loss_multiplier = 1\n",
    "\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "for epoch in range(1):\n",
    "#     if epoch < 10:\n",
    "#         hypernet_loss_multiplier += 100\n",
    "\n",
    "    for step, sample in enumerate(test_dataloader):\n",
    "        sample = dict_to_gpu(sample)\n",
    "        '''\n",
    "        out_dict = {'model_out':model_output, 'intermed_predictions':intermed_predictions, \n",
    "                    'crossAttHypNet_loss':crossAttHypNet_loss, \n",
    "                    'model_output_hypernet': model_output_hypernet}\n",
    "\n",
    "        return out_dict, fast_params, meta_params, pred_specialized_param\n",
    "        '''\n",
    "        t0 = time.time()\n",
    "        model_output, fast_params, meta_params, pred_specialized_param = meta_siren(sample)    \n",
    "        loss = ((model_output['model_out'] - sample['query']['y'])**2).mean() + hypernet_loss_multiplier * model_output['crossAttHypNet_loss']\n",
    "        print(time.time() - t0)\n",
    "#         for name in pred_specialized_param:\n",
    "#             loss += 10*((pred_specialized_param[name] - fast_params[name].detach()) ** 2).mean()\n",
    "        if False:\n",
    "            pred_specialized_param_corrected = OrderedDict()\n",
    "            l1, l2 = pred_specialized_param.keys(), fast_params.keys()\n",
    "            for (name1, name2) in list(zip(l1, l2)):\n",
    "                pred_specialized_param_corrected[name2] = meta_params[name2] + pred_specialized_param[name1]\n",
    "#                 pred_specialized_param_corrected[name2] = pred_specialized_param[name1]\n",
    "                loss += 1*((pred_specialized_param_corrected[name2] - fast_params[name2].detach()) ** 2).mean()\n",
    "       \n",
    "        \n",
    "        if (step % steps_til_summary == 0) and (epoch % 1 == 0): \n",
    "            print(\"Epoch %d, Step %d,\\tTotal loss: %0.6f,\\tHypernet loss: %0.6f\" % (epoch, step, loss, model_output['crossAttHypNet_loss']))\n",
    "            print('\\tPSNR:', np.mean(psnr_list), '\\tSSIM:', np.mean(ssim_list))\n",
    "            fig, axes = [], list(range(6))#plt.subplots(1,6, figsize=(36,6))\n",
    "            ax_titles = ['Learned Initialization', 'Inner step 1 output', \n",
    "                        'Inner step 2 output', 'Inner step 3 output', \n",
    "                        'HyperNet output', ## added by me\n",
    "                        'Ground Truth']\n",
    "            images = []\n",
    "            for i, inner_step_out in enumerate(model_output['intermed_predictions']):\n",
    "                img = plot_sample_image(inner_step_out, ax=axes[i])\n",
    "                images += [img]\n",
    "#                 axes[i].set_title(ax_titles[i], fontsize=25)\n",
    "            images += [plot_sample_image(model_output['model_out'], ax=axes[-3])]\n",
    "#             axes[-3].set_title(ax_titles[-3], fontsize=25)\n",
    "\n",
    "            if True:\n",
    "                images += [plot_sample_image(model_output['model_output_hypernet'], ax=axes[-2])]\n",
    "#                 axes[-2].set_title(ax_titles[-2], fontsize=25)\n",
    "\n",
    "            img_ground_truth = plot_sample_image(sample['query']['y'], ax=axes[-1])\n",
    "#             axes[-1].set_title(ax_titles[-1], fontsize=25)\n",
    "            psnrs = [cv2.PSNR(img_ground_truth, img) for img in images]\n",
    "#             for img_idx, img in enumerate(images):\n",
    "#                 cv2.imwrite(f'./images/img_{step}_{ax_titles[img_idx]}.jpg', (img*255.).astype(int))\n",
    "#                 cv2.imshow(f'./images/img_{step}_{ax_titles[img_idx]}.jpg', img)\n",
    "#                 cv2.waitKey()\n",
    "            \n",
    "            images += [img_ground_truth]\n",
    "            pickle.dump({step: images}, open(f'./images/img_{step}', 'wb'))\n",
    "            print(psnrs)\n",
    "            plt.show()\n",
    "#             break\n",
    "            \n",
    "#         optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "        if True:\n",
    "            x = lin2img(model_output['model_output_hypernet']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            x += 1.\n",
    "            x /= 2.\n",
    "            x = torch.clip(x, 0., 1.)\n",
    "            y = lin2img(sample['query']['y']).permute(0,3,1,2).contiguous().cpu().detach()\n",
    "            y += 1.\n",
    "            y /= 2.\n",
    "            y = torch.clip(y, 0., 1.)\n",
    "    #         print(x.shape, lin2img(x).shape)\n",
    "#             print(y.min(), y.max())\n",
    "#             print(x.min(), x.max())\n",
    "\n",
    "#             print('PSNR:', psnr(y, x).mean())\n",
    "#             print('psnr_sklearn:', psnr_sklearn(y.numpy(), x.numpy(), data_range=1.))\n",
    "#             print('SSIM:', ssim_metric(y, x))\n",
    "            psnr_list += psnr(y, x).cpu().detach().numpy().tolist()\n",
    "            ssim_list += ssim_metric(y, x).cpu().detach().numpy().tolist()\n",
    "        del model_output, fast_params, meta_params, pred_specialized_param\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "print('PSNR:', np.mean(psnr_list))\n",
    "print('SSIM:', np.mean(ssim_list))\n",
    "\n",
    "l = len(psnr_list) // 1\n",
    "print(len(psnr_list), l)\n",
    "for i in range(1):\n",
    "    print(f'\\nepoch:{i}\\nPSNR:', np.mean(psnr_list[l*i:l*(i+1)]))\n",
    "    print('SSIM:', np.mean(ssim_list[l*i:l*(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MetaSDF_on_Cifar_10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
